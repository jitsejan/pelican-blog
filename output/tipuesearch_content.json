{"pages":[{"title":"Latex cheatsheet","text":"This is my Latex cheatsheet Add style to a code listing styles.tex \\definecolor { dkgreen }{ rgb }{ 0,0.6,0 } \\definecolor { gray }{ rgb }{ 0.5,0.5,0.5 } \\definecolor { mauve }{ rgb }{ 0.58,0,0.82 } \\definecolor { light-gray }{ gray }{ 0.25 } \\usepackage { listings } \\lstdefinestyle { java }{ language=Java, aboveskip=3mm, belowskip=3mm, showstringspaces=false, columns=flexible, basicstyle= { \\footnotesize\\ttfamily } , numberstyle= { \\tiny } , numbers=left, keywordstyle= \\color { blue } , commentstyle= \\color { dkgreen } , stringstyle= \\color { mauve } , breaklines=true, breakatwhitespace=true, tabsize=3, } main.tex \\input { styles.tex } \\lstinputlisting [style=Java, frame=single, caption={Hello world}, captionpos=b] { helloworld.java } Bonus: rename the caption from Listing to Code snippet with the following: \\renewcommand*\\lstlistingname { Code snippet } Subfigures \\usepackage { graphicx } \\usepackage { caption } \\usepackage { subcaption } \\begin { figure* } [h] \\centering \\begin { subfigure } [t] { 0.5 \\textwidth } \\centering \\includegraphics [width=2.2in] { image _ one.png } \\caption { Image one } \\end { subfigure } % ~ \\begin { subfigure } [t] { 0.5 \\textwidth } \\centering \\includegraphics [width=2.2in] { image _ two.png } \\caption { Image two } \\end { subfigure } \\caption { These are two images } \\end { figure* }","tags":"Latex","url":"pages/latex-cheatsheet.html"},{"title":"Pandas cheatsheet","text":"This is my Pandas cheatsheet. Note that I import pandas the 'standard' way: import pandas as pd Convert with dataframes Create dataframe from a dictionary character_df = pd . DataFrame . from_dict ( characters ) Create dictionary from dataframe characters = character_df . to_dict ( orient = 'records' ) Convert CSV to dataframe character_df = pd . DataFrame . from_csv ( \"characters.csv\" , sep = ' \\t ' , encoding = 'utf-8' ) Convert dataframe to CSV character_df . to_csv ( 'characters.csv' , sep = ' \\t ' , encoding = 'utf-8' ) Convert database query to dataframe db = create_engine ( 'postgresql:// %s : %s @ %s : %d /characters' % ( POSTGRES_USER , POSTGRES_PASS , POSTGRES_HOST , POSTGRES_PORT )) character_df = pd . read_sql_query ( 'SELECT * FROM \"character_collection\"' , con = db ) Cleaning dataframes Replace in column character_df [ 'name' ] = character_df [ 'name' ] . str . replace ( '-' , ' ' ) Regex replace in whole dataframe character_df . replace ( r '-' , r ' ' , regex = True , inplace = True ) Regex extract in column character_df [ 'introduction_year' ] = character_df [ 'date_of_introduction' ] . str . extract ( '(\\d{4})-..-..' , expand = True ) Remove all Not-a-Numbers character_df = character_df . replace ({ 'NaN' : None }, regex = True ) Rename a column character_df . rename ( columns = { 'name' : 'character_name' }, inplace = True ) Drop a column character_df = character_df . drop ( 'origin' , axis = 1 ) Drop a row Drop all rows where the name is NaN. character_df . dropna ( subset = [ 'name' ], inplace = True ) Delete a column del character_df [ 'special_diet' ] Convert to integer character_df [ 'introduction_year' ] = character_df [ 'introduction_year' ] . fillna ( - 1 ) . astype ( 'int64' ) Convert to category character_df [ 'superpower' ] = character_df [ 'superpower' ] . astype ( 'category' ) Convert string to list # Convert a string with surrounding brackets and quotes to a list def convert_string_to_list ( column ): \"\"\" Convert unicode string to list \"\"\" return column . str . strip ( '{}' ) . astype ( str ) . apply ( lambda x : x . split ( ',' )[ 0 ] . strip ( \" \\\" \" ) if len ( x ) > 0 else \"\" ) character_df [ 'superpowers' ] = convert_string_to_list ( character_df [ 'superpowers' ]) Create column from index character_df . index . names = [ 'Name' ] character_df = character_df . reset_index () Extend dictionary cell to columns df = pd . concat ([ df . drop ([ 'meta' ], axis = 1 ), df [ 'meta' ] . apply ( pd . Series )], axis = 1 ) Find data Describe the data character_df [ 'age' ] . describe () Unique values characters = character_df [ 'character_name' ] . unique () Field contains character_df [ character_df [ 'name' ] . str . contains ( \"Koopa\" ) . fillna ( False )] Count by character_df . groupby ([ 'superpowers' ]) . count () Loop through data for element in character_df . index : superpower = character_df . iloc [ element ][ 'superpower' ] if not pd . isnull ( superpower ): print 'Super!' Substract Substract two consecutive cells df [ 'difference' ] = df [ 'amount' ] - df [ 'amount' ] . shift ( + 1 ) Add a maximum column for a groupby df [ 'group_maximum' ] = df . groupby ([ 'category' ])[ 'score' ] . transform ( max ) Create category based on values def set_category ( row ): if row [ 'score' ] < float ( row [ 'maximum' ] / 3 ): return 'beginner' elif row [ 'score' ] >= float ( row [ 'maximum' ] / 3 * 2 ): return 'expert' else : return 'intermediate' df [ 'category' ] = df . apply ( set_category , axis = 1 ) Apply lambda function df [ 'inverse_number' ] = df [ 'number' ] . apply ( lambda x : x ** ( - 1 ))","tags":"pandas","url":"pages/pandas-cheatsheet.html"},{"title":"Python cheatsheet","text":"This is my Python cheatsheet Pretty print a dictionary print json . dumps ( characters [: 1 ], indent = 4 ) Find index of item in dictionary index = next ( index for ( index , d ) in enumerate ( characters ) if d [ \"name\" ] == 'Mario' ) Databases Retrieve from Postgresql import psycopg2 connnection = psycopg2 . connect ( database = \"character_db\" , user = \"character_user\" , password = \"character1234\" , host = \"localhost\" , port = '5433' ) cursor = connnection . cursor () cursor . execute ( \"SELECT * FROM \\\" characters \\\" \" ) characters = cursor . fetchall () Dictionary to CSV import csv def write_dictionary_to_csv ( o_file , d ): \"\"\" Write dictionary to output file \"\"\" with open ( o_file , 'wb' ) as csvfile : outputwriter = csv . writer ( csvfile , delimiter = ';' , quoting = csv . QUOTE_MINIMAL ) outputwriter . writerow ( d . keys ()) outputwriter . writerows ( zip ( * d . values ())) output_file = 'output.csv' write_dictionary_to_csv ( output_file , data ) Web crawling Use urllib2 to retrieve page content import urllib2 req = urllib2 . Request ( 'http://www.mariowiki.com' ) data = urllib2 . urlopen ( req ) . read () Set header for urllib2 import urllib2 # Header to request a page with NL as country HEADER = { 'Accept-Language' : 'nl-NL' , 'User-Agent' : \"\"\"Mozilla/5.0 (Windows; U; Windows NT 6.1; nl-NL; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729); nl-NL\"\"\" } req = urllib2 . Request ( 'http://www.mariowiki.com' , headers = HEADER ) data = urllib2 . urlopen ( req ) . read () Use requests to retrieve page content import requests resp = requests . get ( 'http://www.mariowiki.com' ) data = resp . content Use Selenium to retrieve page content from selenium import webdriver browser = webdriver . Chrome () browser . get ( 'http://www.mariowiki.com' ) data = browser . page_source browser . quit () Scroll down in Selenium browser . execute_script ( \"window.scrollTo(0, document.body.scrollHeight);\" ) Find an element and click in Selenium By CSS selector: browser . find_element_by_css_selector ( '#clickme' ) . click () By attribute: browser . find_element_by_xpath ( '//input[@title=\"Open page\"]' ) . click () Make a list of links with cssselect import lxml.html tree = lxml . html . fromstring ( data ) links = [ 'http://www.mariowiki.com' + link . get ( 'href' ) for link in tree . cssselect ( 'div[role*= \\' navigation \\' ] a' )] Switch between tabs with Selenium browser . switch_to . window ( window_name = browser . window_handles [ 1 ]) browser . quit () browser . switch_to . window ( window_name = browser . window_handles [ 0 ]) Pickle Use the pickle libary to save a variable to a file and load it again. import pickle colors = [ 'blue' , 'red' ] pickle . dump ( colors , open ( \"colors.p\" , \"wb\" ) ) saved_colors = pickle . load ( open ( \"colors.p\" , \"rb\" ) ) saved_colors","tags":"python","url":"pages/python-cheatsheet.html"},{"title":"Splunk cheatsheet","text":"This is my Splunk cheatsheet. Replace single quote with double quote | rex mode=sed \"s/\\'/\\\"/g\" field=myfield Extract JSON data from an JSON array The following will try to find ten matches for strings contained in curly brackets. Next it will be expanded to a multi value field so we can use spath on each extracted field. | rex max_match=10 \"(?<json_field>{[&#94;}]+})\" field=myjsonarrayfield | mvexpand json_field | spath input=json_field | rename field_in_json_field AS field Drilldown of areachart <drilldown> <set token= \"form.character\" > $click.name2$ </set> </drilldown> Create a range between limits | eval range_field = mvrange(start, end, step) | mvexpand range_field | stats count by range_field Frequency of Splunk restarts index=_internal \"Splunkd starting\" | timechart span=1d count(_raw) as Event","tags":"splunk","url":"pages/splunk-cheatsheet.html"},{"title":"Hadoop Experiment - Spark with Pyspark in a Jupyter notebook","text":"Pig Using the Pig language , we can make a script to perform the MapReduce actions similar to the previous post . Note that I will be using the same CSV file as before. gamedata_01.pig gamedata = LOAD 'nesgamedata.csv' AS ( index : int , name : chararray , grade : chararray , publisher : chararray , reader_rating : chararray , number_of_votes : int , publish_year : int , total_grade : chararray ); DESCRIBE gamedata ; DUMP gamedata ; [ root@quickstart gamedata ] # pig -f gamedata_01.pig ... ( 269 ,Winter Games,12,Epyx,13,24,1987,12.96 ) ( 270 ,Wizards and Warriors,9,Rare,6,55,1987,6.053571428571429 ) ( 271 ,World Games,6,Epyx,9,8,1986,8.666666666666666 ) ( 272 ,Wrath of the Black Manta,7,Taito,6,31,1989,6.03125 ) ( 273 ,Wrecking Crew,10,Nintendo,8,18,1985,8.105263157894736 ) ( 274 ,Xevious,5,Namco,6,36,1988,5.972972972972973 ) ( 275 ,Xexyz,10,Hudson Soft,5,26,1989,5.185185185185185 ) ( 276 ,Yoshi,5,Nintendo,6,41,1992,5.976190476190476 ) ( 277 ,Yoshi ' s Cookie,5,Nintendo,7,23,1993,6.916666666666667 ) ( 278 ,Zanac,2,Pony,3,21,1986,2.9545454545454546 ) ( 279 ,Zelda II: The Adventure of Link,3,Nintendo,4,112,1989,3.9911504424778763 ) ( 280 ,Zelda, The Legend of,3,Nintendo,3,140,1986,3.0 ) ( 281 ,Zombie Nation,4,Kaze,8,26,1991,7.851851851851852 ) Now lets calculate the average rating given by users for each different rating given by the author of the website for all Nintendo games. gamedata_02.pig gamedata = LOAD 'nesgamedata.csv' AS ( index : int , name : chararray , grade : int , publisher : chararray , reader_rating : int , number_of_votes : int , publish_year : int , total_grade : float ); gamesNintendo = FILTER gamedata BY publisher == 'Nintendo' ; gamesRatings = GROUP gamesNintendo BY grade ; averaged = FOREACH gamesRatings GENERATE group as rating , AVG ( gamesNintendo . total_grade ) AS avgRating ; DUMP averaged ; ` Run the script on the Hadoop machine: [ root@quickstart gamedata ] # pig -f gamedata_02.pig ... ( 1 ,2.321279764175415 ) ( 2 ,3.3024109601974487 ) ( 3 ,3.7930258750915526 ) ( 4 ,3.0212767124176025 ) ( 5 ,5.381512546539307 ) ( 6 ,5.773015689849854 ) ( 7 ,6.020833492279053 ) ( 8 ,9.833333015441895 ) ( 9 ,6.624411582946777 ) ( 10 ,8.105262756347656 ) ( 12 ,8.070609092712402 ) ( 13 ,10.066511631011963 ) From this we can observe that on average the users do not really agree with the author on the ratings. Often the author gives higher grades to a game than the users.","tags":"posts","url":"hadoop-experiment-pig-scripting.html"},{"title":"Hadoop Experiment - Spark with Pyspark in a Jupyter notebook","text":"Docker setup I will use the Docker image from Jupyter. It contains Spark and Jupyter and makes developing and testing pyspark very easy. The Dockerfile will retrieve the Jupyter pyspark notebook image, add the Python requirements file and install the dependencies. It will start the Notebook server using Jupyter Lab on the given port. The resulting image can be found on my Docker repo . # Dockerfile FROM jupyter/pyspark-notebook ADD requirements.txt ./ RUN pip install -r requirements.txt CMD [\"start.sh\", \"jupyter\", \"lab\", \"--notebook-dir=/opt/notebooks\", \"--ip='*'\", \"--no-browser\", \"--allow-root\", \"--port=8559\"] To start the container, I use the following docker-compose.yml version: '2' services: pyspark: image: jitsejan/pyspark volumes: - ./notebooks:/opt/notebooks - ./data:/opt/data ports: - \"8559:8559\" Using Pyspark from pyspark import SparkConf , SparkContext import collections Configure the Spark connection conf = SparkConf () . setMaster ( \"local\" ) . setAppName ( \"GameRatings\" ) sc = SparkContext ( conf = conf ) Verify that the Spark context is working by creating a random RDD of 1000 values and pick 5 values. rdd = sc . parallelize ( range ( 1000 )) rdd . takeSample ( False , 5 ) [820, 967, 306, 62, 448] Next we can create an RDD from the data from the previous Hadoop notebook. lines = sc.textFile(\"../data/nesgamedata.csv\") Experiment one Lets calculate the average rating of the voters compared to the votes of the author. def parseLine ( line ): fields = line . split ( ' \\t ' ) index = int ( fields [ 0 ]) name = fields [ 1 ] grade = float ( fields [ 2 ]) publisher = fields [ 3 ] reader_rating = float ( fields [ 4 ]) number_of_votes = int ( fields [ 5 ]) publish_year = int ( fields [ 6 ]) total_grade = float ( fields [ 7 ]) return ( grade , total_grade , name , publisher , reader_rating , number_of_votes , publish_year ) We return the grade and the total grade as a tuple from the parseLine function. games_rdd = lines . map ( parseLine ) games_rdd . take ( 2 ) [(12.0, 10.044444444444444, '10-Yard Fight', 'Nintendo', 10.0, 44, 1985), (11.0, 8.044776119402986, '1942', 'Capcom', 8.0, 66, 1985)] Add a 1 to each line so we can sum the total_grades . games_mapped = games_rdd . mapValues ( lambda x : ( x , 1 )) games_mapped . take ( 2 ) [(12.0, (10.044444444444444, 1)), (11.0, (8.044776119402986, 1))] Sum all total_grades by using the key grade . For each row this will sum the grades and it will sum the 1's that we've added. games_reduced = games_mapped . reduceByKey ( lambda x , y : ( x [ 0 ] + y [ 0 ], x [ 1 ] + y [ 1 ])) games_reduced . take ( 2 ) [(12.0, (70.26902777777778, 7)), (11.0, (193.88550134591918, 25))] Calculate the average total_grade for each grade . average_grade = games_reduced . mapValues ( lambda x : x [ 0 ] / x [ 1 ]) results = average_grade . collect () for result in results : print ( result ) (12.0, 10.03843253968254) (11.0, 7.755420053836767) (5.0, 5.05270714136425) (13.0, 10.26375094258694) (7.0, 6.212705308155384) (4.0, 4.909347517549459) (8.0, 7.0328696656678105) (9.0, 6.519739721431783) (6.0, 5.63766311790758) (2.0, 3.495086595355065) (3.0, 4.1090931800649315) (10.0, 6.9165990377786555) (1.0, 2.321295734457781) Experiment two Filter out all Nintendo games, where the publisher is the 4-th element in the row. nintendoGames = games_rdd . filter ( lambda x : 'Nintendo' in x [ 3 ]) nintendoGames . take ( 2 ) [(12.0, 10.044444444444444, '10-Yard Fight', 'Nintendo', 10.0, 44, 1985), (5.0, 4.014705882352941, 'Balloon Fight', 'Nintendo', 4.0, 67, 1984)] Take the year and the total grade. nintendoYears = nintendoGames . map ( lambda x : ( x [ - 1 ], x [ 1 ])) nintendoYears . take ( 2 ) [(1985, 10.044444444444444), (1984, 4.014705882352941)] Calculate the minimum grade for each year. minYears = nintendoYears . reduceByKey ( lambda x , y : min ( x , y )) results = minYears . collect () for result in results : print ( 'Year: {:d} \\t Minimum score: {:.2f}' . format ( result [ 0 ] , result [ 1 ])) Year : 1985 Minimum score : 2.00 Year : 1984 Minimum score : 3.96 Year : 1991 Minimum score : 2.98 Year : 1990 Minimum score : 2.00 Year : 1989 Minimum score : 3.99 Year : 1988 Minimum score : 3.99 Year : 1986 Minimum score : 2.98 Year : 1987 Minimum score : 1.99 Year : 1983 Minimum score : 6.02 Year : 1992 Minimum score : 5.98 Year : 1993 Minimum score : 6.92 Lets try using FlatMap to count the most occurring words in the titles of the NES games. words = games_rdd . flatMap ( lambda x : x [ 2 ] . split ()) words . take ( 10 ) ['10-Yard', 'Fight', '1942', '1943', '720', 'Degrees', '8', 'Eyes', 'Abadox', 'Adventure'] Now we count the words and sort by count. wordCounts = words . map ( lambda x : ( x , 1 )) . reduceByKey ( lambda x , y : x + y ) wordCountsSorted = wordCounts . map ( lambda x : ( x [ 1 ], x [ 0 ])) . sortByKey () results = wordCountsSorted . collect () for count , word in reversed ( results ): print ( word , count ) The 20 of 17 Super 11 the 11 Baseball 9 and 8 2 8 Ninja 7 Man 7 II 7 Mega 6 3 6 Dragon 5 Adventure 5 Tecmo 4 Spy 4 version) 4 .... This will result in a list of words with weird characters, spaces and other unwanted content. The text can be filtered in the flatMap function. import re def normalizeWords ( text ): \"\"\" Remove unwanted text \"\"\" return re . compile ( r '\\W+' , re . UNICODE ) . split ( text [ 2 ] . lower ()) words_normalized = games_rdd . flatMap ( normalizeWords ) wordNormCounts = words_normalized . countByValue () for word , count in sorted ( wordNormCounts . items (), key = lambda x : x [ 1 ], reverse = True ): if word . encode ( 'ascii' , 'ignore' ): print ( word , count ) the 31 of 17 2 12 ii 11 super 11 baseball 9 s 9 and 8 3 7 man 7 ninja 7 dragon 6 mega 6 adventure 5 adventures 4 n 4 donkey 4 kong 4 mario 4 monster 4 warriors 4 version 4 .... We can of course improve the normalize function and use NLTK or any other language processing library to clean up the stopwords, verbs and other undesired words in the text. The notebook can be found here .","tags":"posts","url":"hadoop-experiment-spark-with-pyspark-in-jupyter.html"},{"title":"Hadoop Experiment - MapReduce on Cloudera","text":"In this example I will extract data with NES reviews from http://videogamecritic.com. I will create a dataframe, add some extra fields and save the data to a CSV-file. This file will be used for a simple MapReduce script. Note: I have a Docker container running with Selenium instead of installing all dependencies on my system. See this page . Extract script The script below is used to retrieve the data. It is ugly code, but it is doing the job. import lxml.html import requests from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities import pandas as pd URL = 'http://videogamecritic.com/nes.htm' def map_rating ( rating ): \"\"\" Function to convert the rating to a number \"\"\" if ( rating == \"A+\" ): return 1 ; if ( rating == \"A\" ): return 2 ; if ( rating == \"A-\" ): return 3 ; if ( rating == \"B+\" ): return 4 ; if ( rating == \"B\" ): return 5 ; if ( rating == \"B-\" ): return 6 ; if ( rating == \"C+\" ): return 7 ; if ( rating == \"C\" ): return 8 ; if ( rating == \"C-\" ): return 9 ; if ( rating == \"D+\" ): return 10 ; if ( rating == \"D\" ): return 11 ; if ( rating == \"D-\" ): return 12 ; if ( rating == \"F\" ): return 13 ; if ( rating == \"F-\" ): return 14 ; return 15 ; resp = requests . get ( URL ) if resp . status_code != 200 : raise Exception ( 'GET ' + link + ' {}' . format ( resp . status_code )) tree = lxml . html . fromstring ( resp . content ) gamepages = [ 'http://videogamecritic.com/' + game . get ( 'href' ) for game in tree . cssselect ( 'h3 a' )] driver = webdriver . Remote ( \"http://localhost:4444/wd/hub\" , DesiredCapabilities . CHROME ) gamedata = [] for page in gamepages : # Retrieve the data driver . get ( page ) data = lxml . html . fromstring ( driver . page_source ) # Extract the fields grades = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' hdr \\' ]' )] names = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' hdl \\' ]' )] metadata = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' mdl \\' ]' )] votes = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' vote \\' ]' )] # Append to dataset gamedata += list ( zip ( names , votes , grades , metadata )) driver . quit () ## # DataFrame magic ## df = pd . DataFrame . from_dict ( gamedata ) df = df . rename ( columns = { 0 : \"name\" , 1 : \"vote\" , 2 : \"grade\" , 3 : \"publisher\" }) # Extract and convert data df [ 'reader_rating' ] = df [ 'vote' ] . str . extract ( 'Readers:\\s(.*?)\\s\\(' , expand = True ) df [ 'reader_rating' ] = df [ 'reader_rating' ] . apply ( lambda x : map_rating ( x )) . astype ( 'int' ) df [ 'number_of_votes' ] = df [ 'vote' ] . str . extract ( '\\((\\d*)\\svotes\\)' , expand = True ) . astype ( 'int' ) df [ 'grade' ] = df [ 'grade' ] . str . replace ( \"Grade:\" , \"\" ) . str . strip () . apply ( lambda x : map_rating ( x )) . astype ( 'int' ) df [ 'publish_year' ] = df [ 'publisher' ] . str . extract ( '\\((\\d*)\\)Reviewed' , expand = True ) df [ 'publisher' ] = df [ 'publisher' ] . str . extract ( \"Publisher:\\s(.*?)\\s\\(\" , expand = True ) df . drop ( 'vote' , axis = 1 , inplace = True ) # Calculate the total grade df [ 'total_grade' ] = ( df [ 'grade' ] + df [ 'reader_rating' ] * df [ 'number_of_votes' ]) / ( df [ 'number_of_votes' ] + 1 ) # Corrections df [ 'publisher' ] = df [ 'publisher' ] . str . replace ( 'Electrobrain' , 'Electro Brain' ) # Save to file df . to_csv ( 'nesgamedata.csv' , sep = ' \\t ' , header = False ) This will result in the following file. 0 10 -Yard Fight 12 Nintendo 10 44 1985 10 .044444444444444 1 1942 11 Capcom 8 65 1985 8 .045454545454545 2 1943 5 Capcom 4 58 1988 4 .016949152542373 3 720 Degrees 13 Tengen 11 24 1989 11 .08 4 8 Eyes 13 Taxan 10 30 1989 10 .096774193548388 5 Abadox 11 Abadox 6 34 1989 6 .142857142857143 6 Adventure Island 7 Hudson Soft 6 63 1987 6 .015625 7 Adventure Island 2 4 Hudson Soft 5 40 1990 4 .975609756097561 8 Adventure Island 3 4 Hudson Soft 5 27 1992 4 .964285714285714 9 Adventures in the Magic Kingdom 5 Capcom 8 23 1990 7 .875 ... MapReduce script Calculate how often what rating is used. Map the reader ratings Reduce to counts per rating Lets create the script that will perform my first MapReduce action. The content of the following cell will be saved to a file which can in turn be used to perform the mapping and reducing. from mrjob.job import MRJob from mrjob.step import MRStep class GamesBreakdown ( MRJob ): def steps ( self ): return [ MRStep ( mapper = self . mapper_get_ratings , reducer = self . reducer_count_ratings ) ] def mapper_get_ratings ( self , _ , line ): ( index , name , grade , publisher , reader_rating , number_of_votes , publish_year , total_grade ) = line . split ( ' \\t ' ) yield reader_rating , 1 def reducer_count_ratings ( self , key , values ): yield key , sum ( values ) if __name__ == '__main__' : GamesBreakdown . run () Execution To use Hadoop, the command should look like the following and should be run on the Hadoop machine, with the hadoop-streaming-jar argument only given in case the .jar is not found: python gamesbreakdown.py -r hadoop --hadoop-streaming-jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar nesgamedata.csv To do this, upload the nesgamesdata.csv file and the script gamesbreakdown.py to HDFS, using the Hue files view and upload functionality. Next copy them to the local folder to be used in the command. Of course other methods to get the files locally on the Hadoop machine can be used. I am using the Cloudera Quickstart image to experiment with Docker and Cloudera and the docker-compose.yml contains the following: version: '2' services: cloudera: hostname: quickstart.cloudera command: /usr/bin/docker-quickstart tty: true privileged: true image: cloudera/quickstart:latest volumes: - ./data:/opt/data ports: - \"8020:8020\" - \"8022:22\" # ssh - \"7180:7180\" # Cloudera Manager - \"8888:8888\" # HUE - \"11000:11000\" # Oozie - \"50070:50070\" # HDFS REST Namenode - \"2181:2181\" - \"11443:11443\" - \"9090:9090\" - \"8088:8088\" - \"19888:19888\" - \"9092:9092\" - \"8983:8983\" - \"16000:16000\" - \"16001:16001\" - \"42222:22\" - \"8042:8042\" - \"60010:60010\" - \"8080:8080\" - \"7077:7077\" Connect to the machine and verify the file is present: jitsejan@ssdnodes-jj-kvm:~/cloudera_docker$ docker exec -ti clouderadocker_cloudera_1 bash [ root@quickstart / ] # cd home/ [ root@quickstart home ] # mkdir gamedata && cd $_ [ root@quickstart gamedata ] # hadoop fs -get gamedata/gamesbreakdown.py gamesbreakdown.py [ root@quickstart gamedata ] # hadoop fs -get gamedata/nesgamedata.csv nesgamedata.csv [ root@quickstart gamedata ] # ll total 20 -rw-r--r-- 1 root root 590 Sep 5 13 :42 gamesbreakdown.py -rw-r--r-- 1 root root 15095 Sep 5 13 :56 nesgamedata.csv Install the mrjob library on the Cloudera container. [ root@quickstart gamedata ] # yum install python-pip -y [ root@quickstart gamedata ] # pip install mrjob Run the script without using Hadoop to verify the installation. [ root@quickstart gamedata ] # python gamesbreakdown.py nesgamedata.csv No configs found ; falling back on auto-configuration Creating temp directory /tmp/gamesbreakdown.root.20170905.135809.859796 Running step 1 of 1 ... Streaming final output from /tmp/gamesbreakdown.root.20170905.135809.859796/output... \"10\" 15 \"11\" 14 \"12\" 12 \"13\" 4 \"2\" 9 \"3\" 28 \"4\" 44 \"5\" 34 \"6\" 48 \"7\" 31 \"8\" 28 \"9\" 15 Removing temp directory /tmp/gamesbreakdown.root.20170905.135809.859796... Now use Hadoop to start the cluster magic. [ root@quickstart gamedata ] # python gamesbreakdown.py -r hadoop nesgamedata.csv No configs found ; falling back on auto-configuration Looking for hadoop binary in $PATH ... Found hadoop binary: /usr/bin/hadoop Using Hadoop version 2 .6.0 Looking for Hadoop streaming jar in /home/hadoop/contrib... Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce... Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar Creating temp directory /tmp/gamesbreakdown.root.20170905.135908.241472 Copying local files to hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472/files/... Running step 1 of 1 ... packageJobJar: [] [ /usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar ] /tmp/streamjob3556720292725669631.jar tmpDir = null Connecting to ResourceManager at /0.0.0.0:8032 Connecting to ResourceManager at /0.0.0.0:8032 Total input paths to process : 1 number of splits:2 Submitting tokens for job: job_1504618114531_0001 Submitted application application_1504618114531_0001 The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504618114531_0001/ Running job: job_1504618114531_0001 Job job_1504618114531_0001 running in uber mode : false map 0 % reduce 0 % map 50 % reduce 0 % map 100 % reduce 0 % map 100 % reduce 100 % Job job_1504618114531_0001 completed successfully Output directory: hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472/output Counters: 49 File Input Format Counters Bytes Read = 19191 File Output Format Counters Bytes Written = 86 File System Counters FILE: Number of bytes read = 2307 FILE: Number of bytes written = 358424 FILE: Number of large read operations = 0 FILE: Number of read operations = 0 FILE: Number of write operations = 0 HDFS: Number of bytes read = 19527 HDFS: Number of bytes written = 86 HDFS: Number of large read operations = 0 HDFS: Number of read operations = 9 HDFS: Number of write operations = 2 Job Counters Data-local map tasks = 2 Launched map tasks = 2 Launched reduce tasks = 1 Total megabyte-seconds taken by all map tasks = 8678400 Total megabyte-seconds taken by all reduce tasks = 3588096 Total time spent by all map tasks ( ms )= 8475 Total time spent by all maps in occupied slots ( ms )= 8475 Total time spent by all reduce tasks ( ms )= 3504 Total time spent by all reduces in occupied slots ( ms )= 3504 Total vcore-seconds taken by all map tasks = 8475 Total vcore-seconds taken by all reduce tasks = 3504 Map-Reduce Framework CPU time spent ( ms )= 2450 Combine input records = 0 Combine output records = 0 Failed Shuffles = 0 GC time elapsed ( ms )= 367 Input split bytes = 336 Map input records = 282 Map output bytes = 1737 Map output materialized bytes = 2313 Map output records = 282 Merged Map outputs = 2 Physical memory ( bytes ) snapshot = 651063296 Reduce input groups = 12 Reduce input records = 282 Reduce output records = 12 Reduce shuffle bytes = 2313 Shuffled Maps = 2 Spilled Records = 564 Total committed heap usage ( bytes )= 679477248 Virtual memory ( bytes ) snapshot = 4099473408 Shuffle Errors BAD_ID = 0 CONNECTION = 0 IO_ERROR = 0 WRONG_LENGTH = 0 WRONG_MAP = 0 WRONG_REDUCE = 0 Streaming final output from hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472/output... \"10\" 15 \"11\" 14 \"12\" 12 \"13\" 4 \"2\" 9 \"3\" 28 \"4\" 44 \"5\" 34 \"6\" 48 \"7\" 31 \"8\" 28 \"9\" 15 Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472... Removing temp directory /tmp/gamesbreakdown.root.20170905.135908.241472... Now lets add another reducer step to sort the counts of the ratings. from mrjob.job import MRJob from mrjob.step import MRStep class GamesBreakdownUpdate ( MRJob ): def steps ( self ): return [ MRStep ( mapper = self . mapper_get_ratings , reducer = self . reducer_count_ratings ), MRStep ( reducer = self . reducer_sorted_output ) ] def mapper_get_ratings ( self , _ , line ): ( index , name , grade , publisher , reader_rating , number_of_votes , publish_year , total_grade ) = line . split ( ' \\t ' ) yield reader_rating , 1 def reducer_count_ratings ( self , key , values ): yield str ( sum ( values )) . zfill ( 2 ), key def reducer_sorted_output ( self , count , ratings ): for rating in ratings : yield rating , count if __name__ == '__main__' : GamesBreakdownUpdate . run () [ root@quickstart gamedata ] # python gamesbreakdownupdate.py nesgamedata.csv No configs found ; falling back on auto-configuration Creating temp directory /tmp/gamesbreakdownupdate.root.20170905.142738.885314 Running step 1 of 2 ... Running step 2 of 2 ... Streaming final output from /tmp/gamesbreakdownupdate.root.20170905.142738.885314/output... \"13\" \"04\" \"2\" \"09\" \"12\" \"12\" \"11\" \"14\" \"10\" \"15\" \"9\" \"15\" \"3\" \"28\" \"8\" \"28\" \"7\" \"31\" \"5\" \"34\" \"4\" \"44\" \"6\" \"48\" Removing temp directory /tmp/gamesbreakdownupdate.root.20170905.142738.885314... That concludes my first experiments with Hadoop and MapReduce. Next step: using Pig or Spark to calculate similar statistics without all the overhead.","tags":"posts","url":"hadoop-experiment-mapreduce-on-cloudera.html"},{"title":"Getting started with the Hortonworks Hadoop Sandbox","text":"Download the HDP Docker image from Hortonworks. jitsejan@ssdnodes-jj-kvm:~/downloads$ wget https://downloads-hortonworks.akamaized.net/sandbox-hdp-2.6.1/HDP_2_6_1_docker_image_28_07_2017_14_42_40.tar Load the Docker image from the TAR-file. jitsejan@ssdnodes-jj-kvm:~/downloads$ docker load -i HDP_2_6_1_docker_image_28_07_2017_14_42_40.tar jitsejan@ssdnodes-jj-kvm:~/downloads$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE anaconda3docker_anaconda latest 4faa1524bf2d 18 hours ago 3 .397 GB sandbox-hdp latest c3cef4760133 4 weeks ago 12 .2 GB continuumio/anaconda3 latest f3a9cb1bc160 12 weeks ago 2 .317 GB Download and run the start-up script. jitsejan@ssdnodes-jj-kvm:~/downloads$ wget https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/sandbox-deployment-and-install-guide/assets/start_sandbox-hdp.sh jitsejan@ssdnodes-jj-kvm:~/downloads$ chmod +x start_sandbox-hdp.sh jitsejan@ssdnodes-jj-kvm:~/downloads$ ./start_sandbox-hdp.sh Verify the container is started. jitsejan@ssdnodes-jj-kvm:~/downloads$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 26bdf90de81b sandbox-hdp \"/usr/sbin/sshd -D\" About an hour ago Up About an hour 0 .0.0.0:1000->1000/tcp, 0 .0.0.0:1100->1100/tcp, 0 .0.0.0:1220->1220/tcp, 0 .0.0.0:1988->1988/tcp, 0 .0.0.0:2049->2049/tcp, 0 .0.0.0:2100->2100/tcp, 0 .0.0.0:2181->2181/tcp, 0 .0.0.0:3000->3000/tcp, 0 .0.0.0:4040->4040/tcp, 0 .0.0.0:4200->4200/tcp, 0 .0.0.0:4242->4242/tcp, 0 .0.0.0:5007->5007/tcp, 0 .0.0.0:5011->5011/tcp, 0 .0.0.0:6001->6001/tcp, 0 .0.0.0:6003->6003/tcp, 0 .0.0.0:6008->6008/tcp, 0 .0.0.0:6080->6080/tcp, 0 .0.0.0:6188->6188/tcp, 0 .0.0.0:8000->8000/tcp, 0 .0.0.0:8005->8005/tcp, 0 .0.0.0:8020->8020/tcp, 0 .0.0.0:8032->8032/tcp, 0 .0.0.0:8040->8040/tcp, 0 .0.0.0:8042->8042/tcp, 0 .0.0.0:8080->8080/tcp, 0 .0.0.0:8082->8082/tcp, 0 .0.0.0:8086->8086/tcp, 0 .0.0.0:8088->8088/tcp, 0 .0.0.0:8090-8091->8090-8091/tcp, 0 .0.0.0:8188->8188/tcp, 0 .0.0.0:8443->8443/tcp, 0 .0.0.0:8744->8744/tcp, 0 .0.0.0:8765->8765/tcp, 0 .0.0.0:8886->8886/tcp, 0 .0.0.0:8888-8889->8888-8889/tcp, 0 .0.0.0:8983->8983/tcp, 0 .0.0.0:8993->8993/tcp, 0 .0.0.0:9000->9000/tcp, 0 .0.0.0:9995-9996->9995-9996/tcp, 0 .0.0.0:10000-10001->10000-10001/tcp, 0 .0.0.0:10015-10016->10015-10016/tcp, 0 .0.0.0:10500->10500/tcp, 0 .0.0.0:10502->10502/tcp, 0 .0.0.0:11000->11000/tcp, 0 .0.0.0:15000->15000/tcp, 0 .0.0.0:15002->15002/tcp, 0 .0.0.0:15500-15505->15500-15505/tcp, 0 .0.0.0:16000->16000/tcp, 0 .0.0.0:16010->16010/tcp, 0 .0.0.0:16020->16020/tcp, 0 .0.0.0:16030->16030/tcp, 0 .0.0.0:18080-18081->18080-18081/tcp, 0 .0.0.0:19888->19888/tcp, 0 .0.0.0:21000->21000/tcp, 0 .0.0.0:33553->33553/tcp, 0 .0.0.0:39419->39419/tcp, 0 .0.0.0:42111->42111/tcp, 0 .0.0.0:50070->50070/tcp, 0 .0.0.0:50075->50075/tcp, 0 .0.0.0:50079->50079/tcp, 0 .0.0.0:50095->50095/tcp, 0 .0.0.0:50111->50111/tcp, 0 .0.0.0:60000->60000/tcp, 0 .0.0.0:60080->60080/tcp, 0 .0.0.0:2222->22/tcp, 0 .0.0.0:1111->111/tcp sandbox-hdp Login to the machine and run the first Hadoop command. jitsejan@ssdnodes-jj-kvm:~/downloads$ ssh 127 .0.0.1 -p 2222 -l maria_dev [ maria_dev@sandbox ~ ] $ hadoop fs -ls Found 2 items drwxr-xr-x - maria_dev hdfs 0 2017 -09-01 09 :29 .Trash drwxr-xr-x - maria_dev hdfs 0 2017 -09-01 08 :16 hive","tags":"posts","url":"getting-started-with-hortonworks-sandbox.html"},{"title":"Using Anaconda with Docker","text":"Install Docker and add user jitsejan@ssdnodes-jj-kvm:~$ sudo apt install docker.io docker-compose -y jitsejan@ssdnodes-jj-kvm:~$ sudo usermod -aG docker $USER Create the folder structure jitsejan@ssdnodes-jj-kvm:~/anaconda3_docker$ tree . ├── data ├── docker-compose.yml ├── Dockerfile ├── notebooks ├── README.md └── requirements.txt The data folder will contain input and output data for the notebooks. The notebooks folder will contain the Jupyter notebooks. The Dockerfile will create the folders in the container and install the Python requirements from the requirements.txt. Content of Dockerfile: FROM continuumio/anaconda3 ADD requirements.txt / RUN pip install -r requirements.txt CMD [\"/opt/conda/bin/jupyter\", \"notebook\", \"--notebook-dir=/opt/notebooks\", \"--ip='*'\", \"--no-browser\", \"--allow-root\"] Content of docker-compose.yml: version: '2' services: anaconda: build: . volumes: - ./notebooks:/opt/notebooks ports: - \"8888:8888\" Start the container jitsejan@ssdnodes-jj-kvm:~/anaconda3_docker$ docker-compose up --build Action Go to your IP-address on the given port and start coding. My final notebook setup can be found on my Github .","tags":"posts","url":"using-anaconda-with-docker.html"},{"title":"Using Scrapy in Jupyter notebook","text":"This notebook makes use of the Scrapy library to scrape data from a website. Following the basic example, we create a QuotesSpider and call the CrawlerProcess with this spider to retrieve quotes from http://quotes.toscrape.com . In this notebook two pipelines are defined, both writing results to a JSON file. The first option is to create a separate class that defines the pipeline and explicitly has the functions to write to a file per found item. It enables more flexibility when dealing with stranger data formats, or if you want to setup a custom way of writing items to file. The pipeline is set in the custom_settings parameter ITEM_PIPELINES inside the QuoteSpider class. However, I simply want to write the list of items that are found in the spider to a JSON file and therefor it is easier to choose the second option, where only the FEED_FORMAT has to be set to JSON and the output file needs to be defined in FEED_URI inside the custom settings of the spider. No additional classes or definitions need to be created, making the FEED_FORMAT/FEED_URI a convenient option. Once the quotes are retrieved the JSON file will be created on disk and can be loaded to a Pandas dataframe. This dataframe can then be analyzed, modified and be used for further processing. This notebook simply loads the JSON file to a dataframe and writes it again to a pickle. In [1]: # Settings for notebook from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" # Show Python version import platform platform . python_version () Out[1]: '3.6.1' Import Scrapy In [2]: try : import scrapy except : ! pip install scrapy import scrapy from scrapy.crawler import CrawlerProcess Setup a pipeline This class creates a simple pipeline that writes all found items to a JSON file, where each line contains one JSON element. In [3]: import json class JsonWriterPipeline ( object ): def open_spider ( self , spider ): self . file = open ( 'quoteresult.jl' , 'w' ) def close_spider ( self , spider ): self . file . close () def process_item ( self , item , spider ): line = json . dumps ( dict ( item )) + \" \\n \" self . file . write ( line ) return item Define the spider The QuotesSpider class defines from which URLs to start crawling and which values to retrieve. I set the logging level of the crawler to warning, otherwise the notebook is overloaded with DEBUG messages about the retrieved data. In [4]: import logging class QuotesSpider ( scrapy . Spider ): name = \"quotes\" start_urls = [ 'http://quotes.toscrape.com/page/1/' , 'http://quotes.toscrape.com/page/2/' , ] custom_settings = { 'LOG_LEVEL' : logging . WARNING , 'ITEM_PIPELINES' : { '__main__.JsonWriterPipeline' : 1 }, # Used for pipeline 1 'FEED_FORMAT' : 'json' , # Used for pipeline 2 'FEED_URI' : 'quoteresult.json' # Used for pipeline 2 } def parse ( self , response ): for quote in response . css ( 'div.quote' ): yield { 'text' : quote . css ( 'span.text::text' ) . extract_first (), 'author' : quote . css ( 'span small::text' ) . extract_first (), 'tags' : quote . css ( 'div.tags a.tag::text' ) . extract (), } Start the crawler In [5]: process = CrawlerProcess ({ 'USER_AGENT' : 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)' }) process . crawl ( QuotesSpider ) process . start () 2017-08-02 15:22:02 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot) 2017-08-02 15:22:02 [scrapy.utils.log] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'} Out[5]: <Deferred at 0x7f8b9a41c7b8> Check the files Verify that the files has been created on disk. As we can observe the files are both created and have data. The .jl file has line separated JSON elements, while the .json file has one big JSON array containing all the quotes. In [6]: ll quoteresult .* -rw-rw-r-- 1 jitsejan 5551 Aug 2 15:22 quoteresult.jl -rw-rw-r-- 1 jitsejan 5573 Aug 2 15:22 quoteresult.json In [7]: ! tail -n 2 quoteresult.jl {\"text\": \"\\u201cA woman is like a tea bag; you never know how strong it is until it's in hot water.\\u201d\", \"author\": \"Eleanor Roosevelt\", \"tags\": [\"misattributed-eleanor-roosevelt\"]} {\"text\": \"\\u201cA day without sunshine is like, you know, night.\\u201d\", \"author\": \"Steve Martin\", \"tags\": [\"humor\", \"obvious\", \"simile\"]} In [8]: ! tail -n 2 quoteresult.json {\"text\": \"\\u201cA day without sunshine is like, you know, night.\\u201d\", \"author\": \"Steve Martin\", \"tags\": [\"humor\", \"obvious\", \"simile\"]} ] Create dataframes Pandas can now be used to create dataframes and save the frames to pickles. The .sjon file can be loaded directly into a frame, whereas for the .jl file we need to specify the JSON objects are divided per line. In [9]: import pandas as pd dfjson = pd . read_json ( 'quoteresult.json' ) dfjson Out[9]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } author tags text 0 Marilyn Monroe [friends, heartbreak, inspirational, life, lov... \"This life is what you make it. No matter what... 1 J.K. Rowling [courage, friends] \"It takes a great deal of bravery to stand up ... 2 Albert Einstein [simplicity, understand] \"If you can't explain it to a six year old, yo... 3 Bob Marley [love] \"You may not be her first, her last, or her on... 4 Dr. Seuss [fantasy] \"I like nonsense, it wakes up the brain cells.... 5 Douglas Adams [life, navigation] \"I may not have gone where I intended to go, b... 6 Elie Wiesel [activism, apathy, hate, indifference, inspira... \"The opposite of love is not hate, it's indiff... 7 Friedrich Nietzsche [friendship, lack-of-friendship, lack-of-love,... \"It is not a lack of love, but a lack of frien... 8 Mark Twain [books, contentment, friends, friendship, life] \"Good friends, good books, and a sleepy consci... 9 Allen Saunders [fate, life, misattributed-john-lennon, planni... \"Life is what happens to us while we are makin... 10 Albert Einstein [change, deep-thoughts, thinking, world] \"The world as we have created it is a process ... 11 J.K. Rowling [abilities, choices] \"It is our choices, Harry, that show what we t... 12 Albert Einstein [inspirational, life, live, miracle, miracles] \"There are only two ways to live your life. On... 13 Jane Austen [aliteracy, books, classic, humor] \"The person, be it gentleman or lady, who has ... 14 Marilyn Monroe [be-yourself, inspirational] \"Imperfection is beauty, madness is genius and... 15 Albert Einstein [adulthood, success, value] \"Try not to become a man of success. Rather be... 16 André Gide [life, love] \"It is better to be hated for what you are tha... 17 Thomas A. Edison [edison, failure, inspirational, paraphrased] \"I have not failed. I've just found 10,000 way... 18 Eleanor Roosevelt [misattributed-eleanor-roosevelt] \"A woman is like a tea bag; you never know how... 19 Steve Martin [humor, obvious, simile] \"A day without sunshine is like, you know, nig... In [10]: dfjl = pd . read_json ( 'quoteresult.jl' , lines = True ) dfjl Out[10]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } author tags text 0 Marilyn Monroe [friends, heartbreak, inspirational, life, lov... \"This life is what you make it. No matter what... 1 J.K. Rowling [courage, friends] \"It takes a great deal of bravery to stand up ... 2 Albert Einstein [simplicity, understand] \"If you can't explain it to a six year old, yo... 3 Bob Marley [love] \"You may not be her first, her last, or her on... 4 Dr. Seuss [fantasy] \"I like nonsense, it wakes up the brain cells.... 5 Douglas Adams [life, navigation] \"I may not have gone where I intended to go, b... 6 Elie Wiesel [activism, apathy, hate, indifference, inspira... \"The opposite of love is not hate, it's indiff... 7 Friedrich Nietzsche [friendship, lack-of-friendship, lack-of-love,... \"It is not a lack of love, but a lack of frien... 8 Mark Twain [books, contentment, friends, friendship, life] \"Good friends, good books, and a sleepy consci... 9 Allen Saunders [fate, life, misattributed-john-lennon, planni... \"Life is what happens to us while we are makin... 10 Albert Einstein [change, deep-thoughts, thinking, world] \"The world as we have created it is a process ... 11 J.K. Rowling [abilities, choices] \"It is our choices, Harry, that show what we t... 12 Albert Einstein [inspirational, life, live, miracle, miracles] \"There are only two ways to live your life. On... 13 Jane Austen [aliteracy, books, classic, humor] \"The person, be it gentleman or lady, who has ... 14 Marilyn Monroe [be-yourself, inspirational] \"Imperfection is beauty, madness is genius and... 15 Albert Einstein [adulthood, success, value] \"Try not to become a man of success. Rather be... 16 André Gide [life, love] \"It is better to be hated for what you are tha... 17 Thomas A. Edison [edison, failure, inspirational, paraphrased] \"I have not failed. I've just found 10,000 way... 18 Eleanor Roosevelt [misattributed-eleanor-roosevelt] \"A woman is like a tea bag; you never know how... 19 Steve Martin [humor, obvious, simile] \"A day without sunshine is like, you know, nig... In [11]: dfjson . to_pickle ( 'quotejson.pickle' ) dfjl . to_pickle ( 'quotejl.pickle' ) In [12]: ll * pickle -rw-rw-r-- 1 jitsejan 5676 Aug 2 15:22 quotejl.pickle -rw-rw-r-- 1 jitsejan 5676 Aug 2 15:22 quotejson.pickle","tags":"posts","url":"using-scrapy-in-jupyter-notebook.html"},{"title":"Extend dictionary cell to columns in Pandas dataframe","text":"df = pd . concat ([ df . drop ([ 'meta' ], axis = 1 ), df [ 'meta' ] . apply ( pd . Series )], axis = 1 )","tags":"posts","url":"extend-dictionary-cell-in-pandas.html"},{"title":"Using Python and Javascript together with Flask","text":"Introduction In this project I am experimenting with sending data between Javascript and Python using the web framework Flask. Additionally I will use matplotlib to generate a dynamic graph based on the provided user input data. Key learning points Sending data from Python to Javascript Receiving data in Python from Javascript Creating an image dynamically using a special Flask route Important bits Send the outputData from Javascript to Python with a POST call to postmethod and use the form variable canvas_data. The POST call give a response from Python and the page is redirected to the results page with the given uuid. ... $ . post ( \"/postmethod\" , { canvas_data : JSON . stringify ( outputData ) }, function ( err , req , resp ){ window . location . href = \"/results/\" + resp [ \"responseJSON\" ][ \"uuid\" ]; }); ... Retrieve the canvas_data from the POST request and write the content to a file. Return the unique id that was used for writing to the file. ... @app.route ( '/postmethod' , methods = [ 'POST' ]) def post_javascript_data (): jsdata = request . form [ 'canvas_data' ] unique_id = create_csv ( jsdata ) params = { 'uuid' : unique_id } return jsonify ( params ) ... Implementation The core of the web application is inside this file. Here I define the different routes for the website and specify the settings. The default route shows the index.html where a canvas is shown. The result route will show the image once a picture is drawn, based on the provided unique ID. The postmethod route is defined to handle the data coming from Javascript into Python via a POST call. The content of the POST variable are written to a CSV file which can be used again on the result page where data is loaded from this same file. app.py from __future__ import print_function from flask import Flask , render_template , make_response from flask import redirect , request , jsonify , url_for import io import os import uuid from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas from matplotlib.figure import Figure import numpy as np app = Flask ( __name__ ) app . secret_key = 's3cr3t' app . debug = True app . _static_folder = os . path . abspath ( \"templates/static/\" ) @app.route ( '/' , methods = [ 'GET' ]) def index (): title = 'Create the input' return render_template ( 'layouts/index.html' , title = title ) @app.route ( '/results/<uuid>' , methods = [ 'GET' ]) def results ( uuid ): title = 'Result' data = get_file_content ( uuid ) return render_template ( 'layouts/results.html' , title = title , data = data ) @app.route ( '/postmethod' , methods = [ 'POST' ]) def post_javascript_data (): jsdata = request . form [ 'canvas_data' ] unique_id = create_csv ( jsdata ) params = { 'uuid' : unique_id } return jsonify ( params ) @app.route ( '/plot/<imgdata>' ) def plot ( imgdata ): data = [ float ( i ) for i in imgdata . strip ( '[]' ) . split ( ',' )] data = np . reshape ( data , ( 200 , 200 )) fig = Figure () axis = fig . add_subplot ( 1 , 1 , 1 ) axis . axis ( 'off' ) axis . imshow ( data , interpolation = 'nearest' ) canvas = FigureCanvas ( fig ) output = io . BytesIO () canvas . print_png ( output ) response = make_response ( output . getvalue ()) response . mimetype = 'image/png' return response def create_csv ( text ): unique_id = str ( uuid . uuid4 ()) with open ( 'images/' + unique_id + '.csv' , 'a' ) as file : file . write ( text [ 1 : - 1 ] + \" \\n \" ) return unique_id def get_file_content ( uuid ): with open ( 'images/' + uuid + '.csv' , 'r' ) as file : return file . read () if __name__ == '__main__' : app . run ( host = '0.0.0.0' , port = 5000 ) The second part of the magic happens in the Javascript file. In this file a canvas is generated and added to the DOM. The mouse is used to draw dots on the canvas with a predefined color and radius. One button is used to send the data of the current drawing on the canvas and another one is used to clear the canvas. templates/static/js/script/js $ ( document ). ready ( function () { function createCanvas ( parent , width , height ) { var canvas = document . getElementById ( \"inputCanvas\" ); canvas . context = canvas . getContext ( '2d' ); return canvas ; } function init ( container , width , height , fillColor ) { var canvas = createCanvas ( container , width , height ); var ctx = canvas . context ; ctx . fillCircle = function ( x , y , radius , fillColor ) { this . fillStyle = fillColor ; this . beginPath (); this . moveTo ( x , y ); this . arc ( x , y , radius , 0 , Math . PI * 2 , false ); this . fill (); }; ctx . clearTo = function ( fillColor ) { ctx . fillStyle = fillColor ; ctx . fillRect ( 0 , 0 , width , height ); }; ctx . clearTo ( \"#fff\" ); canvas . onmousemove = function ( e ) { if ( ! canvas . isDrawing ) { return ; } var x = e . pageX - this . offsetLeft ; var y = e . pageY - this . offsetTop ; var radius = 10 ; var fillColor = 'rgb(102,153,255)' ; ctx . fillCircle ( x , y , radius , fillColor ); }; canvas . onmousedown = function ( e ) { canvas . isDrawing = true ; }; canvas . onmouseup = function ( e ) { canvas . isDrawing = false ; }; } var container = document . getElementById ( 'canvas' ); init ( container , 200 , 200 , '#ddd' ); function clearCanvas () { var canvas = document . getElementById ( \"inputCanvas\" ); var ctx = canvas . getContext ( \"2d\" ); ctx . clearRect ( 0 , 0 , canvas . width , canvas . height ); } function getData () { var canvas = document . getElementById ( \"inputCanvas\" ); var imageData = canvas . context . getImageData ( 0 , 0 , canvas . width , canvas . height ); var data = imageData . data ; var outputData = [] for ( var i = 0 ; i < data . length ; i += 4 ) { var brightness = 0.34 * data [ i ] + 0.5 * data [ i + 1 ] + 0.16 * data [ i + 2 ]; outputData . push ( brightness ); } $ . post ( \"/postmethod\" , { canvas_data : JSON . stringify ( outputData ) }, function ( err , req , resp ){ window . location . href = \"/results/\" + resp [ \"responseJSON\" ][ \"uuid\" ]; }); } $ ( \"#clearButton\" ). click ( function (){ clearCanvas (); }); $ ( \"#sendButton\" ). click ( function (){ getData (); }); }); Finally we need to define a base template to be used by the index and result page. I know this could be split up nicer and I could make better use of the templating engine, but for this experiment it seemed sufficient. templates/layouts/base.html <!doctype html> < html > < head > {% block head %} < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" > < link rel = \"stylesheet\" href = \"/static/css/style.css\" > < script src = \"https://code.jquery.com/jquery-2.1.4.min.js\" ></ script > < script src = \"/static/js/script.js\" ></ script > < title > {% block title %}{% endblock %} - Simple Flask app </ title > {% endblock %} </ head > < body > < div class = \"container\" > < nav class = \"navbar navbar-default\" role = \"navigation\" > < div class = \"navbar-header\" > < button type = \"button\" class = \"navbar-toggle\" data-toggle = \"collapse\" data-target = \".navbar-collapse\" > < span class = \"icon-bar\" ></ span > < span class = \"icon-bar\" ></ span > < span class = \"icon-bar\" ></ span > </ button > </ div > < a class = \"navbar-brand\" href = \"#\" ></ a > < div class = \"navbar-collapse collapse\" > < ul class = \"nav navbar-nav navbar-right\" > < li >< a href = \"/\" > Home </ a ></ li > < li >< a href = \"/results\" > Results </ a ></ li > </ ul > </ div > </ nav > </ div > < div id = \"content\" class = \"container main-container\" > {% block content %}{% endblock %} </ div > < div id = \"footer\" class = \"container text-center\" > {% block footer %} &copy; Copyright 2017 by Jitse-Jan. {% endblock %} </ div > < footer > < script src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\" ></ script > </ footer > </ body > </ html > The code for the index.html and results.html can be kept to a minimum this way. templates/layouts/index.html {% extends \"layouts/base.html\" %} {% block title %}{{title}}{% endblock %} {% block head %} {{ super() }} {% endblock %} {% block content %} < div class = \"row text-center\" > < h1 > {{ title}} </ h1 > < canvas id = \"inputCanvas\" width = \"200\" height = \"200\" ></ canvas > </ div > < br />< br /> < div class = \"row text-center\" > < button class = \"btn btn-primary\" id = \"clearButton\" > Clear </ button > < button class = \"btn btn-primary\" id = \"sendButton\" > Send data </ button > </ div > {% endblock %} templates/layouts/result.html {% extends \"layouts/base.html\" %} {% block title %}{{title}}{% endblock %} {% block head %} {{ super() }} {% endblock %} {% block content %} < div class = \"row text-center\" > < h1 > Results </ h1 > < img src = \"{{ url_for('plot', imgdata = data) }}\" alt = \"Image Placeholder\" height = \"500\" > </ div > {% endblock %} Important: Please note that for the source of the image the specific URL for the matplotlib image is used. The route for plot is called with the parameter imgdata containing the data. I have kept the stylesheet very basic since this project is not aimed at making the most slick interface. templates/static/css/style.css . btn { background-color : rgb ( 102 , 153 , 255 ); } # inputCanvas { border : 2 px solid rgb ( 102 , 153 , 255 ); } # footer { margin-top : 100 px ; } After putting all the files together the application can be started and visited on port 5000 on the localhost. ~/code/flask-app $ FLASK_APP = app.py FLASK_DEBUG = 1 flask run","tags":"posts","url":"python-and-javascript-in-flask.html"},{"title":"Create an API using Eve in Python","text":"/Users/jitsejan/code $ mkdir eve-api /Users/jitsejan/code $ cd eve-api/ /Users/jitsejan/code/eve-api $ python3 -m pip install eve /Users/jitsejan/code/eve-api $ touch app.py /Users/jitsejan/code/eve-api $ sublime app.py api.py from eve import Eve import settings app = Eve ( settings = settings . settings ) if __name__ == '__main__' : app . run () /Users/jitsejan/code/eve-api $ touch settings.py /Users/jitsejan/code/eve-api $ sublime settings.py settings.py character = { 'schema' : { 'name' : { 'type' : 'string' }, 'color' : { 'type' : 'string' }, 'superpower' : { 'type' : 'string' }, }, } settings = { 'MONGO_HOST' : 'localhost' , 'MONGO_DBNAME' : 'nintendo-database' , 'MONGO_USERNAME' : 'db-user' , 'MONGO_PASSWORD' : 'db-pass' , 'RESOURCE_METHODS' : [ 'GET' ], 'DOMAIN' : { 'character' : character , }, } /Users/jitsejan/code/eve-api $ python app.py Use Postman to connect to localhost:5000 and start using your API. Since the character schema has been defined, the characters are listed on localhost:5000/character. You can simply search by name and only retrieve a specific field using additional parameters. localhost : 5000 / character ? where ={ \"name\" : \"Mario\" }& projection ={ \"name\" : 1 , \"superpower\" : 1 }","tags":"posts","url":"creating-api-using-eve-and-mongodb.html"},{"title":"Using Pythons pickle to save and load variables","text":"Recently I was playing with some code that generated big dictionaries and had to manipulate these dictonaries several times. I used to save them via Pythons pandas to CSV and load them back from the CSV the next time I was using my script. Luckily I found out an easier way to deal with saving and loading variables, namely by using Pickle . Import import pickle Saving pickle . dump ( variable , open ( picklename , 'wb' )) Loading pickle . load ( open ( picklename , \"rb\" ) )","tags":"posts","url":"using-pythons-pickle-to-save-and-load-variables.html"},{"title":"Fancy select boxes using FontAwesome","text":"See the example on my bl.ocks.org . index.html The necessary JS and CSS files are included. Two select boxes are added to the main container. < head > < script type = \"text/javascript\" src = \"https://code.jquery.com/jquery-3.2.1.min.js\" ></ script > < script type = \"text/javascript\" src = \"https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js\" ></ script > < script type = \"text/javascript\" src = \"script.js\" ></ script > < link rel = \"stylesheet\" type = \"text/css\" href = \"https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css\" > < link rel = \"stylesheet\" type = \"text/css\" href = \"https://netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css\" > < link rel = \"stylesheet\" type = \"text/css\" href = \"style.css\" > </ head > < body > < div class = \"container\" > < div class = \"row\" > < h1 > Drinks </ h1 > < div class = \"input-group col-md-12\" > < input id = \"coffee_cb\" type = \"checkbox\" class = \"coffee drink_cb\" checked /> < label for = \"coffee_cb\" ></ label > < input id = \"wine_cb\" type = \"checkbox\" class = \"wine drink_cb\" checked /> < label for = \"wine_cb\" ></ label > </ div > </ div > < div class = \"row\" > < h1 > Selected filter </ h1 > < table > < tr > < th > Coffee </ th > < td id = \"filterValCoffee\" ></ td > </ tr > < tr > < th > Wine </ th > < td id = \"filterValWine\" ></ td > </ tr > </ table > </ div > </ div > </ body > style.css The checkbox itself is hidden and a background using FontAwesome is used instead. body { margin : 30 px ; } input [ type = checkbox ] { display : none ; } input [ type = checkbox ] + label { color : black ; font-size : 28 px ; } input [ type = checkbox ] + label : before { font-family : FontAwesome ; display : inline-block ; width : 50 px ; height : 50 px ; padding : 2 px ; background-color : white ; text-align : center ; -webkit- border-radius : 50 % ; -moz- border-radius : 50 % ; border-radius : 50 % ; border : black 2 px solid ; margin : 5 px ; } input [ type = checkbox ] . coffee + label : before { content : \"\\f0f4\" ; } input [ type = checkbox ] . wine + label : before { content : \"\\f000\" ; } input [ type = checkbox ] : checked + label : before { color : white ; background-color : black ; } script.js Retrieve the value of the checkboxes and set the HTML of its corresponding element. $ ( document ). ready ( function () { function setItemValues () { $ ( \"#coffee_cb\" ). is ( \":checked\" ) ? coffeeCheck = 'Yes' : coffeeCheck = 'No' ; $ ( \"#wine_cb\" ). is ( \":checked\" ) ? wineCheck = 'Yes' : wineCheck = 'No' ; $ ( '#filterValCoffee' ). html ( coffeeCheck ); $ ( '#filterValWine' ). html ( wineCheck ); } $ ( '.drink_cb' ). change ( function () { setItemValues (); }); setItemValues (); });","tags":"posts","url":"select-boxes-with-font-awesome.html"},{"title":"Using D3.js in Jupyter notebook","text":"D3.js and Jupyter A short description how to use D3.js in a Jupyter notebook. Input data Lets create a CSV file containing the amounts of 5 crypto currencies over a small period. In [1]: csvstring = \"\"\" Time;BTC;DOGE;ETH;LTC;REP 2017-05-03 23:17;19,70;;78,88;20,81; 2017-05-04 20:18;21,21;;90,45;24,91; 2017-05-05 19:11;20,1;11,58;91,73;24,06; 2017-05-06 18:56;20,28;12,37;92,78;25,91; 2017-05-09 0:50;21,77;20,34;89,27;28,45; 2017-05-09 2:15;21,99;20,58;88,62;28,49;34,70 2017-05-09 23:59;22,46;17,51;87,45;30,14;32,45 2017-05-11 0:57;23,15;18,83;86,94;32,18;34,36 2017-05-11 22:17;24,17;17,48;87,87;29,62;34,36 2017-05-12 1:55;24,13;17,99;88,05;30,02;36,08 2017-05-13 1:57;22,41;17,48;85,25;27,00;33,72 2017-05-14 15:32;23,67;17,04;89,19;28,90;34,24 2017-05-14 23:34;23,47;17,51;88,60;28,01;33,72 2017-05-15 22:12;22,34;16,16;90,20;24,43;32,58 2017-05-16 20:25;23,12;15,68;88,54;24,19;31,36 2017-05-17 22:01;24,00;19,26;86,28;24,50;30,62 2017-05-18 23:45;24,84;21,05;94,93;27,93;32,62 2017-05-19 22:01;25,59;21,98;118,55;27,03;33,62\"\"\" Write the CSV text fo a file. In [2]: ! echo \" $csvstring \" > wallet.csv Check if the file exists. In [3]: ! ls -la wallet.csv -rw-rw-r-- 1 jitsejan jitsejan 833 Aug 2 10:38 wallet.csv Check the last entry of the CSV file. In [4]: ! tail -n 1 wallet.csv 2017-05-19 22:01;25,59;21,98;118,55;27,03;33,62 Read the file into a dataframe. In [5]: import pandas as pd wallet_data = pd . read_csv ( 'wallet.csv' , sep = ';' , decimal = \",\" ) wallet_data = wallet_data . fillna ( 0 ) wallet_data . head () Out[5]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Time BTC DOGE ETH LTC REP 0 2017-05-03 23:17 19.70 0.00 78.88 20.81 0.0 1 2017-05-04 20:18 21.21 0.00 90.45 24.91 0.0 2 2017-05-05 19:11 20.10 11.58 91.73 24.06 0.0 3 2017-05-06 18:56 20.28 12.37 92.78 25.91 0.0 4 2017-05-09 0:50 21.77 20.34 89.27 28.45 0.0 Add a total column for the currency columns. In [6]: wallet_data [ 'total' ] = wallet_data . ix [:, wallet_data . columns != 'Time' ] . sum ( axis = 1 ) /home/jitsejan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: .ix is deprecated. Please use .loc for label based indexing or .iloc for positional indexing See the documentation here: http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix \"\"\"Entry point for launching an IPython kernel. In [7]: wallet_data Out[7]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Time BTC DOGE ETH LTC REP total 0 2017-05-03 23:17 19.70 0.00 78.88 20.81 0.00 119.39 1 2017-05-04 20:18 21.21 0.00 90.45 24.91 0.00 136.57 2 2017-05-05 19:11 20.10 11.58 91.73 24.06 0.00 147.47 3 2017-05-06 18:56 20.28 12.37 92.78 25.91 0.00 151.34 4 2017-05-09 0:50 21.77 20.34 89.27 28.45 0.00 159.83 5 2017-05-09 2:15 21.99 20.58 88.62 28.49 34.70 194.38 6 2017-05-09 23:59 22.46 17.51 87.45 30.14 32.45 190.01 7 2017-05-11 0:57 23.15 18.83 86.94 32.18 34.36 195.46 8 2017-05-11 22:17 24.17 17.48 87.87 29.62 34.36 193.50 9 2017-05-12 1:55 24.13 17.99 88.05 30.02 36.08 196.27 10 2017-05-13 1:57 22.41 17.48 85.25 27.00 33.72 185.86 11 2017-05-14 15:32 23.67 17.04 89.19 28.90 34.24 193.04 12 2017-05-14 23:34 23.47 17.51 88.60 28.01 33.72 191.31 13 2017-05-15 22:12 22.34 16.16 90.20 24.43 32.58 185.71 14 2017-05-16 20:25 23.12 15.68 88.54 24.19 31.36 182.89 15 2017-05-17 22:01 24.00 19.26 86.28 24.50 30.62 184.66 16 2017-05-18 23:45 24.84 21.05 94.93 27.93 32.62 201.37 17 2017-05-19 22:01 25.59 21.98 118.55 27.03 33.62 226.77 Convert the data to a dictionary that Javascript can use. In [8]: wallet_data = wallet_data . to_json ( orient = 'records' ) Attach the data to the current window by using Javascript. In [9]: from IPython.display import Javascript Javascript ( \"\"\" window.walletData= {} ; \"\"\" . format ( wallet_data )) Out[9]: var element = $('#47833c23-6e5b-4218-b2c3-6c0f6719ab8a'); window.walletData=[{\"Time\":\"2017-05-03 23:17\",\"BTC\":19.7,\"DOGE\":0.0,\"ETH\":78.88,\"LTC\":20.81,\"REP\":0.0,\"total\":119.39},{\"Time\":\"2017-05-04 20:18\",\"BTC\":21.21,\"DOGE\":0.0,\"ETH\":90.45,\"LTC\":24.91,\"REP\":0.0,\"total\":136.57},{\"Time\":\"2017-05-05 19:11\",\"BTC\":20.1,\"DOGE\":11.58,\"ETH\":91.73,\"LTC\":24.06,\"REP\":0.0,\"total\":147.47},{\"Time\":\"2017-05-06 18:56\",\"BTC\":20.28,\"DOGE\":12.37,\"ETH\":92.78,\"LTC\":25.91,\"REP\":0.0,\"total\":151.34},{\"Time\":\"2017-05-09 0:50\",\"BTC\":21.77,\"DOGE\":20.34,\"ETH\":89.27,\"LTC\":28.45,\"REP\":0.0,\"total\":159.83},{\"Time\":\"2017-05-09 2:15\",\"BTC\":21.99,\"DOGE\":20.58,\"ETH\":88.62,\"LTC\":28.49,\"REP\":34.7,\"total\":194.38},{\"Time\":\"2017-05-09 23:59\",\"BTC\":22.46,\"DOGE\":17.51,\"ETH\":87.45,\"LTC\":30.14,\"REP\":32.45,\"total\":190.01},{\"Time\":\"2017-05-11 0:57\",\"BTC\":23.15,\"DOGE\":18.83,\"ETH\":86.94,\"LTC\":32.18,\"REP\":34.36,\"total\":195.46},{\"Time\":\"2017-05-11 22:17\",\"BTC\":24.17,\"DOGE\":17.48,\"ETH\":87.87,\"LTC\":29.62,\"REP\":34.36,\"total\":193.5},{\"Time\":\"2017-05-12 1:55\",\"BTC\":24.13,\"DOGE\":17.99,\"ETH\":88.05,\"LTC\":30.02,\"REP\":36.08,\"total\":196.27},{\"Time\":\"2017-05-13 1:57\",\"BTC\":22.41,\"DOGE\":17.48,\"ETH\":85.25,\"LTC\":27.0,\"REP\":33.72,\"total\":185.86},{\"Time\":\"2017-05-14 15:32\",\"BTC\":23.67,\"DOGE\":17.04,\"ETH\":89.19,\"LTC\":28.9,\"REP\":34.24,\"total\":193.04},{\"Time\":\"2017-05-14 23:34\",\"BTC\":23.47,\"DOGE\":17.51,\"ETH\":88.6,\"LTC\":28.01,\"REP\":33.72,\"total\":191.31},{\"Time\":\"2017-05-15 22:12\",\"BTC\":22.34,\"DOGE\":16.16,\"ETH\":90.2,\"LTC\":24.43,\"REP\":32.58,\"total\":185.71},{\"Time\":\"2017-05-16 20:25\",\"BTC\":23.12,\"DOGE\":15.68,\"ETH\":88.54,\"LTC\":24.19,\"REP\":31.36,\"total\":182.89},{\"Time\":\"2017-05-17 22:01\",\"BTC\":24.0,\"DOGE\":19.26,\"ETH\":86.28,\"LTC\":24.5,\"REP\":30.62,\"total\":184.66},{\"Time\":\"2017-05-18 23:45\",\"BTC\":24.84,\"DOGE\":21.05,\"ETH\":94.93,\"LTC\":27.93,\"REP\":32.62,\"total\":201.37},{\"Time\":\"2017-05-19 22:01\",\"BTC\":25.59,\"DOGE\":21.98,\"ETH\":118.55,\"LTC\":27.03,\"REP\":33.62,\"total\":226.77}]; Create the graph First include the D3.js library. In [1]: %% javascript require . config ({ paths : { d3 : '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min' } }); var element = $('#fdbe76d1-a468-4b02-a7e5-d47373928059'); require.config({ paths: { d3: 'https://api.tiles.mapbox.com/mapbox-gl-js/v0.39.1/mapbox-gl' } }); I copied the content of my gist to recreate the graph inside this notebook. Use the HTML function of IPython to attach inline style to the page. In [11]: from IPython.core.display import HTML HTML ( \"\"\" <style> path { stroke-width: 1; fill: none; stroke-linejoin: round; stroke-linecap: round; } circle { stroke-width: 1; } .axis path, .axis line { fill: none; stroke: grey; stroke-width: 1; shape-rendering: crispEdges; } .legend, .label, .hover-text{ font-size: x-small; background-color: white; } </style> \"\"\" ) Out[11]: path { stroke-width: 1; fill: none; stroke-linejoin: round; stroke-linecap: round; } circle { stroke-width: 1; } .axis path, .axis line { fill: none; stroke: grey; stroke-width: 1; shape-rendering: crispEdges; } .legend, .label, .hover-text{ font-size: x-small; background-color: white; } Create the graph by retrieving the data from the window. In [12]: %% javascript require ([ 'd3' ], function ( d3 ) { //a weird idempotency thing $ ( \"#chart1\" ). remove (); //create canvas element . append ( \"<svg id='chart1' width='960' height='500'></svg>\" ); var svg = d3 . select ( 'svg' ), margin = { top : 20 , right : 50 , bottom : 100 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); // Graph title g . append ( 'text' ) . attr ( 'x' , ( width / 2 )) . attr ( 'y' , 0 - ( margin . top / 3 )) . attr ( 'text-anchor' , 'middle' ) . style ( 'font-size' , '16px' ) . text ( 'Wallet chart' ); // Function to convert a string into a time var parseTime = d3 . time . format ( '%Y-%m-%d %H:%M' ). parse ; // Function to show specific time format var formatTime = d3 . time . format ( '%e %B' ); // Set data var data = window . walletData ; data . forEach ( function ( d ) { d . date = parseTime ( d . Time ); }); // Set the X scale var x = d3 . time . scale (). range ([ 0 , width ], 0.5 ); // Set the Y scale var y = d3 . scale . linear (). range ([ height , 0 ]); // Set the color scale var color = d3 . scale . category10 (); var xAxis = d3 . svg . axis () . scale ( x ) . orient ( \"bottom\" ); var yAxis = d3 . svg . axis () . scale ( y ) . orient ( \"left\" ); var line = d3 . svg . line () // .interpolate(\"basis\") . x ( function ( d ) { return x ( d . date ); }) . y ( function ( d ) { return y ( d . worth ); }); color . domain ( d3 . keys ( data [ 0 ]). filter ( function ( key ) { return key !== \"Time\" && key !== \"date\" ; })); var currencies = color . domain (). map ( function ( name ) { return { name : name , values : data . map ( function ( d ) { return { date : d . date , worth : + d [ name ] }; }) }; }); x . domain ( d3 . extent ( data , function ( d ) { return d . date ; })); // Set the Y domain y . domain ([ d3 . min ( currencies , function ( c ) { return d3 . min ( c . values , function ( v ) { return v . worth ; }); }), d3 . max ( currencies , function ( c ) { return d3 . max ( c . values , function ( v ) { return v . worth ; }); }) ]); // Set the X axis g . append ( \"g\" ) . attr ( \"class\" , \"x axis\" ) // .attr(\"fill\", \"none\") . attr ( \"transform\" , \"translate(0,\" + height + \")\" ) . call ( xAxis ); // Set the Y axis g . append ( \"g\" ) . attr ( \"class\" , \"y axis\" ) . call ( yAxis ) . append ( \"text\" ) . attr ( \"transform\" , \"rotate(-90)\" ) . attr ( \"y\" , 6 ) . attr ( \"dy\" , \".71em\" ) . style ( \"text-anchor\" , \"end\" ) . text ( \"Value (USD)\" ); // Draw the lines var currency = g . selectAll ( \".currency\" ) . data ( currencies ) . enter (). append ( \"g\" ) . attr ( \"class\" , \"currency\" ); currency . append ( \"path\" ) . attr ( \"class\" , \"line\" ) . attr ( \"fill\" , \"none\" ) . attr ( \"d\" , function ( d ) { return line ( d . values ); }) . style ( \"stroke\" , function ( d ) { return color ( d . name ); }); // Add the circles currency . append ( \"g\" ). selectAll ( \"circle\" ) . data ( function ( d ) { return d . values }) . enter () . append ( \"circle\" ) . attr ( \"r\" , 2 ) . attr ( \"cx\" , function ( dd ) { return x ( dd . date ) }) . attr ( \"cy\" , function ( dd ) { return y ( dd . worth ) }) . attr ( \"fill\" , \"none\" ) . attr ( \"stroke\" , function ( d ) { return color ( this . parentNode . __data__ . name ) }); // Add label to the end of the line currency . append ( \"text\" ) . attr ( \"class\" , \"label\" ) . datum ( function ( d ) { return { name : d . name , value : d . values [ d . values . length - 1 ] }; }) . attr ( \"transform\" , function ( d ) { return \"translate(\" + x ( d . value . date ) + \",\" + y ( d . value . worth ) + \")\" ; }) . attr ( \"x\" , 3 ) . attr ( \"dy\" , \".35em\" ) . text ( function ( d ) { return d . name ; }); }); var element = $('#c03185f6-2462-43cf-8785-ce2d4c28efa9'); require(['d3'], function(d3) { //a weird idempotency thing $(\"#chart1\").remove(); //create canvas element.append(\"<svg id='chart1' width='960' height='500'></svg>\"); var svg = d3.select('svg'), margin = { top: 20, right: 50, bottom: 100, left: 50 }, width = +svg.attr('width') - margin.left - margin.right, height = +svg.attr('height') - margin.top - margin.bottom, g = svg.append('g').attr('transform', 'translate(' + margin.left + ',' + margin.top + ')'); // Graph title g.append('text') .attr('x', (width / 2)) .attr('y', 0 - (margin.top / 3)) .attr('text-anchor', 'middle') .style('font-size', '16px') .text('Wallet chart'); // Function to convert a string into a time var parseTime = d3.time.format('%Y-%m-%d %H:%M').parse; // Function to show specific time format var formatTime = d3.time.format('%e %B'); // Set data var data = window.walletData; data.forEach(function(d) { d.date = parseTime(d.Time); }); // Set the X scale var x = d3.time.scale().range([0, width], 0.5); // Set the Y scale var y = d3.scale.linear().range([height, 0]); // Set the color scale var color = d3.scale.category10(); var xAxis = d3.svg.axis() .scale(x) .orient(\"bottom\"); var yAxis = d3.svg.axis() .scale(y) .orient(\"left\"); var line = d3.svg.line() // .interpolate(\"basis\") .x(function(d) { return x(d.date); }) .y(function(d) { return y(d.worth); }); color.domain(d3.keys(data[0]).filter(function(key) { return key !== \"Time\" && key !== \"date\"; })); var currencies = color.domain().map(function(name) { return { name: name, values: data.map(function(d) { return { date: d.date, worth: +d[name] }; }) }; }); x.domain(d3.extent(data, function(d) { return d.date; })); // Set the Y domain y.domain([ d3.min(currencies, function(c) { return d3.min(c.values, function(v) { return v.worth; }); }), d3.max(currencies, function(c) { return d3.max(c.values, function(v) { return v.worth; }); }) ]); // Set the X axis g.append(\"g\") .attr(\"class\", \"x axis\") // .attr(\"fill\", \"none\") .attr(\"transform\", \"translate(0,\" + height + \")\") .call(xAxis); // Set the Y axis g.append(\"g\") .attr(\"class\", \"y axis\") .call(yAxis) .append(\"text\") .attr(\"transform\", \"rotate(-90)\") .attr(\"y\", 6) .attr(\"dy\", \".71em\") .style(\"text-anchor\", \"end\") .text(\"Value (USD)\"); // Draw the lines var currency = g.selectAll(\".currency\") .data(currencies) .enter().append(\"g\") .attr(\"class\", \"currency\"); currency.append(\"path\") .attr(\"class\", \"line\") .attr(\"fill\", \"none\") .attr(\"d\", function(d) { return line(d.values); }) .style(\"stroke\", function(d) { return color(d.name); }); // Add the circles currency.append(\"g\").selectAll(\"circle\") .data(function(d) { return d.values }) .enter() .append(\"circle\") .attr(\"r\", 2) .attr(\"cx\", function(dd) { return x(dd.date) }) .attr(\"cy\", function(dd) { return y(dd.worth) }) .attr(\"fill\", \"none\") .attr(\"stroke\", function(d) { return color(this.parentNode.__data__.name) }); // Add label to the end of the line currency.append(\"text\") .attr(\"class\", \"label\") .datum(function(d) { return { name: d.name, value: d.values[d.values.length - 1] }; }) .attr(\"transform\", function(d) { return \"translate(\" + x(d.value.date) + \",\" + y(d.value.worth) + \")\"; }) .attr(\"x\", 3) .attr(\"dy\", \".35em\") .text(function(d) { return d.name; }); });","tags":"posts","url":"using-d3-in-jupyter-notebook.html"},{"title":"Building a crypto app with ExpressJS, MongoDB and D3.js","text":"In this post I will describe my initial version of my crypto app, an application where I will simply show some data of my experiments with crypto currencies. Data is handled by Python, put in MongoDB and displayed using ExpressJS and D3.js. The post is a bit long, but I try give my steps as clear as possible so it can be of any help to anyone. Structure of the application The final structure of the app will look like the following. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app$ tree . ├── app.js ├── data │ ├── mining.csv │ └── wallet.csv ├── notebook │ └── CSV to MongoDB.ipynb ├── package.json ├── public │ └── css │ └── style.css └── views ├── pages │ ├── index.ejs │ ├── mining.ejs │ └── wallet.ejs └── partials ├── footer.ejs ├── head.ejs └── header.ejs 7 directories, 11 files Version check jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ git --version git version 2 .11.0 ( Apple Git-81 ) jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ npm -v 4 .2.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ node -v v7.10.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ python --version Python 3 .6.0 :: Anaconda 4 .3.1 ( x86_64 ) jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ jupyter --version 4 .2.1 Initialize jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ git init jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ npm init jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ echo node_modules >> .gitignore jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ git add . && git commit -am \"Initial commit\" Create the back-end using a Jupyter notebook Using Python we will read two CSV files and add them to MongoDB. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ echo notebook/.ipynb_checkpoints >> .gitignore jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app/notebook $ jupyter notebook Content of the notebook Convert CSV to MongoDB Reading the data We have two CSV files. One containing the amount of currency my simple miner is gathering. Another one containing the worth of my crypto wallet where I bought a few small amounts of crypto currencies. In [1]: ! ls ../data/ mining.csv wallet.csv In [2]: import pandas as pd pd . __version__ Out[2]: '0.19.2' Mining data In [3]: # Read data to dataframe mining_df = pd . read_csv ( '../data/mining.csv' , sep = ';' ) # Fill missing numbers mining_df . fillna ( 0 , inplace = True ) mining_df Out[3]: Date DRK BTC LTC 0 2017-04-28 0.011367 0.000000 0.000000 1 2017-04-29 0.010086 0.000000 0.000000 2 2017-04-30 0.011435 0.000000 0.000000 3 2017-05-01 0.012280 0.000000 0.000000 4 2017-05-02 0.012998 0.000000 0.000000 5 2017-05-03 0.010910 0.000000 0.000000 6 2017-05-04 0.010627 0.000004 0.000317 7 2017-05-05 0.007488 0.000573 0.003707 8 2017-05-06 0.007381 0.000149 0.008580 9 2017-05-07 0.007243 0.000152 0.008722 10 2017-05-08 0.005381 0.000117 0.006145 11 2017-05-09 0.007240 0.000146 0.008197 12 2017-05-10 0.008117 0.000136 0.008230 13 2017-05-11 0.006478 0.000114 0.005798 14 2017-05-12 0.000391 0.000548 0.000388 15 2017-05-13 0.000000 0.000527 0.000000 16 2017-05-14 0.002847 0.000357 0.000000 In [4]: mining_dict = mining_df . to_dict ( orient = 'records' ) mining_dict [ 0 ] Out[4]: {'BTC': 0.0, 'DRK': 0.01136728, 'Date': '2017-04-28', 'LTC': 0.0} Wallet data In [5]: # Read data to dataframe wallet_df = pd . read_csv ( '../data/wallet.csv' , sep = ';' , decimal = \",\" ) # Fill missing numbers wallet_df . fillna ( 0 , inplace = True ) # Convert types wallet_df [ 'Time' ] = pd . to_datetime ( wallet_df [ 'Time' ]) wallet_df Out[5]: Time BTC DOGE ETH LTC REP 0 2017-05-03 23:17:00 19.70 0.00 78.88 20.81 0.00 1 2017-05-04 20:18:00 21.21 0.00 90.45 24.91 0.00 2 2017-05-05 19:11:00 20.10 11.58 91.73 24.06 0.00 3 2017-05-06 18:56:00 20.28 12.37 92.78 25.91 0.00 4 2017-05-09 00:50:00 21.77 20.34 89.27 28.45 0.00 5 2017-05-09 02:15:00 21.99 20.58 88.62 28.49 34.70 6 2017-05-09 23:59:00 22.46 17.51 87.45 30.14 32.45 7 2017-05-11 00:57:00 23.15 18.83 86.94 32.18 34.36 8 2017-05-11 22:17:00 24.17 17.48 87.87 29.62 34.36 9 2017-05-12 01:55:00 24.13 17.99 88.05 30.02 36.08 10 2017-05-13 01:57:00 22.41 17.48 85.25 27.00 33.72 11 2017-05-14 15:32:00 23.67 17.04 89.19 28.90 34.24 12 2017-05-14 23:34:00 23.47 17.51 88.60 28.01 33.72 13 2017-05-15 22:12:00 22.34 16.16 90.20 24.43 32.58 In [6]: wallet_dict = wallet_df . to_dict ( orient = 'records' ) wallet_dict [ 0 ] Out[6]: {'BTC': 19.7, 'DOGE': 0.0, 'ETH': 78.88, 'LTC': 20.81, 'REP': 0.0, 'Time': Timestamp('2017-05-03 23:17:00')} Saving the data Import the Pymongo library and connect to MongoDB In [7]: try : import pymongo except : ! pip install pymongo import pymongo print ( pymongo . version ) client = pymongo . MongoClient ( 'mongodb://localhost:27017/' ) 3.4.0 Create a new database for the data In [8]: db = client [ 'crypto-data' ] Create a new table for the mining data In [9]: minings = db . minings Delete the old data In [10]: minings . delete_many ({}) Out[10]: <pymongo.results.DeleteResult at 0x118567120> Insert the data by looping through the data In [11]: for item in mining_dict : minings . insert_one ( item ) Check the existing data In [12]: minings_in_database = list ( minings . find ({})) minings_in_database [ 0 ] Out[12]: {'BTC': 0.0, 'DRK': 0.01136728, 'Date': '2017-04-28', 'LTC': 0.0, '_id': ObjectId('591a36700a4693e45bb97079')} Repeat this for the wallet data In [13]: wallets = db . wallets wallets . delete_many ({}) for item in wallet_dict : wallets . insert_one ( item ) wallets_in_database = list ( wallets . find ({})) wallets_in_database [ 0 ] Out[13]: {'BTC': 19.7, 'DOGE': 0.0, 'ETH': 78.88, 'LTC': 20.81, 'REP': 0.0, 'Time': datetime.datetime(2017, 5, 3, 23, 17), '_id': ObjectId('591a36700a4693e45bb9708a')} Now the data is in the database we are ready to create the front-end. Create the front-end First install the packages we need and save them to package.json . jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ npm install --save express mongoose ejs mongodb This will result in the following package.json . { \"name\" : \"crypto-app\" , \"version\" : \"1.0.0\" , \"description\" : \"Simple app to track some crypto investments\" , \"main\" : \"app.js\" , \"scripts\" : { \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\" : [ \"crypto\" , \"Python\" , \"mongoose\" , \"expressjs\" ], \"author\" : \"jitsejan\" , \"license\" : \"ISC\" , \"dependencies\" : { \"ejs\" : \"&#94;2.5.6\" , \"express\" : \"&#94;4.15.2\" , \"mongodb\" : \"&#94;2.2.26\" , \"mongoose\" : \"&#94;4.9.9\" } } The core I will create three pages. A blank frontpage, a page for the wallet data and a page for the mining data. First we need setup the application in app.js . This file is responsible for the database connection and serving the templates for each route containing the correct data. // Requirements var express = require ( 'express' ); var app = express (); var mongoose = require ( 'mongoose' ); // Make sure we can use HTML and JavaScript interchangeably app . set ( 'view engine' , 'ejs' ); // Database connection mongoose . connect ( 'mongodb://localhost/crypto-data' ); var db = mongoose . connection ; db . on ( 'error' , console . error . bind ( console , 'connection error:' )); db . once ( 'open' , function callback () { console . log ( 'Connected to Mongo database' ); }); // Define the schema using Mongoose var Schema = mongoose . Schema ; // The Mining schema should be the same as the data we put in Python var miningSchema = new Schema ({ Date : Date , BTC : Number , DRK : Number , LTC : Number }); // Create the model var Mining = db . model ( 'mining' , miningSchema ); // The Wallet schema should be the same as the data we put in Python var walletSchema = new Schema ({ Time : Date , BTC : Number , DOGE : Number , ETH : Number , LTC : Number , REP : Number }); // Create the model var Wallet = db . model ( 'wallet' , walletSchema ); // Create the route for the frontpage app . get ( '/' , function ( req , res ) { res . render ( 'pages/index' , { title : 'Home' }); }); // Create the route to the mining page app . get ( '/mining' , function ( req , res ) { Mining . find ({}, null , { sort : { 'Date' :+ 1 }}, function ( err , minings ){ console . log ( minings ); res . render ( 'pages/mining' , { title : 'Mining' , minings : minings }); }) }); // Create the route to the wallet page app . get ( '/wallet' , function ( req , res ) { Wallet . find ({}, null , { sort : { 'Time' :+ 1 }}, function ( err , wallets ){ console . log ( wallets ); res . render ( 'pages/wallet' , { title : 'Wallet' , wallets : wallets }); }) }); // Define the public directory (where the stylesheet lives) // Normally this would be a subdirectory 'public/css/' app . use ( express . static ( __dirname )); // Start the app on port 3000 app . listen ( 3000 ); console . log ( 'listening on port 3000' ); Partials To easily create templates for the different pages, I will first create the partials for the head, footer and header. I will use Bootstrap to make creating the layout easier. head.ejs <meta charset= \"utf-8\" > <meta http-equiv= \"X-UA-Compatible\" content= \"IE=edge\" > <meta name= \"viewport\" content= \"width=device-width, initial-scale=1\" > <meta name= \"description\" content= \"\" > <meta name= \"author\" content= \"\" > <title> Crypto app </title> <!-- Bootstrap Core CSS --> <link href= \"http://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css\" rel= \"stylesheet\" > <!-- Custom CSS --> <link rel= \"stylesheet\" type= \"text/css\" href= \"public/css/style.css\" > <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries --> <!-- WARNING: Respond.js doesn't work if you view the page via file:// --> <!--[if lt IE 9]> <script src=\"https://oss.maxcdn.com/libs/html5shiv/3.7.3/html5shiv.js\"></script> <script src=\"https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js\"></script> <![endif]--> <script src= \"http://d3js.org/d3.v3.min.js\" charset= \"utf-8\" ></script> <script src= \"http://labratrevenge.com/d3-tip/javascripts/d3.tip.v0.6.3.js\" ></script> header.ejs <!-- Navigation --> <nav class= \"navbar navbar-inverse navbar-fixed-top\" role= \"navigation\" > <div class= \"container\" > <!-- Brand and toggle get grouped for better mobile display --> <div class= \"navbar-header\" > <button type= \"button\" class= \"navbar-toggle\" data-toggle= \"collapse\" data-target= \"#bs-example-navbar-collapse-1\" > <span class= \"sr-only\" > Toggle navigation </span> <span class= \"icon-bar\" ></span> <span class= \"icon-bar\" ></span> <span class= \"icon-bar\" ></span> </button> <a class= \"navbar-brand\" href= \"/\" > Crypto app </a> </div> <!-- /.navbar-header --> <div class= \"collapse navbar-collapse\" id= \"bs-example-navbar-collapse-1\" > <ul class= \"nav navbar-nav\" > <li> <a href= \"./mining\" > Mining </a> </li> <li> <a href= \"./wallet\" > Wallet </a> </li> </ul> </div> <!-- /.navbar-collapse --> </div> <!-- /.container --> </nav> footer.ejs <p class= \"text-center text-small text-muted\" > © Copyright 2017 - Crypto app </p> <!-- JQuery JS--> <script src= \"https://code.jquery.com/jquery-3.2.1.min.js\" ></script> <!-- Bootstrap Core JS --> <script src= \"//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js\" ></script> Pages Now creating the pages is simple. The frontpage is currently empty and will simply look like this: index.ejs <!DOCTYPE html> < html lang = \"en\" > < head > <% include ../ partials / head %> </ head > < body > < header > <% include ../ partials / header %> </ header > < main class = \"main wrap\" > < div class = \"container clear-top\" > < div class = \"row\" > < div class = \"col-lg-12 text-center\" > < h1 > <%- title %> </ h1 > </ div > </ div > <!-- /.row --> </ div > <!-- /.container --> </ main > < footer class = \"footer\" > <% include ../ partials / footer %> </ footer > </ body > </ html > The mining page template is identical to the frontpage, but we add a placeholder for the D3.js graph and show the data in a table. mining.ejs <!DOCTYPE html> < html lang = \"en\" > < head > <% include ../ partials / head %> </ head > < body > < header > <% include ../ partials / header %> </ header > < main class = \"main wrap\" > < div class = \"container\" > < div class = \"row\" > < div class = \"col-lg-12 text-center\" > < h1 > <%- title %> </ h1 > </ div > </ div > <!-- /.row --> < div class = \"row chart-container\" > < svg class = \"svg-chart\" width = \"960\" height = \"500\" > <!-- placeholder for the chart --> </ svg > </ div > <!-- /.row --> < div class = \"row\" > < div class = \"col-lg-12\" > < table class = \"table\" > < tr > < th > Date </ th > < th > BTC </ th > < th > DRL </ th > < th > LTC </ th > </ tr > <% minings . forEach ( function ( mining ) { %> < tr > < td > <%= mining . Date %> </ td > < td > <%= mining . BTC %> </ td > < td > <%= mining . DRK %> </ td > < td > <%= mining . LTC %> </ td > </ tr > <% }); %> </ table > </ div > </ div > <!-- /.row --> </ div > <!-- /.container --> </ main > < footer > <% include ../ partials / footer %> </ footer > </ body > </ html > Next I append te template with a Javascript block containing the D3.js graph code. // Convert the bitcoins data to the data we can use in DS.js var data = <%- JSON . stringify ( minings ) %> ; var svg = d3.select('svg'), margin = { top: 20, right: 50, bottom: 100, left: 50 }, width = +svg.attr('width') - margin.left - margin.right, height = +svg.attr('height') - margin.top - margin.bottom, g = svg.append('g').attr('transform', 'translate(' + margin.left + ',' + margin.top + ')'); // Graph title g.append('text') .attr('x', (width / 2)) .attr('y', 0 - (margin.top / 3)) .attr('text-anchor', 'middle') .style('font-size', '16px') .text('Mining chart'); // Function to convert a string into a time var parseTime = d3.time.format('%Y-%m-%dT%H:%M:%S.%LZ').parse; // Function to show specific time format var formatTime = d3.time.format('%e %B'); var tip = d3.tip() .attr('class', 'd3-tip') .offset([-10, 0]) .html(function(d) { return \" <span style= 'color:red' > \" + d.worth + \" </span> <strong> \" + d.currency + \" </strong> \"; }) svg.call(tip); var color = d3.scale.category10(); color.domain(d3.keys(data[0]).filter(function(key) { return key !== \"Date\" && key !== \"_id\"; })); // Correct the types data.forEach(function(d) { d.date = parseTime(d.Date); }); var rewards = color.domain().map(function(name) { return { name: name, values: data.map(function(d) { return { date: d.date, worth: +d[name], currency: name }; }) }; }); var num_bars = d3.keys(rewards).length; var num_days = data.length; var y = d3.scale.linear().range([height, 0]); y.domain([ 0, d3.max(rewards, function(c) { return d3.max(c.values, function(v) { return v.worth; }); }) ]); var x0 = d3.scale.ordinal() .domain(d3.range(num_days)) .rangeBands([0, width], .2); var x1 = d3.scale.ordinal() .domain(d3.range(num_bars)) .rangeBands([0, x0.rangeBand()]); var color = d3.scale.category10(); var xAxis = d3.svg.axis() .scale(x0) .tickFormat(function(d) { return formatTime(parseTime(data[d].Date)); }) .orient(\"bottom\"); var yAxis = d3.svg.axis() .scale(y) .orient(\"left\"); g.append(\"g\") .attr(\"class\", \"y axis\") .call(yAxis) .append(\"text\") .attr(\"transform\", \"rotate(-90)\") .attr(\"y\", 6) .attr(\"dy\", \".71em\") .style(\"text-anchor\", \"end\") .text(\"Amount\"); g.append(\"g\") .attr(\"class\", \"x axis\") .attr(\"transform\", \"translate(0,\" + height + \")\") .call(xAxis) .selectAll(\"text\") .style(\"text-anchor\", \"end\") .attr(\"dx\", \"-.8em\") .attr(\"dy\", \".15em\") .attr(\"transform\", function(d) { return \"rotate(-90)\" }); // Add the bars g.append(\"g\").selectAll(\".bar\") .data(rewards) .enter().append(\"g\") .style(\"fill\", function(d, i) { return color(i); }) .attr(\"transform\", function(d, i) { return \"translate(\" + x1(i) + \",0)\"; }) .selectAll(\"rect\") .data(function(d) { return d.values; }) .enter().append(\"rect\") .attr(\"class\", \"bar\") .attr(\"width\", x1.rangeBand()) .attr(\"height\", function(d) { return height - y(d.worth); }) .attr(\"x\", function(d, i) { return x0(i); }) .attr(\"y\", function(d) { return y(d.worth); }) .on('mouseover', tip.show) .on('mouseout', tip.hide); var legend = g.append(\"g\") .attr(\"font-family\", \"sans-serif\") .attr(\"font-size\", 10) .attr(\"text-anchor\", \"end\") .selectAll(\"g\") .data(rewards) .enter().append(\"g\") .attr(\"transform\", function(d, i) { return \"translate(0,\" + i * 20 + \")\"; }); legend.append(\"rect\") .attr(\"x\", width - 19) .attr(\"width\", 19) .attr(\"height\", 19) .attr(\"fill\", function(d, i) { return color(i); }) legend.append(\"text\") .attr(\"x\", width - 24) .attr(\"y\", 9.5) .attr(\"dy\", \"0.32em\") .text(function(d) { return d.name; }); For the wallet page I have a similar approach. wallet.ejs <!DOCTYPE html> < html lang = \"en\" > < head > <% include ../ partials / head %> </ head > < body > < header > <% include ../ partials / header %> </ header > < main class = \"main wrap\" > < div class = \"container\" > < div class = \"row\" > < div class = \"col-lg-12 text-center\" > < h1 > <%- title %> </ h1 > </ div > </ div > <!-- /.row --> < div class = \"row chart-container\" > < svg class = \"svg-chart\" width = \"960\" height = \"500\" > <!-- placeholder for the chart --> </ svg > </ div > <!-- /.row --> < div class = \"row\" > < div class = \"col-lg-12\" > < table class = \"table\" > < tr > < th > Date </ th > < th > BTC </ th > < th > DOGE </ th > < th > ETH </ th > < th > LTC </ th > < th > REP </ th > </ tr > <% wallets . forEach ( function ( wallet ) { %> < tr > < td > <%= wallet . Time %> </ td > < td > <%= wallet . BTC %> </ td > < td > <%= wallet . DOGE %> </ td > < td > <%= wallet . ETH %> </ td > < td > <%= wallet . LTC %> </ td > < td > <%= wallet . REP %> </ td > </ tr > <% }); %> </ table > </ div > </ div > <!-- /.row --> </ div > <!-- /.container --> </ main > < footer > <% include ../ partials / footer %> </ footer > </ body > </ html > and the Javascript // Convert the bitcoins data to the data we can use in DS.js var data = <%- JSON . stringify ( wallets ) %> ; // Draw a line chart var svg = d3.select('svg.svg-chart'), margin = { top: 20, right: 50, bottom: 30, left: 50 }, width = +svg.attr('width') - margin.left - margin.right, height = +svg.attr('height') - margin.top - margin.bottom, g = svg.append('g').attr('transform', 'translate(' + margin.left + ',' + margin.top + ')'); // Graph title g.append('text') .attr('x', (width / 2)) .attr('y', 0 - (margin.top / 3)) .attr('text-anchor', 'middle') .style('font-size', '16px') .text('Wallet chart'); // Function to convert a string into a time var parseTime = d3.time.format('%Y-%m-%dT%H:%M:%S.%LZ').parse; // Function to show specific time format var formatTime = d3.time.format('%e %B'); // Set the X scale var x = d3.time.scale().range([0, width], 0.5); // Set the Y scale var y = d3.scale.linear().range([height, 0]); // Set the color scale var color = d3.scale.category10(); var xAxis = d3.svg.axis() .scale(x) .orient(\"bottom\"); var yAxis = d3.svg.axis() .scale(y) .orient(\"left\"); var line = d3.svg.line() // .interpolate(\"basis\") .x(function(d) { return x(d.date); }) .y(function(d) { return y(d.worth); }); // Select the important columns color.domain(d3.keys(data[0]).filter(function(key) { return key !== \"Time\" && key !== \"_id\"; })); // Correct the types data.forEach(function(d) { d.date = parseTime(d.Time); }); var currencies = color.domain().map(function(name) { return { name: name, values: data.map(function(d) { return { date: d.date, worth: +d[name] }; }) }; }); // Set the X domain x.domain(d3.extent(data, function(d) { return d.date; })); // Set the Y domain y.domain([ d3.min(currencies, function(c) { return d3.min(c.values, function(v) { return v.worth; }); }), d3.max(currencies, function(c) { return d3.max(c.values, function(v) { return v.worth; }); }) ]); // Set the X axis g.append(\"g\") .attr(\"class\", \"x axis\") .attr(\"transform\", \"translate(0,\" + height + \")\") .call(xAxis); // Set the Y axis g.append(\"g\") .attr(\"class\", \"y axis\") .call(yAxis) .append(\"text\") .attr(\"transform\", \"rotate(-90)\") .attr(\"y\", 6) .attr(\"dy\", \".71em\") .style(\"text-anchor\", \"end\") .text(\"Value (USD)\"); // Draw the lines var currency = g.selectAll(\".currency\") .data(currencies) .enter().append(\"g\") .attr(\"class\", \"currency\"); currency.append(\"path\") .attr(\"class\", \"line\") .attr(\"d\", function(d) { return line(d.values); }) .style(\"stroke\", function(d) { return color(d.name); }); // Add the circles currency.append(\"g\").selectAll(\"circle\") .data(function(d) { return d.values }) .enter() .append(\"circle\") .attr(\"r\", 2) .attr(\"cx\", function(dd) { return x(dd.date) }) .attr(\"cy\", function(dd) { return y(dd.worth) }) .attr(\"fill\", \"none\") .attr(\"stroke\", function(d) { return color(this.parentNode.__data__.name) }); // Add label to the end of the line currency.append(\"text\") .attr(\"class\", \"label\") .datum(function(d) { return { name: d.name, value: d.values[d.values.length - 1] }; }) .attr(\"transform\", function(d) { return \"translate(\" + x(d.value.date) + \",\" + y(d.value.worth) + \")\"; }) .attr(\"x\", 3) .attr(\"dy\", \".35em\") .text(function(d) { return d.name; }); // Add the mouse line var mouseG = g.append(\"g\") .attr(\"class\", \"mouse-over-effects\"); mouseG.append(\"path\") .attr(\"class\", \"mouse-line\") .style(\"stroke\", \"black\") .style(\"stroke-width\", \"1px\") .style(\"opacity\", \"0\"); var lines = document.getElementsByClassName('line'); var mousePerLine = mouseG.selectAll('.mouse-per-line') .data(currencies) .enter() .append(\"g\") .attr(\"class\", \"mouse-per-line\"); mousePerLine.append(\"circle\") .attr(\"r\", 7) .style(\"stroke\", function(d) { return color(d.name); }) .style(\"fill\", \"none\") .style(\"stroke-width\", \"2px\") .style(\"opacity\", \"0\"); mousePerLine.append(\"text\") .attr(\"class\", \"hover-text\") .attr(\"dy\", \"-1em\") .attr(\"transform\", \"translate(10,3)\"); // Append a rect to catch mouse movements on canvas mouseG.append('svg:rect') .attr('width', width) .attr('height', height) .attr('fill', 'none') .attr('pointer-events', 'all') .on('mouseout', function() { // on mouse out hide line, circles and text d3.select(\".mouse-line\") .style(\"opacity\", \"0\"); d3.selectAll(\".mouse-per-line circle\") .style(\"opacity\", \"0\"); d3.selectAll(\".mouse-per-line text\") .style(\"opacity\", \"0\"); }) .on('mouseover', function() { // on mouse in show line, circles and text d3.select(\".mouse-line\") .style(\"opacity\", \"1\"); d3.selectAll(\".mouse-per-line circle\") .style(\"opacity\", \"1\"); d3.selectAll(\".mouse-per-line text\") .style(\"opacity\", \"1\"); }) .on('mousemove', function() { // mouse moving over canvas var mouse = d3.mouse(this); d3.selectAll(\".mouse-per-line\") .attr(\"transform\", function(d, i) { var xDate = x.invert(mouse[0]), bisect = d3.bisector(function(d) { return d.date; }).left; idx = bisect(d.values, xDate); d3.select(this).select('text') .text(y.invert(y(d.values[idx].worth)).toFixed(2)); d3.select(\".mouse-line\") .attr(\"d\", function() { var data = \"M\" + x(d.values[idx].date) + \",\" + height; data += \" \" + x(d.values[idx].date) + \",\" + 0; return data; }); return \"translate(\" + x(d.values[idx].date) + \",\" + y(d.values[idx].worth) + \")\"; }); }); Last thing we need to add is some custom style to the pages and graphs. style.css html , body { height : 100 % ; } body { background-color : #eee ; padding-top : 70 px ; padding-bottom : 70 px ; } . wrap { min-height : 100 % ; } . text-small { font-size : small ; } . chart-container { margin : 0 px auto ; padding : 40 px ; } . main { overflow : auto ; padding-bottom : 50 px ; } . footer { position : relative ; margin-top : -50 px ; height : 50 px ; clear : both ; padding-top : 20 px ; } /* Visualization */ path { stroke-width : 1 ; fill : none ; stroke-linejoin : round ; stroke-linecap : round ; } circle { stroke-width : 1 ; fill : steelblue } . axis path , . axis line { fill : none ; stroke : grey ; stroke-width : 1 ; shape-rendering : crispEdges ; } . legend , . label , . hover-text { font-size : x-small ; background-color : white ; } . axis text { font : 10 px sans-serif ; } . axis path , . axis line { fill : none ; stroke : #000 ; shape-rendering : crispEdges ; } . bar : hover { fill : orangered ; } . d3-tip { line-height : 1 ; font-weight : bold ; padding : 12 px ; background : rgba ( 0 , 0 , 0 , 0.8 ); color : #fff ; border-radius : 2 px ; } /* Creates a small triangle extender for the tooltip */ . d3-tip : after { box-sizing : border-box ; display : inline ; font-size : 10 px ; width : 100 % ; line-height : 1 ; color : rgba ( 0 , 0 , 0 , 0.8 ); content : \"\\25BC\" ; position : absolute ; text-align : center ; } /* Style northward tooltips differently */ . d3-tip . n : after { margin : -1 px 0 0 0 ; top : 100 % ; left : 0 ; } And that is it. Now by running the server, the application can be viewed on port 3000 of your localhost! jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ node app.js Result I did not deploy the application on a server (yet) so it cannot be viewed. However, I have put my code on Bitbucket and the two charts can be viewed as Gist or bl.ocks.org . The line chart can be viewed on this page. Using rawgit I was able to display it using an iframe. The bar chart can be viewed on this page. Because I am using an additional library to create fancy tooltips, the iframe won't load properly. Check my Github repo for the source code.","tags":"posts","url":"building-a-crypto-app.html"},{"title":"Setting up a Dapp with Truffle and Metamask","text":"Update npm jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sudo npm install -g npm Update nodejs jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sudo npm install -g n jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sudo n stable Install geth jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ brew tap ethereum/ethereum jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ brew install ethereum Check versions jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sw_vers ProductName: Mac OS X ProductVersion: 10 .12.4 BuildVersion: 16E195 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ geth version Geth Version: 1 .6.1-stable Git Commit: 021c3c281629baf2eae967dc2f0a7532ddfdc1fb Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: darwin GOPATH = GOROOT = /usr/local/Cellar/go/1.8.1/libexec jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ node -v v7.10.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ npm -v 4 .2.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ python -V Python 2 .7.13 :: Anaconda 4 .3.1 ( x86_64 ) Create structure for the app jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ mkdir truffle-dapp jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ cd truffle-dapp/ && git init && npm init Install web3 and testrpc Note that you need to use Python 2.7 in order to be able to install the Ethereum testrpc. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ npm install ethereumjs-testrpc web3 --save --python = python2.7 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ echo node_modules/ >> .gitignore jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ git commit -a -m \"Initial commit\" Start the testrpc This includes 10 unlocked accounts with 100 Ether each. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ node_modules/.bin/testrpc Secp256k1 bindings are not compiled. Pure JS implementation will be used. EthereumJS TestRPC v3.0.5 Available Accounts ================== ( 0 ) 0x21af84b1d1b0c26dd470d1e13074d784981a1ca7 ( 1 ) 0xebf0475f0c9dec6d39d353cab90d10b27f0575a8 ( 2 ) 0xc0ffba2ed3bbe73e123cc31fa01215fdd9be3233 ( 3 ) 0x5868efd6f3255925268c53e523a81164f4d86733 ( 4 ) 0xb3ff5cf0790e67e46fad5e476967d1bd42e9a288 ( 5 ) 0x6581ff04d20ad77025d2775c6479bbcf1d292f0c ( 6 ) 0x105a59eb345e602a38553432e7a18360ac3040a8 ( 7 ) 0x8febd769876d764d8d1a7bb3d3d4360df9282401 ( 8 ) 0xb15505faddaa401f23738843956eab8ab0078d74 ( 9 ) 0x56e5a52e65329c75314ec9642725fd272ac908f8 Private Keys ================== ( 0 ) b4b11e97ab1055d41ae8b93d76cb699cc637ab6c45ac3ce769208b37ac7d4e9f ( 1 ) a1222fd97545205ff2e10143a8e6bbe89ea3aa429b4cef64e641885c302a8e4c ( 2 ) 879f33b2dc93fc3add7ab2f189f00c5cf77490090d7d9c46cb4883dd65ece305 ( 3 ) 3ced2c7e98b75eb8220edf8109d934ba229d3d5c996d7a297d0ad3961b716275 ( 4 ) 3bbaeaa98f28ad987c71815c0f07a79be3bc89c1391ae808ea0af98b61201914 ( 5 ) ec6b3c03a904bd7650c803cccb5cf29ccdaa444af336103c021af782b866873c ( 6 ) b1584edd48d57a35acc1176b1a02f7d5c5401e3e00495ce859cf76f8be24b207 ( 7 ) 9aa9ce75c4165b30c4847262a4cbb634a2f199228b3386b92e5fa184830bc95b ( 8 ) 68ab448c5a453ea723561b63fb756879bae3fbb44fc003311de62827023ca49a ( 9 ) 674cf39b8c96a8793bf6e0c6c0b3a7c234644eaf7536de1406148fd8699fede7 HD Wallet ================== Mnemonic: search romance drip card right human valley tilt depart detail nation rich Base HD Path: m/44 '/60' /0 ' /0/ { account_index } Listening on localhost:8545 Note. The mnemonic with the 12 words will be used later in Metamask. Connect to the testrpc Create a client and check the balance. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ node > Web3 = require ( 'web3' ) { [ Function: Web3 ] providers: { HttpProvider: [ Function: HttpProvider ] , IpcProvider: [ Function: IpcProvider ] } } > web3 = new Web3 ( new Web3.providers.HttpProvider ( \"http://localhost:8545\" )) ; > primary = web3.eth.accounts [ 0 ] ; '0x914cb49b14a339d000858dc4c8b4cb0e9195c574' > web3.fromWei ( web3.eth.getBalance ( primary ) , \"ether\" ) .toString () '100' Install the Truffle scaffold jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ npm install truffle --save --python = python2.7 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle version Truffle v3.2.2 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle init webpack jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ git add . jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ git commit -m \"Truffle init\" Compile jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle compile Compiling ./contracts/ConvertLib.sol... Compiling ./contracts/MetaCoin.sol... Compiling ./contracts/Migrations.sol... Writing artifacts to ./build/contracts Migrate jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle migrate Deploy Start the server in the development environment. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ npm run dev Now by accessing localhost:8080 the web application is visible and we can send some MetaCoin. Open up Google Chrome and make sure Metamask is installed. Next open up Metamask and restore from DEN and fill in the 12 words that the testrpc showed you before. Make sure you are connected to localhost:8545. Click in Metamask on the icon on the top to switch accounts. Click on add and the other accounts will appear. We can now choose one of the other 9 accounts to send some MetaCoin. Click on the copy icon next to the account and enter it in the input field in the form. Add the amount of MetaCoin you want to send and hit Send MetaCoin. Metamask will show a pop-up asking you to accept the transaction. Once the MetaCoin are sent, the amount on the frontpage are reduced. Clicking again on Metamask will show the history of transactions.","tags":"posts","url":"creating-dapp-with-truffle-and-metamask.html"},{"title":"Using the Ethereum Web3 client in Python","text":"Ethereum - Web3 Similar to the RPC client, I will use the web3 API to make a transaction on my private blockchain. Read The Docs GitHub In [1]: from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" I willl use the Python 2.7 environment since Python 3.6 is not yet supported. In [2]: import sys sys . version Out[2]: '2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]' ( python2 ) jitsejan@jjvps:~$ pip install web3 Collecting web3 Requirement already satisfied: requests> = 2 .12.4 in /home/jitsejan/anaconda3/envs/python2/lib/python2.7/site-packages ( from web3 ) Collecting ethereum-utils> = 0 .2.0 ( from web3 ) Requirement already satisfied: pysha3> = 0 .3 in /home/jitsejan/anaconda3/envs/python2/lib/python2.7/site-packages ( from web3 ) Collecting ethereum-abi-utils> = 0 .4.0 ( from web3 ) Requirement already satisfied: rlp> = 0 .4.7 in /home/jitsejan/anaconda3/envs/python2/lib/python2.7/site-packages ( from web3 ) Collecting pylru> = 1 .0.9 ( from web3 ) Installing collected packages: ethereum-utils, ethereum-abi-utils, pylru, web3 Successfully installed ethereum-abi-utils-0.4.0 ethereum-utils-0.2.0 pylru-1.0.9 web3-3.8.1 Note: make sure geth is started with arguments --rpc and --rpcapi=\"db,eth,net,web3,personal,web3\" jitsejan@jjvps:~$ geth --networkid 23 --nodiscover --maxpeers 0 --port 30333 --rpc --rpcapi = \"db,eth,net,web3,personal,miner\" In [3]: import web3 from web3 import Web3 , KeepAliveRPCProvider , IPCProvider web3 . __version__ Out[3]: '3.8.1' Setup RPC connection In [4]: web3 = Web3 ( KeepAliveRPCProvider ( host = 'localhost' , port = '8545' )) Retrieve the accounts In [5]: web3 . eth . accounts Out[5]: [u'0x8cf9deda0712f2291fb16739f8759e4a0a575854'] In [6]: address = web3 . eth . accounts [ 0 ] Set the two other machines In [7]: address_vps_one = \"0xc257beaea430afb3a09640ce7f020c906331f805\" address_vps_two = \"0xe86ee31b7d32b743907fa7438c422a1803717deb\" Show their balances In [8]: print \"Host has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address ), 'ether' ) print \"VPS 1 has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address_vps_one ), 'ether' ) print \"VPS 2 has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address_vps_two ), 'ether' ) prev_host_amount = float ( web3 . fromWei ( web3 . eth . getBalance ( address ), 'ether' )) Host has 411 Ether VPS 1 has 150 Ether VPS 2 has 83 Ether Transaction Lets send 12 Ether from the main machine to VPS one. In [9]: amount = 12 # Ether sending_address = address receiving_address = address_vps_one Get the password to unlock the sending account In [10]: from getpass import getpass pw = getpass ( prompt = 'Enter the password for the sender: ' ) Enter the password for the sender: ········ Unlock the account In [11]: web3 . personal . unlockAccount ( address , pw ) Out[11]: True Create the transaction In [12]: params = {} params [ 'to' ] = receiving_address params [ 'from' ] = sending_address params [ 'value' ] = web3 . toWei ( amount , \"ether\" ) tx_hash = web3 . eth . sendTransaction ( params ) Check the transaction In [13]: web3 . eth . getTransaction ( tx_hash ) Out[13]: {u'blockHash': u'0x0000000000000000000000000000000000000000000000000000000000000000', u'blockNumber': None, u'from': u'0x8cf9deda0712f2291fb16739f8759e4a0a575854', u'gas': 90000, u'gasPrice': 20000000000, u'hash': u'0x46e0a5a96dbea0391adbfd401b087ad1d1dfb120684462d8652df7d8b8d7f069', u'input': u'0x', u'nonce': 24, u'r': u'0x63d0a6d39e9f57bf42e874bb268fc11c67cdb72d86cd21a65181b48063ddb531', u's': u'0x44bb390959b28e0f14d4a7891b5ac44c84816a69c2434aeafdc5c94a42997b2b', u'to': u'0xc257beaea430afb3a09640ce7f020c906331f805', u'transactionIndex': 0, u'v': u'0x42', u'value': 12000000000000000000L} Check the receipt The miner is not running, so the receipt returns nothing In [14]: web3 . eth . getTransactionReceipt ( tx_hash ) Start and stop the miner Note: web3.admin.sleepBlocks seems not te be available so I use a simple sleep. Within this sleep it could be that there are more than 1 mining cycles. In [15]: import time miner = web3 . miner miner . start ( 1 ) time . sleep ( 5 ) miner . stop () Out[15]: True Check the receipt again Now the transaction has taken place In [16]: web3 . eth . getTransactionReceipt ( tx_hash ) Out[16]: {u'blockHash': u'0x635486b2100a4a9de155b57c1a9fad4dfafcf1db1d62f9aa55bd38d6a3864869', u'blockNumber': 110, u'contractAddress': None, u'cumulativeGasUsed': 21000, u'from': u'0x8cf9deda0712f2291fb16739f8759e4a0a575854', u'gasUsed': 21000, u'logs': [], u'logsBloom': u'0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000', u'root': u'0xe7ca6b8a99b6e69f52c2834ae84276dd9b0b89daf459c7391d29e2121e49e27e', u'to': u'0xc257beaea430afb3a09640ce7f020c906331f805', u'transactionHash': u'0x46e0a5a96dbea0391adbfd401b087ad1d1dfb120684462d8652df7d8b8d7f069', u'transactionIndex': 0} Check the balances again The host should have 12 Ether less, but received 5 Ether for each mining cycle. The first VPS should have received 12 Ether. In [17]: print \"Host has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address ), 'ether' ) print \"VPS 1 has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address_vps_one ), 'ether' ) print \"VPS 2 has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address_vps_two ), 'ether' ) Host has 404 Ether VPS 1 has 162 Ether VPS 2 has 83 Ether In [18]: mine_bonus = 5 # Ether num_cycles = ( float ( web3 . fromWei ( web3 . eth . getBalance ( address ), 'ether' )) - prev_host_amount + amount ) / mine_bonus print \" %d mining cycle(s) are elapsed\" % num_cycles 1 mining cycle(s) are elapsed","tags":"posts","url":"using-ethereum-web3-client-python.html"},{"title":"Using the Ethereum RPC client in Python","text":"Ethereum - RPC client In [1]: from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" In this notebook I will try out the RPC client of Ethereum using Python. See JSON RPC for more information on JSON RPC . In [2]: import sys sys . version Out[2]: '3.6.0 |Anaconda 4.3.0 (64-bit)| (default, Dec 23 2016, 12:22:00) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]' Prepare environment Install the Python RPC client for Ethereum. Github jitsejan@jjvps:~$ pip install ethereum-rpc-client Start the blockchain making sure RPC is enabled. jitsejan@jjvps:~$ geth --networkid 23 --nodiscover --maxpeers 0 --port 30333 --rpc Verify that geth is running and the account is listed. In [3]: ! geth account list Account #0: {8cf9deda0712f2291fb16739f8759e4a0a575854} keystore:///home/jitsejan/.ethereum/keystore/UTC--2017-05-01T14-58-43.532247863Z--8cf9deda0712f2291fb16739f8759e4a0a575854 Connect to the RPC client In [4]: from eth_rpc_client import Client client = Client ( host = \"127.0.0.1\" , port = \"8545\" ) Inspect the client In [5]: import pdir pdir ( client ) Out[5]: abstract class: __subclasshook__ attribute access: __delattr__ , __dir__ , __getattribute__ , __setattr__ class customization: __init_subclass__ object customization: __format__ , __hash__ , __init__ , __new__ , __repr__ , __sizeof__ , __str__ other: _coinbase_cache , _coinbase_cache_til , _nonce , async_timeout , host , is_async , port , request_queue , request_thread , results , session pickle: __reduce__ , __reduce_ex__ rich comparison: __eq__ , __ge__ , __gt__ , __le__ , __lt__ , __ne__ special attribute: __class__ , __dict__ , __doc__ , __module__ , __weakref__ descriptor: default_from_address: @property with getter, Cache the coinbase address so that we don't make two requests for every function: _make_request: call: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_call construct_json_request: get_accounts: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_accounts get_balance: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getbalance get_block_by_hash: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getblockbyhash get_block_by_number: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getblockbynumber get_block_number: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_blocknumber<F37> get_code: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getcode get_coinbase: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_coinbase get_filter_changes: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getfilterchanges get_filter_logs: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getfilterlogs get_gas_price: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_gasprice get_logs: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getlogs get_max_gas: get_nonce: get_transaction_by_hash: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_gettransactionbyhash get_transaction_receipt: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_gettransactionreceipt make_request: new_block_filter: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_newblockfilter new_filter: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_newfilter new_pending_transaction_filter: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_newpendingtransactionfilter process_requests: Loop that runs in a thread to process requests synchronously. send_transaction: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_sendtransaction uninstall_filter: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_uninstallfilter wait_for_block: wait_for_transaction: Get the coinbase for the blockchain In [6]: address = client . get_coinbase () address Out[6]: '0x8cf9deda0712f2291fb16739f8759e4a0a575854' Retrieve the balance of the main address In [7]: client . get_balance ( address ) Out[7]: 135000419895999999940 Set the addresses of the two other machines: In [8]: address_vps_one = \"0xc257beaea430afb3a09640ce7f020c906331f805\" address_vps_two = \"0xe86ee31b7d32b743907fa7438c422a1803717deb\" In [9]: client . get_balance ( address_vps_one ) client . get_balance ( address_vps_two ) Out[9]: 6999160060000000000 Out[9]: 83000420044000000060 Transaction Lets send 12 Ether from the main machine to VPS one. In [10]: amount = 12 # Ether sending_address = address receiving_address = address_vps_one 1 GWEI = 0.000000001 Ether Get the password to unlock the sending account In [11]: from getpass import getpass pw = getpass ( prompt = 'Enter the password for the sender: ' ) Enter the password for the sender: ········ Unlock the account via the command line (By lack of a better way) In [12]: command = r 'geth --exec \"personal.unlockAccount(\\\" %s \\\", \\\" %s \\\");\" attach ' % ( sending_address , pw ) result = ! $command if result [ 0 ] != 'true' : print ( 'Fail: %s ' % result [ 0 ]) else : print ( 'Account is unlocked!' ) Account is unlocked! Send the transaction In [13]: tx_hash = client . send_transaction ( to = receiving_address , _from = sending_address , value = amount * 10 ** 9 ) Check the transaction details In [14]: client . get_transaction_by_hash ( tx_hash ) Out[14]: {'blockHash': '0x0000000000000000000000000000000000000000000000000000000000000000', 'blockNumber': None, 'from': '0x8cf9deda0712f2291fb16739f8759e4a0a575854', 'gas': '0x15f90', 'gasPrice': '0x4a817c800', 'hash': '0x3d1a193ccfccc4e9ab2a411044069deeec2feef31a594bbf73726b463e8e90b4', 'input': '0x', 'nonce': '0xb', 'r': '0xe8698846a461938e800698fcc34570e0c4e9a3425f0bc441bf3e0716dab7b3e0', 's': '0x4fcd9bda8a1e98a7b0e8d953dec0cc733238c383d97393aa15c43963551f8daf', 'to': '0xc257beaea430afb3a09640ce7f020c906331f805', 'transactionIndex': '0x0', 'v': '0x42', 'value': '0x2cb417800'} Perform one mining step Execute the miner to validate the transaction. In [15]: prev_balance_sen = client . get_balance ( sending_address ) prev_balance_rec = client . get_balance ( receiving_address ) In [16]: result = ! geth --exec \"miner.start();admin.sleepBlocks(1);miner.stop();\" attach if result [ 0 ] != 'true' : print ( 'Fail: %s ' % result [ 0 ]) else : print ( \"Mining finished!\" ) Mining finished! Check if the Ether has been received In [17]: print ( \"Received %d \" % ( client . get_balance ( receiving_address ) - prev_balance_rec )) Received 12000000000 Check if the Ether was sent First check the difference in the balance. In [18]: print ( \"Difference of the sender %d \" % ( client . get_balance ( sending_address ) - prev_balance_sen )) Difference of the sender 4999999988000000000 For mining, the miner will get a mining bonus. In [19]: mining_bonus = 5000000000000000000 To get the amount of Ether sent we need to substract the mining bonus. In [20]: print ( \"Amount difference: %d \" % int ( client . get_balance ( sending_address ) - prev_balance_sen - mining_bonus )) Amount difference: -12000000000","tags":"posts","url":"using-ethereum-rpc-client-python.html"},{"title":"Setting up a private Ethereum blockchain","text":"Setup For this experiment, I will use three Linux machines running Ubuntu. Machine one will be used as the base, the second machine will send some Ether to the third machine. jitsejan@jjvps:~$ uname -a Linux jjvps 2 .6.32-042stab123.1 #1 SMP Wed Mar 22 15:21:30 MSK 2017 x86_64 x86_64 x86_64 GNU/Linux jitsejan@jjvps:~$ geth version Geth Version: 1 .6.0-stable Git Commit: facc47cb5cec97b22c815a0a6118816a98f39876 Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: linux GOPATH = GOROOT = /usr/lib/go-1.8 jitsejan@jjschi1:~$ uname -a Linux jjschi1 2 .6.32-042stab108.8 #1 SMP Wed Jul 22 17:23:23 MSK 2015 x86_64 x86_64 x86_64 GNU/Linux jitsejan@jjschi1:~$ geth version Geth Version: 1 .6.0-stable Git Commit: facc47cb5cec97b22c815a0a6118816a98f39876 Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: linux GOPATH = GOROOT = /usr/lib/go-1.8 jitsejan@jjschi2:~$ uname -a Linux jjschi2 2 .6.32-042stab094.7 #1 SMP Wed Oct 22 12:43:21 MSK 2014 x86_64 x86_64 x86_64 GNU/Linux jitsejan@jjschi2:~$ geth version Geth Version: 1 .6.0-stable Git Commit: facc47cb5cec97b22c815a0a6118816a98f39876 Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: linux GOPATH = GOROOT = /usr/lib/go-1.8 Create new user accounts Repeat the following for all the machines. jitsejan@jjschi1:~$ geth account new WARN [ 05 -01 | 06 :51:51 ] No etherbase set and no accounts found as default Your new account is locked with a password. Please give a password. Do not forget this password. Passphrase: Repeat passphrase: Address: { fb82f6d873addc0032a08aaa05bb1c338ce49b45 } Create a genesis file Create a genesis file with the 3 addresses from the newly created accounts. Set an initial balance to the accounts so we can transfer some 'money'. Set the gasLimit to the maximum and the difficulty low. jitsejan@jjvps:~$ nano jitsejansGenesis.json { \"config\" : { \"chainId\" : 15 , \"homesteadBlock\" : 0 , \"eip155Block\" : 0 , \"eip158Block\" : 0 }, \"nonce\" : \"0x0000000000000042\" , \"mixhash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\" , \"difficulty\" : \"0x4000\" , \"alloc\" : {}, \"coinbase\" : \"0x0000000000000000000000000000000000000000\" , \"timestamp\" : \"0x00\" , \"parentHash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\" , \"gasLimit\" : \"0xffffffff\" , \"alloc\" : { \"0xfb82f6d873addc0032a08aaa05bb2c338ce49b45\" : { \"balance\" : \"20000000000000000000\" }, \"0xc257beaea430afb3a09640ce7f020c906331f805\" : { \"balance\" : \"40000000000000000000\" }, \"0xe86ee31b7d32b743907fa7438c422a1803717deb\" : { \"balance\" : \"40000000000000000000\" } } } Copy the genesis file to the second machine and third machine. jitsejan@jjschi1:~$ nano jitsejansGenesis.json jitsejan@jjschi2:~$ nano jitsejansGenesis.json Initialize the blockchain Initialize the blockchains on all three machines. jitsejan@jjsvps:~$ geth init jitsejansGenesis.json INFO [ 05 -01 | 06 :56:35 ] Allocated cache and file handles database = /home/jitsejan/.ethereum/geth/chaindata cache = 128 handles = 1024 INFO [ 05 -01 | 06 :56:35 ] Writing custom genesis block INFO [ 05 -01 | 06 :56:35 ] Successfully wrote genesis state hash = 86a3b9…deee1b Start the blockchain On the first machine, start the blockchain. jitsejan@jjvps:~$ geth --networkid 23 --nodiscover --maxpeers 2 --port 30333 console Get information about the hosting node. > admin.nodeInfo.enode \"enode://342a11d352151b3dfeb78db02a4319e1255c9fb49bc9a1dc44485f7c1bca9cc638540833e4577016f9a6180d1e911d907280af9b3892c53120e1e30619594eba@[::]:30333?discport=0\" Connect to the blockchain With the node information from the previous step, we can now create a static nodes files on the second and third machine to connect to the running blockchain. jitsejan@jjschi1~$ nano ~/.ethereum/static-nodes.json Replace the [::] with the IP address of the first machine and copy the file on the third machine too. [ \"enode://84f9c7f807a58f98643ac2bff9ea6691bf6be36fe6d0ccd0ad838a83501d16c1027269a82c3251104a10da5982e4fe905de41ae84dd44ba78e8cfb1659d355e8@192.123.345.567:30303?discport=0\" ] Restart the blockchain on the first machine. jitsejan@jjsvps:~$ geth --networkid 23 --nodiscover --maxpeers 1 --port 30333 console > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 20 Connect to the blockchain with the second machine and third machine. jitsejan@jjschi1:~$ geth --networkid 23 --port 30333 console > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 40 Once you perform a mining action, by default Ether will reward you with 5 ether. > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 40 > miner.start () ; admin.sleepBlocks ( 1 ) ; miner.stop () ; > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 45 Perform a transaction Unlock the sending machine > personal.unlockAccount ( '0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54' ) Unlock account 0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54 Passphrase: true Send the transaction > eth.sendTransaction ({ from: '0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54' , to: '0xfb82f6d873addc0032a08aaa05bb1c338ce49b45' , value: web3.toWei ( 23 , \"ether\" )}) INFO [ 05 -01 | 15 :09:47 ] Submitted transaction fullhash = 0x9ea76acbba2ad0bf65dc9b4295bfd7f2836435329a1fee9162b0649f35855ad3 recipient = 0xfb82f6d873addc0032a08aaa05bb2c338ce49b45 \"0x9ea76acbba2ad0bf65dc9b4295bfd7f2836435329a1fee9162b0649f35855ad3\" Check for pending transactions. The transaction should be queued. > eth.pendingTransactions [{ blockHash: null, blockNumber: null, from: \"0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54\" , gas: 90000 , gasPrice: 20000000000 , hash: \"0xf63024c9828ff5b77e63c118667394b285735da9ad53d01bf44aa8044b824955\" , input: \"0x\" , nonce: 0 , r: \"0x14ac03d0f4f55b4aa73b4f1f9f04752174bdf304366c994e8e4d26448e7decba\" , s: \"0x3ca7ab2f856d5e1edd6b6429df9b7be7a3c08d4afd4b2ac5a4ca9bdad2ec0caf\" , to: \"0xfb82f6d873addc0032a08aaa05bb1c338ce49b45\" , transactionIndex: 0 , v: \"0x41\" , value: 3000000000000000000 > net.peerCount 1 > net.listening true > txpool.status { pending: 1 , queued: 0 } On the receiving machine we start the mining to receive the transaction. > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 96 > miner.start ( 1 ) ; admin.sleepBlocks ( 1 ) ; miner.stop () ; INFO [ 05 -01 | 10 :51:14 ] Updated mining threads threads = 1 INFO [ 05 -01 | 10 :51:14 ] Starting mining operation INFO [ 05 -01 | 10 :51:14 ] Commit new mining work number = 96 txs = 1 uncles = 0 elapsed = 335 .481µs INFO [ 05 -01 | 10 :51:14 ] 🔗 block reached canonical chain number = 14 hash = b323e7…3daf34 INFO [ 05 -01 | 10 :51:20 ] Successfully sealed new block number = 96 hash = 8d2949…3f8a32 INFO [ 05 -01 | 10 :51:20 ] 🔨 mined potential block number = 96 hash = 8d2949…3f8a32 INFO [ 05 -01 | 10 :51:20 ] Commit new mining work number = 97 txs = 0 uncles = 0 elapsed = 772 .386µs true > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 124 .00042 On the sending machine we check again the balance. > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 405 .99958 We can see that indeed the 23 Ether got deducted, plus 0.00042 Ether to pay for the gas. The balance of the receiving machine got another 5 Ether for the mining action and got paid for the gas. If we perform a transaction from the second to the third machine, but perform the mining on the first machine, the third machine will only get the amount transferred while the first machine receives the mining bonus and the payment for the gas.","tags":"posts","url":"setting-up-private-ethereum-blockchain.html"},{"title":"Setting up Ether on my VPS","text":"Installation steps: jitsejan@jjschi2:~$ sudo apt-get install software-properties-common jitsejan@jjschi2:~$ sudo add-apt-repository -y ppa:ethereum/ethereum jitsejan@jjschi2:~$ sudo apt-get update jitsejan@jjschi2:~$ sudo apt-get install ethereum Get a new account with: jitsejan@jjschi2:~$ geth account new jitsejan@jjschi2:~$ geth account list Start a screen and start mining: jitsejan@jjschi2:~$ geth --mine To connect to this session and check your balance: jitsejan@jjschi2:~$ geth attach jitsejan@jjschi2:~$ > eth.getBalance ( eth.accounts [ 0 ]) I am not sure if my slow VPS will ever successfully mine any Ether, but since I want to know the basics of Ether, this is a good starting point.","tags":"posts","url":"setting-up-ether-on-vps.html"},{"title":"Creating a dashboard with MEAN.JS","text":"I simply keep track of the amount, price and location and try to display it in interesting graphs. This tutorial is mainly an attempt to understand the MEAN stack and work with D3.js. Installation of MEAN.JS I will skip the biggest part of installing MEAN.JS itself, since it is clearly explained on their website. Follow the instructions from MEAN.JS to generate the scaffold. jitsejan@jjvps:~/code$ npm install -g yo jitsejan@jjvps:~/code$ npm install -g generator-meanjs jitsejan@jjvps:~/code$ yo meanjs Answer 'No' to all questions. Initialization jitsejan@jjvps:~/code$ cd mean-dashboard/ jitsejan@jjvps:~/code/mean-dashboard$ git init jitsejan@jjvps:~/code/mean-dashboard$ git add . jitsejan@jjvps:~/code/mean-dashboard$ git commit -m \"Initial commit\" jitsejan@jjvps:~/code/mean-dashboard$ grunt At this point you should see the boilerplate of the MEAN.JS application. By changing the modules/core/client/views/home.client.view.html the frontpage can be changed. I stripped it down to the following. < section ng-controller = \"HomeController\" > < div class = \"jumbotron text-center\" > < div class = \"row\" > < p class = \"lead\" > JJ's dashboards </ p > </ div > </ div > </ section > I also changed the title of the page by adapting config/env/default.js. Change the database Change MongoDB url in config/env/development.js from: uri : process . env . MONGOHQ_URL || process . env . MONGOLAB_URI || 'mongodb://' + ( process . env . DB_1_PORT_27017_TCP_ADDR || 'localhost' ) + '/mean-dev' , to uri : process . env . MONGOHQ_URL || process . env . MONGOLAB_URI || 'mongodb://' + ( process . env . DB_1_PORT_27017_TCP_ADDR || 'localhost' ) + '/mean-dashboard' , Create a CRUD-module jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:crud-module refills Next I want to add more fields to the refills by changing the model. The fields I want are: Date of the refill [required] Amount of fuel [required] Litre price [required] Total cost [required] Address [required] Type of fuel [required] Longitude of the location [optional] Latitude of the location [optional] Distance [optional] To modify the model, change the code in modules/refills/server/models/refill.server.model.js. 'use strict' ; /** * Module dependencies. */ var mongoose = require ( 'mongoose' ), Schema = mongoose . Schema ; /** * Refill Schema */ var RefillSchema = new Schema ({ name : { type : String , default : '' , required : 'Please fill Refill name' , trim : true }, created : { type : Date , default : Date . now }, user : { type : Schema . ObjectId , ref : 'User' } }); mongoose . model ( 'Refill' , RefillSchema ); New code: var RefillSchema = new Schema ({ name : { type : String , default : '' , required : 'Please fill Refill name' , trim : true }, date : { type : Date , required : 'Please fill Refill date' }, kilometers : { type : Number , default : 0 , required : 'Please fill Refill kilometers' }, volume : { type : Number , default : 0 , required : 'Please fill Refill volume' }, price : { type : Number , default : 0 , required : 'Please fill Refill litre price' }, cost : { type : Number , default : 0 , required : 'Please fill Refill cost' }, created : { type : Date , default : Date . now }, user : { type : Schema . ObjectId , ref : 'User' } }); To create a new instance, I also updated the form in modules/refills/client/form-refill.client.view.html , but since this is pretty straight forward I will not show the code here. At this point you should be able to create and list the refills. Update the other views to show the new fields. Create the line chart Create a directive and choose refills as the module and 'line-chart' as the name. jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:angular-directive line-chart This will create the file refills/client/directives/line-chart.client.directive.js . Install D3 via Bower. jitsejan@jjvps:~/code/mean-dashboard$ bower install d3 --save Add the JS file to the default config in 'config/assets/default.js' to the client JS. ... 'public/lib/d3/d3.min.js' , ... The challenging part will be creating the D3 visualizations. The first graph is a line chart and the directive will have the following content. ( function () { 'use strict' ; angular . module ( 'refills' ) . directive ( 'lineChart' , lineChart ); lineChart . $inject = [ '$window' ]; function lineChart ( $window ) { return { template : '<svg width=\"960\" height=\"500\"></svg>' , restrict : 'EA' , link : function postLink ( scope , element , attrs ) { var d3 = $window . d3 ; var data = scope . vm . refills ; console . log ( d3 . version ); console . log ( data ); var svg = d3 . select ( 'svg' ), margin = { top : 20 , right : 20 , bottom : 30 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); // 2017-04-18T22:35:19.352Z var parseTime = d3 . utcParse ( '%Y-%m-%dT%H:%M:%S.%LZ' ); var x = d3 . scaleTime (). rangeRound ([ 0 , width ]); var y = d3 . scaleLinear (). rangeRound ([ height , 0 ]); var line = d3 . line () . x ( function ( d ) { return x ( parseTime ( d . date )); }) . y ( function ( d ) { return y ( d . kilometers ); }); x . domain ( d3 . extent ( data , function ( d ) { return parseTime ( d . date ); })); y . domain ( d3 . extent ( data , function ( d ) { return d . kilometers ; })); g . append ( 'g' ) . attr ( 'transform' , 'translate(0,' + height + ')' ) . call ( d3 . axisBottom ( x )) . select ( '.domain' ) . remove (); g . append ( 'g' ) . call ( d3 . axisLeft ( y )) . append ( 'text' ) . attr ( 'fill' , '#000' ) . attr ( 'transform' , 'rotate(-90)' ) . attr ( 'y' , 6 ) . attr ( 'dy' , '0.71em' ) . attr ( 'text-anchor' , 'end' ) . text ( 'Amount of kilometers' ); g . append ( 'path' ) . datum ( data ) . attr ( 'fill' , 'none' ) . attr ( 'stroke' , 'steelblue' ) . attr ( 'stroke-linejoin' , 'round' ) . attr ( 'stroke-linecap' , 'round' ) . attr ( 'stroke-width' , 1.5 ) . attr ( 'd' , line ); } }; } })(); Finally the graph needs to be added to a view. In my case I have added it to the list view of the refills. < div ng-if = \"vm.refills.$resolved && vm.refills.length\" > < div line-chart ></ div > </ div > Important note: I have used the ng-if statement in order to have my data available in the directive. Without the if-statement I was not able to get the data in properly. When you navigate to the refills list page the graph will now be visible. Some additional graphs. Please note that currently the graphs are not-responsive and rescaling the window will not resize the graph. Bar chart jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:angular-directive bar-chart modules/refills/client/directives/bar-chart.client.directive.js ( function () { 'use strict' ; angular . module ( 'refills' ) . directive ( 'barChart' , barChart ); barChart . $inject = [ '$window' ]; function barChart ( $window ) { return { template : '<svg class=\"bar-chart\" width=\"960\" height=\"500\"></svg>' , restrict : 'EA' , link : function postLink ( scope , element , attrs ) { var d3 = $window . d3 ; var data = scope . vm . refills ; // SVG var svg = d3 . select ( 'svg.bar-chart' ), margin = { top : 20 , right : 20 , bottom : 130 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); // Graph title g . append ( 'text' ) . attr ( 'x' , ( width / 2 )) . attr ( 'y' , 0 - ( margin . top / 3 )) . attr ( 'text-anchor' , 'middle' ) . style ( 'font-size' , '16px' ) . text ( 'Volume per refill' ); var parseTime = d3 . time . format ( '%Y-%m-%dT%H:%M:%S.%LZ' ). parse ; var x = d3 . scale . ordinal (). rangeRoundBands ([ 0 , width ], 0.5 ); var y = d3 . scale . linear (). range ([ height , 0 ]); x . domain ( data . map ( function ( d ) { return d . date ; })); y . domain ([ 0 , d3 . max ( data , function ( d ) { return d . volume ; })]); // X axis g . append ( 'g' ) . attr ( 'transform' , 'translate(0,' + height + ')' ) . attr ( 'class' , 'x axis' ) . call ( d3 . svg . axis (). scale ( x ). orient ( 'bottom' ). tickFormat ( function ( d ){ return parseTime ( d ). toISOString (). substring ( 0 , 10 );})) . selectAll ( 'text' ) . style ( 'text-anchor' , 'end' ) . attr ( 'dx' , '-.8em' ) . attr ( 'dy' , '.15em' ) . attr ( 'transform' , function ( d ) { return 'rotate(-65)' ; }); // Y axis g . append ( 'g' ) . call ( d3 . svg . axis (). scale ( y ). orient ( 'left' )); // Bars g . selectAll ( '.bar' ) . data ( data ) . enter (). append ( 'rect' ) . attr ( 'class' , 'bar' ) . attr ( 'x' , function ( d ) { return x ( d . date ); }) . attr ( 'width' , x . rangeBand ()) . attr ( 'y' , function ( d ) { return y ( d . volume ); }) . attr ( 'height' , function ( d ) { return height - y ( d . volume ); }); } }; } })(); World map jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:angular-directive world-map modules/refills/client/directives/world-map.client.directive.js ( function () { 'use strict' ; angular . module ( 'refills' ) . directive ( 'worldMap' , worldMap ); worldMap . $inject = [ '$window' ]; function worldMap ( $window ) { return { template : '<svg class=\"world-map\" width=\"960\" height=\"500\"></svg>' , restrict : 'EA' , link : function postLink ( scope , element , attrs ) { var d3 = $window . d3 ; var topojson = $window . topojson ; var data = scope . vm . refills ; var svg = d3 . select ( 'svg.world-map' ), margin = { top : 20 , right : 20 , bottom : 130 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , scale0 = ( width - 1 ) / 2 / Math . PI , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); svg . append ( 'rect' ) . attr ( 'class' , 'overlay' ) . attr ( 'width' , width ) . attr ( 'height' , height ); // Define the div for the tooltip var div = d3 . select ( 'body' ). append ( 'div' ) . attr ( 'class' , 'tooltip' ) . style ( 'opacity' , 0 ); var projection = d3 . geo . mercator () . center ([ 0 , 5 ]) . scale ( 200 ) . rotate ([ 0 , 0 ]); var path = d3 . geo . path () . projection ( projection ); var zoom = d3 . behavior . zoom () . on ( 'zoom' , function () { g . attr ( 'transform' , 'translate(' + d3 . event . translate . join ( ',' ) + ')scale(' + d3 . event . scale + ')' ); g . selectAll ( 'circle' ) . attr ( 'd' , path . projection ( projection )); g . selectAll ( 'path' ) . attr ( 'd' , path . projection ( projection )); }); // Graph title g . append ( 'text' ) . attr ( 'x' , ( width / 2 )) . attr ( 'y' , 0 - ( margin . top / 3 )) . attr ( 'text-anchor' , 'middle' ) . style ( 'font-size' , '16px' ) . text ( 'Locations' ); d3 . json ( 'https://unpkg.com/world-atlas@1/world/50m.json' , function ( error , world ) { if ( error ) throw error ; g . append ( 'path' ) . datum ({ type : 'Sphere' }) . attr ( 'class' , 'sphere' ) . attr ( 'd' , path ); g . append ( 'path' ) . datum ( topojson . merge ( world , world . objects . countries . geometries )) . attr ( 'class' , 'land' ) . attr ( 'd' , path ); g . append ( 'path' ) . datum ( topojson . mesh ( world , world . objects . countries , function ( a , b ) { return a !== b ; })) . attr ( 'class' , 'boundary' ) . attr ( 'd' , path ); g . selectAll ( 'circle' ) . data ( data ) . enter () . append ( 'circle' ) . attr ( 'cx' , function ( d ) { return projection ([ d . longitude , d . latitude ])[ 0 ]; }) . attr ( 'cy' , function ( d ) { return projection ([ d . longitude , d . latitude ])[ 1 ]; }) . attr ( 'r' , 5 ) . style ( 'fill' , 'red' ); }); svg . call ( zoom ) . call ( zoom . event ); } }; } })();","tags":"posts","url":"creating-dashboard-with-meanjs.html"},{"title":"Using R and Python together in a notebook","text":"Installation $ sudo apt-get install python-rpy2 $ conda install -c r r-essentials $ conda install -c r r-rjson $ pip install rpy2 Load the magic In [1]: % load_ext rpy2.ipython Create a dataframe Use Python and pandas to create a dataframe. In [2]: import pandas as pd df_from_python = pd . DataFrame ({ 'A' : [ 4 , 3 , 5 , 2 , 1 , 7 , 7 , 5 , 9 ], 'B' : [ 0 , 4 , 3 , 6 , 7 , 10 , 11 , 9 , 13 ], 'C' : [ 1 , 2 , 3 , 1 , 2 , 3 , 1 , 2 , 3 ]}) df_from_python Out[2]: A B C 0 4 0 1 1 3 4 2 2 5 3 3 3 2 6 1 4 1 7 2 5 7 10 3 6 7 11 1 7 5 9 2 8 9 13 3 Create plot in R Import the dataframe df with the -i argument. In [3]: %% R - i df_from_python require ( ggplot2 ) # Plot the DataFrame df ggplot ( data = df_from_python ) + geom_point ( aes ( x = A , y = B , color = C )) + ggtitle ( 'R scatter!' ) /home/jitsejan/anaconda3/lib/python3.6/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: Loading required package: ggplot2 warnings.warn(x, RRuntimeWarning) Create a dataframe using R Create a dataframe and export it using -o . In [4]: %% R - o df_from_r d <- c ( 2 , 6 , 6 , 8 , 4 , 7 , 5 , 9 , 3 ) e <- c ( 3 , 1 , 4 , 3 , 6 , 7 , 10 , 11 , 9 ) f <- c ( 3 , 1 , 2 , 3 , 1 , 2 , 3 , 1 , 2 ) df_from_r <- data.frame ( d , e , f ) Now the variable is available for Python. In [5]: df_from_r Out[5]: d e f 1 2.0 3.0 3.0 2 6.0 1.0 1.0 3 6.0 4.0 2.0 4 8.0 3.0 3.0 5 4.0 6.0 1.0 6 7.0 7.0 2.0 7 5.0 10.0 3.0 8 9.0 11.0 1.0 9 3.0 9.0 2.0 Create plot with Python In [6]: import matplotlib.pyplot as plt import matplotlib % matplotlib inline matplotlib . style . use ( 'ggplot' ) plt . scatter ( x = df_from_r [ 'd' ], y = df_from_r [ 'e' ], c = df_from_r [ 'f' ]) plt . title ( 'Python scatter!' ) plt . xlabel ( 'A' ) plt . ylabel ( 'B' ) fig = matplotlib . pyplot . gcf () fig . set_size_inches ( 10 , 10 )","tags":"posts","url":"r-in-notebook.html"},{"title":"MongoDB - First try","text":"My first experience with MongoDB . I will install MongoDB and Pymongo, insert some data and query it. Next step will be to tryout monary , but for this notebook it is out of scope. Installation steps Run the following commands to install MongoDB. jitsejan@vps:/$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6 jitsejan@jjvps:/$ echo \"deb [ arch=amd64,arm64 ] http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list jitsejan@jjvps:/$ sudo apt-get update jitsejan@jjvps:/$ sudo apt-get install -y mongodb-org Start MongoDB jitsejan@jjvps:/$ sudo service mongod start In [1]: ! tail /var/log/mongodb/mongod.log 2017-04-05T10:33:15.620-0400 W FTDC [initandlisten] Error checking directory '/sys/block': No such file or directory 2017-04-05T10:33:15.621-0400 I FTDC [initandlisten] Initializing full-time diagnostic data capture with directory '/var/lib/mongodb/diagnostic.data' 2017-04-05T10:33:15.621-0400 I NETWORK [thread1] waiting for connections on port 27017 2017-04-05T10:34:27.470-0400 I NETWORK [conn4] received client metadata from 127.0.0.1:34370 conn4: { driver: { name: \"PyMongo\", version: \"3.4.0\" }, os: { type: \"Linux\", name: \"debian stretch/sid\", architecture: \"x86_64\", version: \"2.6.32-042stab120.16\" }, platform: \"CPython 3.6.0.final.0\" } 2017-04-05T10:40:10.716-0400 I COMMAND [conn4] dropDatabase nintendo_db starting 2017-04-05T10:40:10.723-0400 I COMMAND [conn4] dropDatabase nintendo_db finished 2017-04-05T10:40:26.823-0400 I NETWORK [conn5] received client metadata from 127.0.0.1:42910 conn5: { driver: { name: \"PyMongo\", version: \"3.4.0\" }, os: { type: \"Linux\", name: \"debian stretch/sid\", architecture: \"x86_64\", version: \"2.6.32-042stab120.16\" }, platform: \"CPython 3.6.0.final.0\" } 2017-04-05T10:40:26.831-0400 I NETWORK [conn6] received client metadata from 127.0.0.1:42912 conn6: { driver: { name: \"PyMongo\", version: \"3.4.0\" }, os: { type: \"Linux\", name: \"debian stretch/sid\", architecture: \"x86_64\", version: \"2.6.32-042stab120.16\" }, platform: \"CPython 3.6.0.final.0\" } 2017-04-05T10:53:00.408-0400 I NETWORK [conn7] received client metadata from 127.0.0.1:35700 conn7: { driver: { name: \"PyMongo\", version: \"3.4.0\" }, os: { type: \"Linux\", name: \"debian stretch/sid\", architecture: \"x86_64\", version: \"2.6.32-042stab120.16\" }, platform: \"CPython 3.6.0.final.0\" } 2017-04-05T10:53:00.410-0400 I NETWORK [conn8] received client metadata from 127.0.0.1:35702 conn8: { driver: { name: \"PyMongo\", version: \"3.4.0\" }, os: { type: \"Linux\", name: \"debian stretch/sid\", architecture: \"x86_64\", version: \"2.6.32-042stab120.16\" }, platform: \"CPython 3.6.0.final.0\" } Install Python module Use Pymongo to communicate with MongoDB. In [2]: ! pip install pymongo Requirement already satisfied: pymongo in /home/jitsejan/anaconda3/lib/python3.6/site-packages Connect to MongoDB In [3]: import pymongo print ( pymongo . version ) client = pymongo . MongoClient ( 'mongodb://localhost:27017/' ) 3.4.0 Check which databases already exist In [4]: client . database_names () Out[4]: ['admin', 'local', 'nintendo_db'] Create a new database You can create a database by simply selecting the non-existing database. Only when a document is written, the database will physically be created. In [5]: db = client . nintendo_db db Out[5]: Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'nintendo_db') Create a new collection In [6]: characters = db . characters characters Out[6]: Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'nintendo_db'), 'characters') Create documents For simplicity, I will use the data that I have used in another notebook for creating documents. In [7]: import pandas as pd character_df = pd . read_csv ( '../data/nintendo_characters.csv' ) character_df Out[7]: id name description color occupation picture 0 2 Luigi This is Luigi green plumber https://upload.wikimedia.org/wikipedia/en/f/f1... 1 1 Mario This is Mario red plumber https://upload.wikimedia.org/wikipedia/en/9/99... 2 3 Peach My name is Peach pink princess https://s-media-cache-ak0.pinimg.com/originals... 3 4 Toad I like funghi red NaN https://upload.wikimedia.org/wikipedia/en/d/d1... In [8]: import json characters_dict = character_df . to_dict ( orient = 'records' ) print ( json . dumps ( characters_dict [ 0 ], indent = 4 )) { \"id\": 2, \"name\": \"Luigi\", \"description\": \"This is Luigi\", \"color\": \"green\", \"occupation\": \"plumber\", \"picture\": \"https://upload.wikimedia.org/wikipedia/en/f/f1/LuigiNSMBW.png\" } In [9]: for character in characters_dict : character_id = characters . insert_one ( character ) . inserted_id print ( character_id ) 58e504df6221ac77482eae4e 58e504df6221ac77482eae4f 58e504df6221ac77482eae50 58e504df6221ac77482eae51 Verify the new collection has been created In [10]: db . collection_names ( include_system_collections = False ) Out[10]: ['characters'] Verify the characters have been added Check the number of documents for the characters collection. In [11]: characters . count () Out[11]: 8 Check if Luigi is in the database. In [12]: characters . find_one ({ \"name\" : \"Luigi\" }) Out[12]: {'_id': ObjectId('58e501db6221ac72c8a3106b'), 'color': 'green', 'description': 'This is Luigi', 'id': 2, 'name': 'Luigi', 'occupation': 'plumber', 'picture': 'https://upload.wikimedia.org/wikipedia/en/f/f1/LuigiNSMBW.png'} Retrieve all documents in the characters collection In [13]: characters_from_db = list ( characters . find ({})) characters_from_db [ 0 ] Out[13]: {'_id': ObjectId('58e501db6221ac72c8a3106b'), 'color': 'green', 'description': 'This is Luigi', 'id': 2, 'name': 'Luigi', 'occupation': 'plumber', 'picture': 'https://upload.wikimedia.org/wikipedia/en/f/f1/LuigiNSMBW.png'} Find the red characters Only retrieve the name and description of the character. In [14]: red_characters = list ( characters . find ({ \"color\" : \"red\" }, { \"name\" : 1 , \"description\" : 1 , \"_id\" : 0 })) red_characters Out[14]: [{'description': 'This is Mario', 'name': 'Mario'}, {'description': 'I like funghi', 'name': 'Toad'}, {'description': 'This is Mario', 'name': 'Mario'}, {'description': 'I like funghi', 'name': 'Toad'}] Create a dataframe from the results In [15]: import pandas as pd red_characters_df = pd . DataFrame . from_dict ( red_characters ) red_characters_df Out[15]: description name 0 This is Mario Mario 1 I like funghi Toad 2 This is Mario Mario 3 I like funghi Toad Drop the database In [16]: client . drop_database ( 'nintendo_db' )","tags":"posts","url":"mongodb_first_try.html"},{"title":"First experiments with NLTK","text":"In [1]: import nltk # Run this the first time you use NLTK # nltk.download() In [2]: from nltk.tokenize import sent_tokenize , word_tokenize EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\" print ( sent_tokenize ( EXAMPLE_TEXT )) print ( word_tokenize ( EXAMPLE_TEXT )) ['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"] ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.'] Stop words Stopwords should be removed from the list of words. In [3]: from nltk.corpus import stopwords In [4]: stop_words = set ( stopwords . words ( 'english' )) In [5]: word_tokens = word_tokenize ( EXAMPLE_TEXT ) filtered_sentence = [ w for w in word_tokens if not w . lower () in stop_words ] print ( word_tokens ) print ( filtered_sentence ) ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.'] ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'weather', 'great', ',', 'Python', 'awesome', '.', 'sky', 'pinkish-blue', '.', \"n't\", 'eat', 'cardboard', '.'] Example one - Bob Dylan - Hurricane's lyrics Lets analyse the lyrics from Bob Dylan's Hurricane. First we get the lyrics from his website: In [6]: import lxml.html import requests response = requests . get ( 'https://bobdylan.com/songs/hurricane/' ) etree = lxml . html . fromstring ( response . text ) lyrics = etree . cssselect ( 'div[class*= \\' lyrics \\' ]' )[ 0 ] . text_content () Tokenize the lyrics based on words: In [7]: word_tokens = word_tokenize ( lyrics ) Remove the stopwords from the words: In [8]: filtered_sentence = [ w for w in word_tokens if not w . lower () in stop_words ] Create a word frequency graph In [9]: from nltk import FreqDist fd = FreqDist ( filtered_sentence ) fd . plot ( 30 , cumulative = False ) What we see in the graph above is that punctuation is disturbing the graph. We use a regular expression now to remove these characters and only select words. In [10]: from nltk.tokenize import RegexpTokenizer tokenizer = RegexpTokenizer ( r '\\w+' ) word_tokens = tokenizer . tokenize ( lyrics ) filtered_sentence = [ w for w in word_tokens if not w . lower () in stop_words ] Lets create the graph again: In [11]: fd = FreqDist ( filtered_sentence ) fd . plot ( 30 , cumulative = False ) Example two - Wikipedia about Nintendo In [12]: import wikipedia nintendo = wikipedia . page ( \"Nintendo\" ) word_tokens = word_tokenize ( nintendo . content ) In [13]: filtered_sentence = [ w for w in word_tokens if not w . lower () in stop_words ] In [14]: fd = FreqDist ( filtered_sentence ) fd . plot ( 10 , cumulative = False ) In [15]: tokenizer = RegexpTokenizer ( r '\\w+' ) word_tokens = tokenizer . tokenize ( nintendo . content ) In [16]: filtered_sentence = [ w for w in word_tokens if not w . lower () in stop_words ] In [17]: fd = FreqDist ( filtered_sentence ) fd . plot ( 10 , cumulative = False )","tags":"posts","url":"nltk-first-experiments.html"},{"title":"Basic search with Elasticsearch","text":"Inspired by this tutorial I tried to continue investigating Elasticsearch since I would like to use a fast indexing tool for the data I am gathering and the applications I am developing. Install the Python library for Elasticsearch https://elasticsearch-py.readthedocs.io/en/master/ $ pip install elasticsearch Note: on my Mac I installed Elasticsearch through Brew $ brew install elasticsearch $ brew services start elasticsearch Creating the data Read the CSV files Read the character data In [1]: import pandas as pd character_df = pd . read_csv ( 'data/nintendo_characters.csv' ) character_df Out[1]: id name description color occupation picture 0 2 Luigi This is Luigi green plumber https://upload.wikimedia.org/wikipedia/en/f/f1... 1 1 Mario This is Mario red plumber https://upload.wikimedia.org/wikipedia/en/9/99... 2 3 Peach My name is Peach pink princess https://s-media-cache-ak0.pinimg.com/originals... 3 4 Toad I like funghi red NaN https://upload.wikimedia.org/wikipedia/en/d/d1... Remove the NaN In [2]: character_df . occupation = character_df . occupation . fillna ( '' ) Read the world data In [3]: world_df = pd . read_csv ( 'data/super_mario_3_worlds.csv' , sep = ';' ) world_df Out[3]: id world name image description picture 0 1 World 1 Grass Land Grass Land.PNG Grass Land is the first world of the game. It ... https://www.mariowiki.com/images/thumb/f/fa/Gr... 1 2 World 2 Desert Land World2SMB3.PNG Desert Land is the second world of the game. I... https://www.mariowiki.com/images/thumb/d/d1/Wo... 2 3 World 3 Water Land Sea Side.PNG Water Land is a water-themed region that was r... https://www.mariowiki.com/images/thumb/b/b7/Se... 3 4 World 4 Giant Land SMAS-Big Island Map.PNG Giant Land is mainly composed of an island in ... https://www.mariowiki.com/images/thumb/9/9c/SM... 4 5 World 5 Sky Land Sky world.PNG Sky Land is the world that has been conquered ... https://www.mariowiki.com/images/thumb/6/69/Sk... 5 6 World 6 Ice Land SMB36.PNG Ice Land is an area covered in snow and ice. T... https://www.mariowiki.com/images/thumb/4/40/SM... 6 7 World 7 Pipe Land Pipe maze.PNG Pipe Land is a series of small islands in a ne... https://www.mariowiki.com/images/thumb/a/aa/Pi... 7 8 World 8 Dark Land Dark land2.PNG The eighth and final world is ruled by King Bo... https://www.mariowiki.com/images/thumb/0/01/Da... 8 9 World 9 Warp Zone World 9.PNG World 9 is only accessible by a Warp Whistle. ... https://www.mariowiki.com/images/thumb/0/09/Wo... Setup Elasticsearch Create the parameters In [4]: ES_HOST = { \"host\" : \"localhost\" , \"port\" : 9200 } INDEX_NAME = 'nintendo' TYPE_NAME = 'character' ID_FIELD = 'id' Setup the Elasticsearch connector In [5]: from elasticsearch import Elasticsearch es = Elasticsearch ( hosts = [ ES_HOST ]) Create the index Create the index for nintendo if it does not exists, otherwise first delete it. In [6]: if es . indices . exists ( INDEX_NAME ): print ( \"Deleting the ' %s ' index\" % ( INDEX_NAME )) res = es . indices . delete ( index = INDEX_NAME ) print ( \"Acknowledged: ' %s '\" % ( res [ 'acknowledged' ])) request_body = { \"settings\" : { \"number_of_shards\" : 1 , \"number_of_replicas\" : 0 } } print ( \"Creating the ' %s ' index!\" % ( INDEX_NAME )) res = es . indices . create ( index = INDEX_NAME , body = request_body ) print ( \"Acknowledged: ' %s '\" % ( res [ 'acknowledged' ])) Deleting the 'nintendo' index Acknowledged: 'True' Creating the 'nintendo' index! Acknowledged: 'True' Create the bulk data Loop through the dataframe and create the data to insert into the index. In [7]: bulk_data = [] In [8]: for index , row in character_df . iterrows (): data_dict = {} for i in range ( len ( row )): data_dict [ character_df . columns [ i ]] = row [ i ] op_dict = { \"index\" : { \"_index\" : 'nintendo' , \"_type\" : 'character' , \"_id\" : data_dict [ 'id' ] } } bulk_data . append ( op_dict ) bulk_data . append ( data_dict ) In [9]: for index , row in world_df . iterrows (): data_dict = {} for i in range ( len ( row )): data_dict [ world_df . columns [ i ]] = row [ i ] op_dict = { \"index\" : { \"_index\" : 'nintendo' , \"_type\" : 'world' , \"_id\" : data_dict [ 'id' ] } } bulk_data . append ( op_dict ) bulk_data . append ( data_dict ) Insert the data into the index In [10]: import json print ( \"Bulk indexing...\" ) res = es . bulk ( index = INDEX_NAME , body = bulk_data , refresh = True ) Bulk indexing... Query using CURL In [11]: ! curl -XGET 'http://localhost:9200/_search?pretty' { \"took\" : 5, \"timed_out\" : false, \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"hits\" : { \"total\" : 3295, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"nintendo\", \"_type\" : \"character\", \"_id\" : \"2\", \"_score\" : 1.0, \"_source\" : { \"id\" : 2, \"name\" : \"Luigi\", \"description\" : \"This is Luigi\", \"color\" : \"green\", \"occupation\" : \"plumber\", \"picture\" : \"https://upload.wikimedia.org/wikipedia/en/f/f1/LuigiNSMBW.png\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"character\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"id\" : 1, \"name\" : \"Mario\", \"description\" : \"This is Mario\", \"color\" : \"red\", \"occupation\" : \"plumber\", \"picture\" : \"https://upload.wikimedia.org/wikipedia/en/9/99/MarioSMBW.png\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"character\", \"_id\" : \"3\", \"_score\" : 1.0, \"_source\" : { \"id\" : 3, \"name\" : \"Peach\", \"description\" : \"My name is Peach\", \"color\" : \"pink\", \"occupation\" : \"princess\", \"picture\" : \"https://s-media-cache-ak0.pinimg.com/originals/d2/4d/77/d24d77cfbba789256c9c1afa1f69b385.png\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"character\", \"_id\" : \"4\", \"_score\" : 1.0, \"_source\" : { \"id\" : 4, \"name\" : \"Toad\", \"description\" : \"I like funghi\", \"color\" : \"red\", \"occupation\" : \"\", \"picture\" : \"https://upload.wikimedia.org/wikipedia/en/d/d1/Toad_3D_Land.png\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"id\" : 1, \"world\" : \"World 1\", \"name\" : \"Grass Land\", \"image\" : \"Grass Land.PNG\", \"description\" : \"Grass Land is the first world of the game. It was attacked by Larry Koopa, who stole the wand of the Grass Land king and turned him into a dog (or a Cobrat from Super Mario Bros. 2 in the remake). The landscape itself is mainly composed of plains, surrounded by hills and even some cliffs in the south. A fortress can be found in the middle of Grass Land, and the king's castle lies to the southeast, surrounded by a circular moat. The enemies Mario encounters here are regular ones, like Goombas, Koopa Troopas and Piranha Plants. The world features a Spade Panel, two Toad Houses and six levels, of which four have to be cleared to reach the king's castle.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/f/fa/Grass_Land.PNG/200px-Grass_Land.PNG\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"2\", \"_score\" : 1.0, \"_source\" : { \"id\" : 2, \"world\" : \"World 2\", \"name\" : \"Desert Land\", \"image\" : \"World2SMB3.PNG\", \"description\" : \"Desert Land is the second world of the game. It is a region within a vast desert, filled with sand, palm trees and some pyramids. A fortress is located in the west part of the desert, and a quicksand field can also be found, as well as a great pyramid that the player needs to traverse in order to reach the king's castle. The king was attacked by Morton Koopa Jr., who turned him into a spider (or a Hoopster from Super Mario Bros. 2 in the remake). The world features two Spade Panels and three Toad Houses, of which one lies in a secret area behind a rock that needs to be crushed by a Hammer. The boulder also hides two Fire Brothers which stole the last Warp Whistle. Four of the five levels need to be cleared to get to the great pyramid and the castle. Desert Land houses many desert-related creatures like Fire Snakes and the extremely rare Angry Sun.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/d/d1/World2SMB3.PNG/200px-World2SMB3.PNG\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"3\", \"_score\" : 1.0, \"_source\" : { \"id\" : 3, \"world\" : \"World 3\", \"name\" : \"Water Land\", \"image\" : \"Sea Side.PNG\", \"description\" : \"Water Land is a water-themed region that was raided by Wendy O. Koopa, who turned the king into a kappa (or a Dino-Rhino from Super Mario World in the remake). While some levels take place on solid ground, most of the levels and even one of the worlds two fortresses involve water in a certain way. At the northern part of the world map, Mario will encounter drawbridges that open and close in a set pattern. The world's castle is located far to the east on a small, remote island that is only accessible through a Warp Pipe. A boat can be unlocked by using a Hammer on a rock in the south. Through it, the player can reach some bonus Spade Panels and Toad Houses. Water Land contains nine levels in total, of which one can be skipped if a certain drawbridge is closed, and houses several water creatures like Bloopers, Cheep Cheeps, and Big Berthas. The world also introduces a very rare Boo known as a Stretch.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/b/b7/Sea_Side.PNG/200px-Sea_Side.PNG\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"4\", \"_score\" : 1.0, \"_source\" : { \"id\" : 4, \"world\" : \"World 4\", \"name\" : \"Giant Land\", \"image\" : \"SMAS-Big Island Map.PNG\", \"description\" : \"Giant Land is mainly composed of an island in the vague shape of a Koopa. It is a relatively green island with plants growing on it that resemble Fire Flowers. The castle at the west coast of the island was attacked by Iggy Koopa, who transformed the king into an orange dinosaur (or Donkey Kong Jr. in the remake). The world has two fortresses, one on the east side and one on a small island in a lake in the world's center. The most prominent feature of Giant Land, which gives this world its name, is the fact that many enlarged versions of regular enemies, blocks, and environmental features can be found here. The world features four Toad Houses, two Spade Panels and six levels, of which five need to be cleared to reach the king's castle.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/9/9c/SMAS-Big_Island_Map.PNG/200px-SMAS-Big_Island_Map.PNG\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"5\", \"_score\" : 1.0, \"_source\" : { \"id\" : 5, \"world\" : \"World 5\", \"name\" : \"Sky Land\", \"image\" : \"Sky world.PNG\", \"description\" : \"Sky Land is the world that has been conquered by Roy Koopa, who has turned its king into a condor (Albatoss in the remake). It is divided into two parts: a ground part and a sky part. The player begins on the ground. The most notable feature of this area is the possibility to gain the Goomba's Shoe, an item that can be obtained in level 5-3. After clearing the levels on the ground, the player can reach a spiraling tower that reaches up to the sky. The main part of the level is located here, and there are also some creatures exclusively to this realm, namely the Para-Beetle. After clearing the tower that serves as a link between the two areas, the player can go back to the ground, but they will have to clear the tower again on their way up. If the Koopaling isn't defeated at the first try, his Airship will be able to move freely between sky and ground. There are nine levels in total, three Spade Panels, three Toad Houses and two fortresses. The castle is on the southwest part of the sky part.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/6/69/Sky_world.PNG/200px-Sky_world.PNG\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"6\", \"_score\" : 1.0, \"_source\" : { \"id\" : 6, \"world\" : \"World 6\", \"name\" : \"Ice Land\", \"image\" : \"SMB36.PNG\", \"description\" : \"Ice Land is an area covered in snow and ice. The castle was attacked by Lemmy Koopa - who has turned its king into a fur seal (Monty Mole in the remake) and Mario has to venture there and reclaim the magic wand just like in the previous worlds. Before he can reach the castle however, the player has to navigate Mario through the levels of Ice Land. These levels feature frozen ground which makes movement more difficult, as Mario has poor footing on them and is likely to slip off into a bottomless pit. In some levels, the player can find ice blocks that contain coins or enemies. These blocks can only be melted with one of Fire Mario's fireballs. There are ten levels in total, three Spade Panels, two Toad Houses, and three fortresses. The castle is far to the east near the sea.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/4/40/SMB36.PNG/200px-SMB36.PNG\" } } ] } } Search all worlds: curl -XGET 'http://localhost:9200/nintendo/world/_search?pretty' Pagination: curl -XGET 'http://localhost:9200/nintendo/world/_search?size=2&from=2&pretty' Specify the fields you want to be returned: curl -XGET 'http://localhost:9200/nintendo/character/_search?pretty&q=name:Luigi&fields=name,occupation' Search for the word 'pipe': curl -XGET 'http://localhost:9200/nintendo/world/_search?pretty&q=pipe'","tags":"posts","url":"basic-search-with-elasticsearch.html"},{"title":"Color recognition of images using OpenCV","text":"As a training for myself to get more familiar with image classification and OpenCV I followed the tutorial on http://www.pyimagesearch.com/2014/08/04/opencv-python-color-detection/ . I modified it slightly to fit my environment. Imports In [1]: import cv2 from IPython.core.interactiveshell import InteractiveShell import matplotlib.pyplot as plt import numpy as np import os Settings In [2]: InteractiveShell . ast_node_interactivity = \"all\" % matplotlib inline DATA_DIR = '../input/images/' Retrieve the first image for each character. In [3]: images = { f [: - 4 ]: DATA_DIR + f for f in os . listdir ( DATA_DIR ) if f . endswith ( '.jpg' ) and '_01' in f } images Out[3]: {'luigi_01': '../input/images/luigi_01.jpg', 'mario_01': '../input/images/mario_01.jpg', 'peach_01': '../input/images/peach_01.jpg', 'toad_01': '../input/images/toad_01.jpg'} Display the Mario image using matplotlib In [4]: img = plt . imread ( images [ 'mario_01' ]) plt . imshow ( img ) plt . axis ( 'off' ); Color detection First read the images using OpenCV. In [5]: mario_image = cv2 . imread ( images [ 'mario_01' ]) luigi_image = cv2 . imread ( images [ 'luigi_01' ]) peach_image = cv2 . imread ( images [ 'peach_01' ]) toad_image = cv2 . imread ( images [ 'toad_01' ]) Next we set up the boundaries for the images. Since I am working with Mario, Luigi, Toad and Peach the easy choices are blue, red, green, pink and white. Note that the colors for OpenCV are not in in RGB order but in BGR. For example for the blue boundary the 0 is the lower bound for blue and 200 the upper bound. In [6]: blue_boundary = ([ 0 , 46 , 0 ], [ 200 , 100 , 112 ]) red_boundary = ([ 0 , 0 , 150 ], [ 40 , 21 , 255 ]) green_boundary = ([ 23 , 140 , 33 ], [ 51 , 191 , 65 ]) pink_boundary = ([ 141 , 79 , 234 ], [ 201 , 144 , 255 ]) white_boundary = ([ 200 , 200 , 200 ], [ 252 , 252 , 252 ]) # Combine the bounaries in one list boundaries = [ blue_boundary , red_boundary , green_boundary , pink_boundary , white_boundary , ] Make a list of the images for the four characters. In [7]: input_images = [ { 'image' : toad_image }, { 'image' : peach_image }, { 'image' : mario_image }, { 'image' : luigi_image } ] Now loop through the images first and for each image loop through the boundaries. The In [8]: # Loop through the images for item in input_images : outputs = [] # Loop through the boundaries for ( lower , upper ) in boundaries : # Create NumPy arrays from the boundaries lower = np . array ( lower , dtype = \"uint8\" ) upper = np . array ( upper , dtype = \"uint8\" ) # Check which pixels fall in between the boundaries and create a mask mask = cv2 . inRange ( item [ 'image' ], lower , upper ) # Apply the mask to the input image output = cv2 . bitwise_and ( item [ 'image' ], item [ 'image' ], mask = mask ) outputs . append ( output ) # Show the mask for each boundary plt . figure () plt . imshow ( cv2 . cvtColor ( np . hstack ([ item [ 'image' ], outputs [ 0 ], outputs [ 1 ], outputs [ 2 ], outputs [ 3 ], outputs [ 4 ]]), cv2 . COLOR_BGR2RGB )) plt . axis ( 'off' ); item [ 'outputs' ] = outputs plt . axis ( 'off' );","tags":"posts","url":"color-recognition-opencv.html"},{"title":"Mocking in unittests in Python","text":"Example - Using mock Imports In [35]: import sys import unittest # Python compatibility if sys . version_info < ( 3 , 3 ): import mock else : import unittest.mock as mock The mock object Create a mock object: In [8]: m = mock . Mock () Show the attributes of the object: In [9]: dir ( m ) Out[9]: ['assert_any_call', 'assert_called_once_with', 'assert_called_with', 'assert_has_calls', 'assert_not_called', 'attach_mock', 'call_args', 'call_args_list', 'call_count', 'called', 'configure_mock', 'method_calls', 'mock_add_spec', 'mock_calls', 'reset_mock', 'return_value', 'side_effect'] Print a fake attribute. It doesn't exist, but will be shown. In [12]: m . fake_attribute Out[12]: <Mock name='mock.fake_attribute' id='2582717621248'> Again show the object. This time the fake_attribute will be shown too. In [13]: dir ( m ) Out[13]: ['assert_any_call', 'assert_called_once_with', 'assert_called_with', 'assert_has_calls', 'assert_not_called', 'attach_mock', 'call_args', 'call_args_list', 'call_count', 'called', 'configure_mock', 'fake_attribute', 'method_calls', 'mock_add_spec', 'mock_calls', 'reset_mock', 'return_value', 'side_effect'] Set a return value for the newly introduced attribute and retrieve it. In [14]: m . fake_attribute . return_value = \"Fake return value\" m . fake_attribute () Out[14]: 'Fake return value' Create another attribute, but this time assign a (fake) function to its return_value . In [108]: def print_fake_value (): print ( \"Fake function is called!\" ) m . another_attribute . return_value = print_fake_value m . another_attribute () Out[108]: <Mock name='mock.another_attribute.print_fake_value()' id='2582735302608'> Same exercise with a function with an argument: In [21]: def print_fake_value_with_arg ( argument ): print ( \"Fake argument %s \" % argument ) m . the_third_attribute . return_value = print_fake_value_with_arg m . the_third_attribute ( 'Print me' ) Out[21]: <function __main__.print_fake_value_with_arg> You can also create a custom exception by using the side_effect . It can be an exception, callable or an iterable. In [22]: m . some_function . side_effect = ValueError ( \"Super error\" ) m . some_function () --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-22-197c49fcbb5e> in <module> () 1 m . some_function . side_effect = ValueError ( \"Super error\" ) ----> 2 m . some_function ( ) C:\\Users\\J-J van Waterschoot\\Anaconda3\\lib\\unittest\\mock.py in __call__ (_mock_self, *args, **kwargs) 915 # in the signature 916 _mock_self . _mock_check_sig ( * args , ** kwargs ) --> 917 return _mock_self . _mock_call ( * args , ** kwargs ) 918 919 C:\\Users\\J-J van Waterschoot\\Anaconda3\\lib\\unittest\\mock.py in _mock_call (_mock_self, *args, **kwargs) 971 if effect is not None : 972 if _is_exception ( effect ) : --> 973 raise effect 974 975 if not _callable ( effect ) : ValueError : Super error To make it an iterable, the following can be used. By calling the mock several times, it will return the values until the limit of the range is reached. In [23]: m . some_iteration_thing . side_effect = range ( 2 ) m . some_iteration_thing () Out[23]: 0 In [24]: m . some_iteration_thing () Out[24]: 1 In [25]: m . some_iteration_thing () --------------------------------------------------------------------------- StopIteration Traceback (most recent call last) <ipython-input-25-218e10d9f5de> in <module> () ----> 1 m . some_iteration_thing ( ) C:\\Users\\J-J van Waterschoot\\Anaconda3\\lib\\unittest\\mock.py in __call__ (_mock_self, *args, **kwargs) 915 # in the signature 916 _mock_self . _mock_check_sig ( * args , ** kwargs ) --> 917 return _mock_self . _mock_call ( * args , ** kwargs ) 918 919 C:\\Users\\J-J van Waterschoot\\Anaconda3\\lib\\unittest\\mock.py in _mock_call (_mock_self, *args, **kwargs) 974 975 if not _callable ( effect ) : --> 976 result = next ( effect ) 977 if _is_exception ( result ) : 978 raise result StopIteration : Finally you can also pass a callable to the side_effect , by doing the following: In [27]: def side_function (): print ( 'This is a side function!' ) m . some_simple_function . side_effect = side_function () m . some_simple_function () This is a side function! Out[27]: <Mock name='mock.some_simple_function()' id='2582717767408'> In [28]: def side_function_with_arg ( argument ): print ( 'This is a side function with argument: %s ' % argument ) m . some_simple_function_with_arg . side_effect = side_function_with_arg m . some_simple_function_with_arg ( 'No argument!' ) This is a side function with argument: No argument! An important function of the side_effect is that you can pass it a class, which can be helpful if you are testing code and verify the behaviour of the class In [29]: class Car ( object ): def __init__ ( self , name ): self . _name = name def print_name ( self ): print ( \"Name: %s \" % self . _name ) m . a_car_attribute . side_effect = Car car = m . a_car_attribute . side_effect ( 'My red car' ) car Out[29]: <__main__.Car at 0x25955fdd6a0> In [30]: car . print_name () Name: My red car Testing Castle Lets define the castle class: In [99]: class Castle ( object ): def __init__ ( self , name ): self . name = name self . boss = 'Bowser' self . world = 'Grass Land' def access ( self , character ): if character . powerup == 'Super Mushroom' : return True else : return False def get_boss ( self ): return self . boss def get_world ( self ): return self . world We will also define a character class: In [100]: class Character ( object ): def __init__ ( self , name ): self . name = name self . powerup = '' def powerup ( self , powerup ): self . powerup = powerup def get_powerup ( self ): return self . powerup Finally we will define a testclass to test the functionality of the classes. In [101]: class CharacterTestClass ( unittest . TestCase ): \"\"\" Defines the tests for the Character class \"\"\" def setUp ( self ): \"\"\" Set the castle for the test cases \"\"\" self . castle = Castle ( 'Bowsers Castle' ) def test_mock_access_denied ( self ): \"\"\" Access denied for star powerup \"\"\" mock_character = mock . Mock ( powerup = 'Starman' ) self . assertFalse ( self . castle . access ( mock_character )) def test_mock_access_granted ( self ): \"\"\" Access granted for mushroom powerup \"\"\" mock_character = mock . Mock ( powerup = 'Super Mushroom' ) self . assertTrue ( self . castle . access ( mock_character )) def test_default_castle_boss ( self ): \"\"\" Verifty the default boss is Bowser \"\"\" self . assertEqual ( self . castle . get_boss (), \"Bowser\" ) def test_default_castle_world ( self ): \"\"\" Verify the default world is Grass Land \"\"\" self . assertEqual ( self . castle . get_world (), \"Grass Land\" ) # Mock a class method @mock . patch . object ( Castle , 'get_boss' ) def test_mock_castle_boss ( self , mock_get_boss ): mock_get_boss . return_value = \"Hammer Bro\" self . assertEqual ( self . castle . get_boss (), \"Hammer Bro\" ) self . assertEqual ( self . castle . get_world (), \"Grass Land\" ) # Mock an instance @mock . patch ( __name__ + '.Castle' ) def test_mock_castle ( self , MockCastle ): instance = MockCastle instance . get_boss . return_value = \"Toad\" instance . get_world . return_value = \"Desert Land\" self . castle = Castle self . assertEqual ( self . castle . get_boss (), \"Toad\" ) self . assertEqual ( self . castle . get_world (), \"Desert Land\" ) # Mock an instance method def test_mock_castle_instance_method ( self ): # Boss is still Bowser self . assertNotEqual ( self . castle . get_boss (), \"Koopa Troopa\" ) # Set a return_value for the get_boss method self . castle . get_boss = mock . Mock ( return_value = \"Koopa Troopa\" ) # Boss is Koopa Troopa now self . assertEqual ( self . castle . get_boss (), \"Koopa Troopa\" ) def test_castle_with_more_bosses ( self ): multi_boss_castle = mock . Mock () # Set a list as side_effect for the get_boss method multi_boss_castle . get_boss . side_effect = [ \"Goomba\" , \"Boo\" ] # First value is Goomba self . assertEqual ( multi_boss_castle . get_boss (), \"Goomba\" ) # Second value is Boo self . assertEqual ( multi_boss_castle . get_boss (), \"Boo\" ) # Third value does not exist and raises a StopIteration self . assertRaises ( StopIteration , multi_boss_castle . get_boss ) def test_calls_to_castle ( self ): self . castle . access = mock . Mock () self . castle . access . return_value = \"No access\" # We should retrieve no access for everybody self . assertEqual ( self . castle . access ( 'Let me in' ), \"No access\" ) self . assertEqual ( self . castle . access ( 'Let me in, please' ), \"No access\" ) self . assertEqual ( self . castle . access ( 'Let me in, please sir!' ), \"No access\" ) # Verify the length of the arguments list self . assertEqual ( len ( self . castle . access . call_args_list ), 3 ) Run the test suite In [102]: import sys suite = unittest . TestLoader () . loadTestsFromTestCase ( CharacterTestClass ) unittest . TextTestRunner ( verbosity = 4 , stream = sys . stderr ) . run ( suite ) test_calls_to_castle (__main__.CharacterTestClass) ... ok test_castle_with_more_bosses (__main__.CharacterTestClass) ... ok test_default_castle_boss (__main__.CharacterTestClass) Verifty the default boss is Bowser ... ok test_default_castle_world (__main__.CharacterTestClass) Verify the default world is Grass Land ... ok test_mock_access_denied (__main__.CharacterTestClass) Access denied for star powerup ... ok test_mock_access_granted (__main__.CharacterTestClass) Access granted for mushroom powerup ... ok test_mock_castle (__main__.CharacterTestClass) ... ok test_mock_castle_boss (__main__.CharacterTestClass) ... ok test_mock_castle_instance_method (__main__.CharacterTestClass) ... ok ---------------------------------------------------------------------- Ran 9 tests in 0.016s OK Out[102]: <unittest.runner.TextTestResult run=9 errors=0 failures=0> In [103]: class CharacterCastleTestClass ( unittest . TestCase ): \"\"\" Defines the tests for the Character and Castle class together \"\"\" @mock . patch ( __name__ + '.Castle' ) @mock . patch ( __name__ + '.Character' ) def test_mock_castle_and_character ( self , MockCharacter , MockCastle ): # Note the order of the arguments of this test MockCastle . name = 'Mocked Castle' MockCharacter . name = 'Mocked Character' self . assertEqual ( Castle . name , 'Mocked Castle' ) self . assertEqual ( Character . name , 'Mocked Character' ) def test_fake_powerup ( self ): character = Character ( \"Sentinel Character\" ) character . powerup = mock . Mock () character . powerup . return_value = mock . sentinel . fake_superpower self . assertEqual ( character . powerup (), mock . sentinel . fake_superpower ) def test_castle_with_more_powerups ( self ): self . castle = Castle ( 'Beautiful Castle' ) multi_characters = mock . Mock () # Set a list as side_effect for the get_boss method multi_characters . get_powerup . side_effect = [ \"mushroom\" , \"star\" ] # First value is mushroom self . assertEqual ( multi_characters . get_powerup (), \"mushroom\" ) # Second value is star self . assertEqual ( multi_characters . get_powerup (), \"star\" ) # Third value does not exist and raises a StopIteration self . assertRaises ( StopIteration , multi_characters . get_powerup ) In [104]: suite = unittest . TestLoader () . loadTestsFromTestCase ( CharacterCastleTestClass ) unittest . TextTestRunner ( verbosity = 2 , stream = sys . stderr ) . run ( suite ) test_castle_with_more_powerups (__main__.CharacterCastleTestClass) ... ok test_fake_powerup (__main__.CharacterCastleTestClass) ... ok test_mock_castle_and_character (__main__.CharacterCastleTestClass) ... ok ---------------------------------------------------------------------- Ran 3 tests in 0.006s OK Out[104]: <unittest.runner.TextTestResult run=3 errors=0 failures=0>","tags":"posts","url":"mocking-in-unittests-in-python.html"},{"title":"Unittesting in a Jupyter notebook","text":"Example - Unittesting In this example I show how to run a unittest within your Jupyter Notebook with two simple classes. Create two classes Castle class The important method for the castle is the access method. It will grant access when the character has the powerup 'Super Mushroom'. In [1]: class Castle ( object ): def __init__ ( self , name ): self . name = name self . _boss = 'Bowser' self . _world = 'Grass Land' def access ( self , character ): if character . powerup == 'Super Mushroom' : return True else : return False def get_boss ( self ): return self . _boss def get_world ( self ): return self . _world Character class By default the powerup of a character is empty. In [2]: class Character ( object ): def __init__ ( self , name ): self . name = name self . powerup = '' def powerup ( self , powerup ): self . powerup = powerup def get_powerup ( self ): return self . powerup Create a test class We will create two characters and test that only the right powerup gives access to the castle. In [5]: import unittest class CharacterTestClass ( unittest . TestCase ): \"\"\" Defines the tests for the Character class \"\"\" def setUp ( self ): \"\"\" Set the castle for the test cases \"\"\" self . castle = Castle ( 'Bowsers Castle' ) def test_default_cannot_access ( self ): \"\"\" Default can not access \"\"\" default = Character ( 'Default' ) self . assertFalse ( self . castle . access ( default )) def test_mario_cannot_access ( self ): \"\"\" Mario cannot access \"\"\" mario = Character ( 'Mario' ) mario . powerup = 'Starman' self . assertFalse ( self . castle . access ( mario )) def test_peach_can_access ( self ): \"\"\" Peach can access \"\"\" peach = Character ( 'Peach' ) peach . powerup = 'Super Mushroom' self . assertTrue ( self . castle . access ( peach )) def test_default_castle_boss ( self ): \"\"\" Verifty the default boss is Bowser \"\"\" self . assertEqual ( self . castle . get_boss (), \"Bowser\" ) def test_default_castle_world ( self ): \"\"\" Verify the default world is Grass Land \"\"\" self . assertEqual ( self . castle . get_world (), \"Grass Land\" ) Run the test suite In [6]: import sys suite = unittest . TestLoader () . loadTestsFromTestCase ( CharacterTestClass ) unittest . TextTestRunner ( verbosity = 4 , stream = sys . stderr ) . run ( suite ) test_default_cannot_access (__main__.CharacterTestClass) Default can not access ... ok test_default_castle_boss (__main__.CharacterTestClass) Verifty the default boss is Bowser ... ok test_default_castle_world (__main__.CharacterTestClass) Verify the default world is Grass Land ... ok test_mario_cannot_access (__main__.CharacterTestClass) Mario cannot access ... ok test_peach_can_access (__main__.CharacterTestClass) Peach can access ... ok ---------------------------------------------------------------------- Ran 5 tests in 0.004s OK Out[6]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>","tags":"posts","url":"unittesting-in-jupyter-notebook.html"},{"title":"Add CSS to Splunk dashboard","text":"The file should be located in %SPLUNK_HOME%/etc/apps/<APPNAME>/appserver/static/ If the file is called dashboard.css , the file will be automatically applied to all dashboards within this application. If you give it a custom name, you need to include it explicitly in the stylesheet attribute in the XML of your dashboard like the following <form stylesheet=\"mycustomstyle.css\"> Currently the base of my dashboard.css looks like the stylesheet of this webpage, to make it clear that I am in my own application. @ import url ( 'https://fonts.googleapis.com/css?family=Raleway' ) ; body , html { background-color : white ; font-family : \"Raleway\" ; height : 100 % ; } header { background-color : rgb ( 102 , 153 , 255 ); border : 0 px ; background-image : url ( './images/Bar.GIF' ); background-repeat : repeat-x ; background-position : bottom ; padding : 0 px 0 px 16 px 0 px ; } . dashboard-body { background-color : white ; height : 100 % ; } . dashboard { background-color : white ; } footer { margin-top : 0 px ; padding-bottom : 50 px ; font-size : small ; color : rgb ( 200 , 200 , 200 ); background-image : url ( './images/Bottom.GIF' ); background-repeat : repeat-x ; background-position : bottom ; } h1 , h2 , a { color : rgb ( 102 , 153 , 255 ); } Note that when a new stylesheet is added, Splunkweb needs a restart. If you simply update the stylesheet, run the following command: http://SPLUNKHOST/en-US/_bump","tags":"posts","url":"add-css-splunk-dashboard.html"},{"title":"Create a Splunk app to monitor Nginx","text":"Using the web interface of Splunk you can easily add a new app. This will create the following structure: nginxwatcher/ |-- bin | `-- README |-- default | |-- app.conf | |-- data | | `-- ui | | |-- nav | | | `-- default.xml | | `-- views | | `-- README | |-- props.conf |-- local | `-- app.conf `-- metadata |-- default.meta `-- local.meta To the nginxwatcher/default folder the files indexes.conf and inputs.conf should be added. The content of indexes.conf should be something like the following: [nginxwatcher] coldPath = $SPLUNK_DB\\nginxwatcher\\colddb enableDataIntegrityControl = 0 enableTsidxReduction = 0 homePath = $SPLUNK_DB\\nginxwatcher\\db maxTotalDataSizeMB = 512000 thawedPath = $SPLUNK_DB\\nginxwatcher\\thaweddb The content of inputs.conf should indicate we are monitoring the Nginx logging folder: [monitor:///var/log/nginx/*.log] disabled = false host = nginxwatcher index = nginxwatcher sourcetype = nginxwatcher_logs The contens of props.conf looks like this: [nginxwatcher_logs] NO_BINARY_CHECK = true TZ = UTC category = Structured pulldown_type = 1 KV_MODE = none disabled = false After restarting Splunk we can start using the new index of the Nginx app and query with: index=\"nginxwatcher\" To retrieve the values of the log files, we can use regular expressions and follow the description on the Nginx website. index=\"nginxwatcher\" | rex field=_raw \"&#94;(?&lt;remote_addr&gt;\\d+.\\d+.\\d+.\\d+)\\s-\\s(?P&lt;remote_user&gt;.*?)\\s\\[(?&lt;localtime&gt;\\d+\\/\\w+\\/\\d{4}:\\d{2}:\\d{2}:\\d{2}\\s\\+\\d+)\\]\\s+\\\"(?&lt;request&gt;.*?)\\\"\\s(?&lt;status&gt;\\d+)\\s(?&lt;bytes_sent&gt;\\d+.*?)\\s\\\"(?&lt;http_refererer&gt;.*?)\\\"\\s\\\"(?&lt;http_user_agent&gt;.*)\\\"\" Obviously we do not want to do this every time and we extract the values using the props.conf . The file is changed to: [nginxwatcher_logs] NO_BINARY_CHECK = true TZ = UTC category = Structured pulldown_type = 1 KV_MODE = none disabled = false EXTRACT-e1 = &#94;(?<remote_addr>\\d+.\\d+.\\d+.\\d+)\\s-\\s(?P<remote_user>.*?)\\s\\[(?<localtime>\\d+\\/\\w+\\/\\d{4}:\\d{2}:\\d{2}:\\d{2}\\s\\+\\d+)\\]\\s+\\\"(?<request>.*?)\\\"\\s(?<status>\\d+)\\s(?<bytes_sent>\\d+.*?)\\s\\\"(?<http_refererer>.*?)\\\"\\s\\\"(?<http_user_agent>.*)\\\" The following query will show a timechart for the status over time, grouped by 30 minutes. index=\"nginxwatcher\" | timechart count(status) span=30m by status Top visitors: index=\"nginxwatcher\" | top limit=20 remote_addr Pages which produce the most errors: index=\"nginxwatcher\" | search status >= 500 | stats count(status) as cnt by request, status | sort cnt desc All these graphs can of course be added to a dashboard to keep a close watch on the webserver.","tags":"posts","url":"create-splunk-app-monitor-nginx.html"},{"title":"Adding aliases to Windows","text":"Open the Registry Editor and go to the following key Computer\\HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Command Processor and add the key AutoRun with the value of the file containing the aliases. In my case this is \"%USERPROFILE%\\alias.cmd\" Save the change to the registry and restart the command prompt. Currently my alias.cmd contains the following: :: Registry path: Computer\\HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Command Processor :: Key: AutoRun :: Value: \"%USERPROFILE%\\alias.cmd\" @echo off DOSKEY alias = \"C:\\Program Files (x86)\\Notepad++\\notepad++.exe\" \"%USERPROFILE%\\alias.cmd\" DOSKEY ls = dir /B DOSKEY proj1 = cd \"%USERPROFILE%\\Documents\\Projects\\proj1\"","tags":"posts","url":"adding-aliases-windows.html"},{"title":"Install KDiff3 on OSX","text":"Installing the diff/merge tool KDiff3 is easy using the package manager Homebrew extension Cask . The extension makes is possible to install (GUI) applications on the Mac without the dragging and dropping of the DMG-files. jitsejan@MBP $ /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" jitsejan@MBP $ brew tap caskroom/cask jitsejan@MBP $ brew cask install kdiff3","tags":"posts","url":"install-kdiff3-on-osx.html"},{"title":"Installing Splunk on Ubuntu 14.04","text":"Download the latest version of Splunk Light (Currently version 6.5) to your download folder. jitsejan@jjsvps:~/Downloads$ wget -O splunklight-6.5.0-59c8927def0f-Linux-x86_64.tgz 'https://www.splunk.com/bin/splunk/DownloadActivityServlet?architecture=x86_64&platform=linux&version=6.5.0&product=splunk_light&filename=splunklight-6.5.0-59c8927def0f-Linux-x86_64.tgz&wget=true' Extract the archive to the /opt/ folder. jitsejan@jjsvps:~/Downloads$ sudo tar zvzf splunklight-6.5.0-59c8927def0f-Linux-x86_64.tgz -C /opt/ Export the folder where Splunk is installed to your environment. jitsejan@jjsvps:/opt/splunk$ echo 'export SPLUNK_HOME=/opt/splunk/' >> ~/.bashrc jitsejan@jjsvps:/opt/splunk$ source ~/.bashrc Make sure the rights of the /opt/splunk/ folder are correctly set. jitsejan@jjsvps:/opt$ sudo chown -R jitsejan:root splunk/ Enable access to the Splunk web interface by adding a subdomain that links to the right port. jitsejan@jjsvps:/etc/nginx/sites-available$ sudo nano splunk Add the following to the configuration file. Change the subdomain and port to the right values for you. server { listen 80 ; server_name subdomain.jitsejan.com ; location / { proxy_pass http : // localhost : 8888 ; } } Enable the subdomain by creating a system link. jitsejan@jjsvps:/etc/nginx/sites-available$ sudo ln -s /etc/nginx/sites-available/splunk /etc/nginx/sites-enabled/ And finally restart the server. jitsejan@jjsvps:/etc/nginx/sites-available$ sudo service nginx restart Now you can open up the browser and go the the subdomain that you just introduced.","tags":"posts","url":"installing-splunk-on-ubuntu-1404.html"},{"title":"Add CSS to Jupyter notebook","text":"To add some style to the notebook, Jupyter has the option to add a custom css file. This file should be located in ~/.jupyter/custom/custom.css . If this file does not exist yet, create the directory if needed and add the stylesheet. Add the following code and refresh the notebook to see if it works. body { background-color : purple ; } To make it easier to modify this file, create a link from the stylesheet to the working directory for the notebooks in order to be able to modify the stylesheet in the browser. jitsejan@jjsvps:~/code/notebooks$ ln -s ~/.jupyter/custom/custom.css .","tags":"posts","url":"add-css-to-jupyter-notebook.html"},{"title":"Create a choropleth for the top 50 artists I listen on Spotify","text":"# Import the settings for the notebooks from notebooksettings import GRACENOTE_USERID , SPOTIFY_USERNAME 1. Connect to Spotify I will use the Spotipy library to connect to Spotify. Both reading the library and reading the top tracks will be enabled by setting the scope appropriately. import sys import spotipy import spotipy.util as util # Set scope to read the library and read the top tracks scope = 'user-library-read user-top-read' username = SPOTIFY_USERNAME token = util . prompt_for_user_token ( username , scope ) 2. Retrieve songs from Spotify After creating a token, you can make a new Spotipy instance and connect to your account. Lets retrieve the top 50 of artists of my account and add the artists to a list. LIMIT = 50 OFFSET = 0 artists = {} if token : sp = spotipy . Spotify ( auth = token ) results = sp . current_user_top_artists ( limit = LIMIT , offset = OFFSET ) for artist in results [ 'items' ]: artist_id = artist [ 'id' ] artists [ artist_id ] = sp . artist ( artist_id )[ 'name' ] else : print \"Can't get token for\" , username 3. Create a placeholder for the country mapping To create a choropleth, I will create a list of countries using the Pycountry library. The country data will contain the name, the three character long abbreviation, the number of occurrences of the country for the different artists and a list of artists. import pycountry country_data = [] for cnt in pycountry . countries : country_data . append ([ cnt . name , cnt . alpha3 , 0 , []]) 4. Create a mapping for the country name to country abbreviation To map the country name to a three character abbreviation, we need to make a mapping linking the two together. mapping = { country . name : country . alpha3 for country in pycountry . countries } 5. Retrieve the country of origin for the artists To find the country of origin, I will make use of the pygn library to connect to Gracenote and find metadata for music. First I create a connection with pygn so I can retrieve the metadata from the Gracenote servers. Next I will find the country and map it to the right abbreviation. Finally I will increase the counter in the country data for the corresponding country and add the artist to the list. import pygn clientID = GRACENOTE_USERID userID = pygn . register ( clientID ) for artist_name in artists . values (): # Retrieve metadata metadata = pygn . search ( clientID = clientID , userID = userID , artist = artist_name ) if '2' in metadata [ 'artist_origin' ] . keys (): country = metadata [ 'artist_origin' ][ '2' ][ 'TEXT' ] elif len ( metadata [ 'artist_origin' ] . keys ()) == 0 : country = None else : country = metadata [ 'artist_origin' ][ '1' ][ 'TEXT' ] # Replace names if country == 'South Korea' : country = 'Korea, Republic of' if country == 'North Korea' : country = \"Korea, Democratic People's Republic of\" # Retrieve the mapping country_code = mapping . get ( country , 'No country found' ) # Increase the counter for corresponding country for index , cnt_entry in enumerate ( country_data ): if cnt_entry [ 1 ] == country_code : country_data [ index ][ 2 ] += 1 country_data [ index ][ 3 ] . append ( artist_name ) 6. Create a dataframe from the data Using Pandas we will now create a DataFrame to convert the data from the country data to a Pandas format. import pandas as pd df = pd . DataFrame ( country_data , columns = [ 'Country name' , 'Code' , 'Amount' , 'Artists' ]) df . head () 7. Create a choropleth from the data Using Plotly we can easily make a choropleth for the data that we just retrieved. In the data settings you indicate the type is a choropleth graph, the locations can be found in the 'Code' column and the important data is the column 'Amount'. Next we set the colors and a title and we are good to go. import plotly.plotly as py from plotly.graph_objs import * data = [ dict ( type = 'choropleth' , locations = df [ 'Code' ], z = df [ 'Amount' ], text = df [ 'Country name' ], colorscale = [[ 0 , \"rgb(0, 228, 97)\" ], [ 0.35 , \"rgb(70, 232, 117)\" ], [ 0.5 , \"rgb(100, 236, 138)\" ], [ 0.6 , \"rgb(120, 240, 172)\" ], [ 0.7 , \"rgb(140, 245, 201)\" ], [ 1 , \"rgb(250, 250, 250)\" ]], autocolorscale = False , reversescale = True , marker = dict ( line = dict ( color = 'rgb(180,180,180)' , width = 0.5 ) ), tick0 = 0 , zmin = 0 , dtick = 1000 , colorbar = dict ( autotick = False , tickprefix = '' , title = 'Number of artists' ), ) ] layout = dict ( title = \"Countries of origin of artists I listen on Spotify\" , geo = dict ( showframe = False , showcoastlines = False , projection = dict ( type = 'Mercator' ) ) ) figure = dict ( data = data , layout = layout ) py . iplot ( figure , validate = False ) 8. Conclusion As we can see in the graph above, the hypothesis is not completely true. The majority is still from the States. 9. Extra Looking at the same type of graph for the top 50 tracks on Spotify, ignoring artist that are double, I retrieve the following graph. In this graph it is slightly more evident that lately I listened to too much K-pop.","tags":"posts","url":"create-choropleth-top-50-artists-I-listen-on-spotify.html"},{"title":"Install Java version 8","text":"Install the common software-properties to be able to use the add-app-repository command. shell jitsejan@jjsvps:~$ sudo apt-get install software-properties-common Add the Java repository to the Ubuntu sources. shell jitsejan@jjsvps:~$ sudo add-apt-repository ppa:webupd8team/java Update the sources to retrieve the new Java repository. shell jitsejan@jjsvps:~$ sudo apt-get update Install Java version 8. shell jitsejan@jjsvps:~$ sudo apt-get install oracle-java8-installer","tags":"posts","url":"install-java-version-8.html"},{"title":"Add Flickr photosets to a Django site","text":"1. Set up connection (Note that I just put a dummy key, secret and userid) import flickrapi key = '123456789abcdefghijklmn' secret = '123456a7890' userid = '123456@N16' flickr = flickrapi . FlickrAPI ( key , secret ) 2. Retrieve the photosets from lxml import etree sets = flickr . photosets . getList ( user_id = userid ) photoset = sets . findall ( \".//photoset\" )[ 0 ] print etree . tostring ( photoset ) will return <photoset id = \"72157674032173850\" primary = \"30292011566\" secret = \"c384c894ce\" server = \"8552\" farm = \"9\" photos = \"31\" videos = \"0\" needs_interstitial = \"0\" visibility_can_see_set = \"1\" count_views = \"0\" count_comments = \"0\" can_comment = \"0\" date_create = \"1476484376\" date_update = \"1476488433\" > <title>Canada 2016 </title> <description>Visit to Toronto, Montreal and the Falls.</description> </photoset> 3. Connect to Django Since I want to add to pictures from Flickr to my Django webpage, I need to connect to the database. import os , sys project_path = '/opt/env/django_project/' os . environ . setdefault ( \"DJANGO_SETTINGS_MODULE\" , \"django_project.settings\" ) sys . path . append ( project_path ) 4. Load the Photoset model Now we set the path to be the Django path, we can import the models from my blog. import django django . setup () from blog.models import Photoset for field in Photoset . _meta . get_fields (): print field This will show the fields of the model. <ManyToOneRel: blog.photo> blog.Photoset.id blog.Photoset.flickr_id blog.Photoset.secret blog.Photoset.title blog.Photoset.description blog.Photoset.date_create blog.Photoset.date_update blog.Photoset.created blog.Photoset.modified 5. Add Flickr photosets to Django if not Photoset . objects . filter ( flickr_id = photoset . get ( 'id' )) . exists (): blog_photoset = Photoset () blog_photoset . flickr_id = photoset . get ( 'id' ) blog_photoset . secret = photoset . get ( 'secret' ) blog_photoset . title = photoset . find ( 'title' ) . text blog_photoset . description = photoset . find ( 'description' ) . text if photoset . find ( 'description' ) . text is 'null' else \"\" blog_photoset . date_create = photoset . get ( 'date_create' ) blog_photoset . date_update = photoset . get ( 'date_update' ) blog_photoset . save () print \"Added photoset ' %s ' to database!\" % ( blog_photoset . title ) else : print \"Photoset ' %s ' already in database!\" % ( photoset . find ( 'title' ) . text ) In my case the photoset has already been added. Photoset 'Canada 2016' already in database! 6. Retrieve photos from Flickr photoset for photo in flickr . walk_set ( photoset . attrib [ 'id' ]): photo_element = flickr . photos . getinfo ( photo_id = photo . get ( 'id' )) . find ( './photo' ) print etree . tostring ( photo_element ) break # Just print one This will return the XML object of a photo. <photo id= \"30326779825\" secret= \"78076b80de\" server= \"8140\" farm= \"9\" dateuploaded= \"1476484428\" isfavorite= \"0\" license= \"0\" safety_level= \"0\" rotation= \"270\" originalsecret= \"cb36a41988\" originalformat= \"jpg\" views= \"1\" media= \"photo\" > <owner nsid= \"45832294@N06\" username= \"jitsejan\" realname= \"Jitse-Jan van Waterschoot\" location= \"\" iconserver= \"5495\" iconfarm= \"6\" path_alias= \"jitsejan\" /> <title> Looking at the city </title> <description/> <visibility ispublic= \"1\" isfriend= \"0\" isfamily= \"0\" /> <dates posted= \"1476484428\" taken= \"2016-08-26 04:14:31\" takengranularity= \"0\" takenunknown= \"0\" lastupdate= \"1476700149\" /> <editability cancomment= \"0\" canaddmeta= \"0\" /> <publiceditability cancomment= \"1\" canaddmeta= \"0\" /> <usage candownload= \"1\" canblog= \"0\" canprint= \"0\" canshare= \"1\" /> <comments> 0 </comments> <notes/> <people haspeople= \"0\" /> <tags/> <urls> <url type= \"photopage\" > https://www.flickr.com/photos/jitsejan/30326779825/ </url> </urls> </photo> 7. Create URL for Flickr photo # url_template = \"http://farm{farm-id}.staticflickr.com/{server-id}/{id}_{secret}_[mstzb].jpg\" url = \"http://farm %(farm)s .staticflickr.com/ %(server)s / %(id)s _ %(secret)s _z.jpg\" % photo . attrib print url Resulting URL: http://farm9.staticflickr.com/8140/30326779825_78076b80de_z.jpg 8. Load the Photo model from blog.models import Photo for field in Photo . _meta . get_fields (): print field Above code will show the fields of the Photo model in Django. blog.Photo.id blog.Photo.flickr_id blog.Photo.title blog.Photo.description blog.Photo.date_posted blog.Photo.date_taken blog.Photo.url blog.Photo.image_url blog.Photo.created blog.Photo.modified blog.Photo.photoset 9. Add Flickr photo to Django blog_photo = Photo () blog_photo . flickr_id = photo . get ( 'id' ) blog_photo . title = photo . get ( 'title' ) blog_photo . description = photo . get ( 'description' ) if photoset . get ( 'description' ) is 'null' else \"\" blog_photo . date_posted = photo_element . find ( 'dates' ) . attrib [ 'posted' ] blog_photo . date_taken = photo_element . find ( 'dates' ) . attrib [ 'taken' ] blog_photo . url = photo_element . find ( 'urls/url' ) . text blog_photo . image_url = url try : blog_photo . photoset = blog_photoset except : blog_photo . photoset = Photoset . objects . filter ( flickr_id = photoset . get ( 'id' ))[ 0 ] if not Photo . objects . filter ( flickr_id = blog_photo . flickr_id ) . exists (): blog_photo . save () print \"Added photo ' %s ' to database!\" % ( blog_photo . title ) else : print \"Photo ' %s ' already in database!\" % ( blog_photo . title ) Again, the item is already in the database in my case. Photo 'Looking at the city' already in database!","tags":"posts","url":"add-flickr-photosets-django.html"},{"title":"Change PostgreSQL database encoding","text":"First login to the PostgreSQL shell. ( env ) jitsejan@jjsvps:/opt/canadalando_env/canadalando_django$ sudo -u postgres psql Check the list of databases. postgres = # \\l Here I could see my database had the wrong encoding, instead of SQL_ASCII I want UTF8. I dropped the database so I can re-create it with the right encoding. Note that I did NOT make a back-up, since my database was still empty. postgres = # DROP DATABASE website_db; In order to use UTF8 the template for the databases needs to be updated first. Disable the template1. postgres = # UPDATE pg_database SET datistemplate = FALSE WHERE datname ='template1'; Drop the database. postgres = # DROP DATABASE template1; Now re-create it with the right encoding. postgres = # CREATE DATABASE template1 WITH TEMPLATE = template0 ENCODING = 'UNICODE'; Activate the template. postgres = # UPDATE pg_database SET datistemplate = TRUE WHERE datname = 'template1'; Now we can re-create the database that we dropped earlier. postgres = # CREATE DATABASE website_db WITH ENCODING 'UNICODE';","tags":"posts","url":"change-postgresql-database-encoding.html"},{"title":"Upgrade PostgreSQL","text":"older/wrong versions. First check which PostgreSQL is running jitsejan@jjsvps:~$ sudo service postgresql status Probably this will list version 9.3 running on port 5432. Add the PostgreSQL repository to the sources to be able to update to newer versions. jitsejan@jjsvps:~$ sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' jitsejan@jjsvps:~$ wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - Now update the system to retrieve data from the new repository. jitsejan@jjsvps:~$ sudo apt-get update jitsejan@jjsvps:~$ sudo apt-get upgrade Next we can install version 9.4 of PostgreSQL. jitsejan@jjsvps:~$ sudo apt-get install postgresql-9.4 If you check the status again, you will see two instances of PostgreSQL running. jitsejan@jjsvps:~$ sudo service postgresql status Optionally the old version can be removed. jitsejan@jjsvps:~$ sudo apt-get remove --purge postgresql-9.3","tags":"posts","url":"upgrade-postgresql.html"},{"title":"Write dictionary to CSV in Python","text":"import csv def write_dictionary_to_csv ( o_file , d ): \"\"\" Write dictionary to output file \"\"\" with open ( o_file , 'wb' ) as csvfile : outputwriter = csv . writer ( csvfile , delimiter = ';' , quoting = csv . QUOTE_MINIMAL ) outputwriter . writerow ( d . keys ()) outputwriter . writerows ( zip ( * d . values ())) dictionary = { \"key1\" : [ 12 , 23 , 34 ], \"key2\" : [ 45 , 56 , 67 ], \"key3\" : [ 78 , 89 , 90 ]} output_file = 'output.csv' write_dictionary_to_csv ( output_file , dictionary )","tags":"posts","url":"write-dictionary-to-csv-python.html"},{"title":"Using Supervisor to start Gunicorn","text":"Install Supervisor. jitsejan@jjsvps:~$ sudo pip install supervisor Create the default configuration file for Supervisor. jitsejan@jjsvps:~$ sudo echo_supervisord_conf > /etc/supervisord.conf Create the configuration file for the website. jitsejan@jjsvps:~$ sudo nano /etc/supervisor/conf.d/website.conf Enter the following. ; /etc/supervisor/conf.d/website.conf [program:website] command=gunicorn -c /opt/env/gunicorn_config.py django_project.wsgi:application directory=/opt/env/django_project/ user=jitsejan autostart=True autorestart=True redirect_stderr=True Make the file executable. jitsejan@jjsvps:~$ sudo chmod a+x /etc/supervisor/conf.d/website.conf Reload Supervisor to find the new file and update the configuration. jitsejan@jjsvps:~$ sudo supervisorctl reread jitsejan@jjsvps:~$ sudo supervisorctl update Now you can start the website with the following command. jitsejan@jjsvps:~$ sudo supervisorctl start website","tags":"posts","url":"using-supervisor-start-gunicorn.html"},{"title":"Check the listening ports on the server","text":"Use netstat to check with ports are listening on the machine. jitsejan@jjsvps:~$ netstat -lnt | awk '$6 == \"LISTEN\"'","tags":"posts","url":"check-listening-ports-on-server.html"},{"title":"Install Anaconda on Ubuntu 14.04","text":"Retrieve the last Anaconda version for your system (32 or 64 bit). jitsejan@jjsvps:~$ cd Downloads/ jitsejan@jjsvps:~/Downloads$ wget https://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh Run the installer. jitsejan@jjsvps:~/Downloads$ bash Anaconda2-4.1.1-Linux-x86_64.sh Update the terminal to include the Anaconda references. jitsejan@jjsvps:~/Downloads$ source ~/.bashrc Test if iPython is working now. jitsejan@jjsvps:~$ ipython -v All set.","tags":"posts","url":"install-anaconda-ubuntu-1404.html"},{"title":"Move Django database between servers","text":"Save the database on the old server. ( oldenv ) jitsejan@oldvps:/opt/oldenv/django_project$ sudo python manage.py dumpdata blog > blog.json Load the data on the new server. Make sure the models for both blogs are identical. ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py loaddata blog.json","tags":"posts","url":"move-django-between-servers.html"},{"title":"Install Django on Ubuntu 14.04 with virtualenv, Nginx, Gunicorn and postgres","text":"Update the system first. jitsejan@jjsvps:~$ sudo apt-get update jitsejan@jjsvps:~$ sudo apt-get upgrade Install the virtual environment for Python. jitsejan@jjsvps:~$ sudo apt-get install python-virtualenv Create a new environment in a folder of your choice. jitsejan@jjsvps:~$ ls /opt jitsejan@jjsvps:~$ sudo virtualenv /opt/env jitsejan@jjsvps:~$ sudo chown jitsejan /opt/env/ Activate the environment. jitsejan@jjsvps:~$ source /opt/env/bin/activate Install Django inside the environment. ( env ) jitsejan@jjsvps:~$ pip install django ( env ) jitsejan@jjsvps:~$ deactivate env Install Postgresql on the system. jitsejan@jjsvps:~$ sudo apt-get install libpq-dev python-dev jitsejan@jjsvps:~$ sudo apt-get install postgresql postgresql-contrib Install the Nginx webserver on the system. jitsejan@jjsvps:~$ sudo apt-get install nginx Install Gunicorn in the environment. jitsejan@jjsvps:~$ source /opt/env/bin/activate ( env ) jitsejan@jjsvps:~$ sudo pip install gunicorn Create a database and a user for the project. jitsejan@jjsvps:~$ sudo -u postgres psql postgres = # CREATE DATABASE django_db; postgres = # CREATE USER django_user WITH PASSWORD 'django_pass'; postgres = # GRANT ALL PRIVILEGES ON DATABASE django_db TO django_user; Create a new project in the environment. ( env ) jitsejan@jjsvps:/opt/env$ django-admin.py startproject django_project Install the Psycopg2 so PostgreSQL can be used in the application. ( env ) jitsejan@jjsvps:~$ sudo pip install psycopg2 Add the database details to the settings.py ( env ) jitsejan@jjsvps:/opt/env/django_project$ nano django_project/settings.py Create the default entries for the application in the database, ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py syncdb ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py migrate ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py makemigrations Start the Django server. ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py runserver 0 .0.0.0:8080 Now use Gunicorn to connect to the server. ( env ) jitsejan@jjsvps:/opt/env/django_project$ gunicorn --bind 0 .0.0.0:8080 django_project.wsgi:application Create a configuration file for Gunicorn. ( env ) jitsejan@jjsvps:/opt/env$ sudo nano gunicorn_config.py Add the following to the configuration file. command = '/opt/env/bin/gunicorn' pythonpath = '/opt/env/django_project' bind = '127.0.0.1:8088' workers = 3 user = 'jitsejan' Use the configuration file for starting Gunicorn. ( env ) jitsejan@jjsvps:/opt/env/django_project$ gunicorn -c /opt/env/gunicorn_config.py django_project/django_project.wsgi:application Create a superuser for Django administration. ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo ./manage.py createsuperuser Add the STATIC_URL to the settings.py. ( env ) jitsejan@jjsvps:/opt/env/django_project$ nano django_project/settings.py ... STATIC_URL = '/static/' ... Now collect the static data ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo ./manage.py collectstatic Create a new site in Nginx for the Django project jitsejan@jjsvps:~$ sudo nano /etc/nginx/sites-available/django_project Add the following. Change the IP address, the folder for the static files and make sure the port is the same as configured for Gunicorn before. server { server_name 123.456.123.456, *.domain.com ; access_log off ; location /static/ { alias /opt/env/django_project/static/ ; } location / { proxy_pass http : // 127.0.0.1 : 8088 ; proxy_set_header X-Forwarded-Host $server_name ; proxy_set_header X-Real-IP $remote_addr ; add_header P3P 'CP=\"ALL DSP COR PSAa PSDa OUR NOR ONL UNI COM NAV\"' ; } } Enable the site by adding a link in the enabled sites. jitsejan@jjsvps:~$ sudo ln -s /etc/nginx/sites-available/django_project /etc/nginx/sites-enabled/ Stop Apache and start Nginx. jitsejan@jjsvps:~$ sudo service apache2 stop jitsejan@jjsvps:~$ sudo service nginx start Now run Gunicorn and visit the page in your browser. ( env ) jitsejan@jjsvps:/opt/env/django_project$ gunicorn -c /opt/env/gunicorn_config.py django_project/django_project.wsgi:application Hopefully the default Django page is shown now.","tags":"posts","url":"install-django-ubuntu-1404-virtualenv-nginx-gunicorn-and-postgres.html"},{"title":"Install Jira on Ubuntu 14.04","text":"Retrieve the last Jira binary from the website. Note that you should pick the right version, either x32 or x64. jitsejan@jjsvps:~/Downloads$ wget https://www.atlassian.com/software/jira/downloads/binary/atlassian-jira-software-7.2.1-x64.bin Make the binary executable. jitsejan@jjsvps:~/Downloads$ chmod a+x atlassian-jira-software-7.2.1-x64.bin Install the dependencies for Jira. jitsejan@jjsvps:~/Downloads$ sudo ​​apt-get install lsb-core​ default-jdk​ default-jre Execute the binary as sudo. ​jitsejan@jjsvps:~/Downloads$ ​$ sudo ./atlassian-jira-software-7.2.1-x64.bin Start the Jira server. jitsejan@jjsvps:~/Downloads$ sudo sh /opt/atlassian/jira/bin/start-jira.sh Create a Jira user and database. jitsejan@jjsvps:~$ sudo -u postgres psql postgres = # CREATE DATABASE jira; postgres = # CREATE USER jira_user WITH PASSWORD 'bla'; postgres = # GRANT ALL PRIVILEGES ON DATABASE jira TO jira_user; Now go to port 8080 on your IP address and perform the set-up. After some configuration you will be able to use Jira for your projects. Update It could be that the server does not start. Check if the permissions are right. jitsejan@jjsvps:~$ sudo chown -R jira:jira /var/atlassian/application-data/jira","tags":"posts","url":"install-jira-ubuntu-1404.html"},{"title":"Install Docker on Ubuntu 14.04","text":"First update the system. $ sudo apt-get update $ sudo apt-get -y upgrade Add the recommended package for the current kernel. $ sudo apt-get install linux-image-extra- $( uname -r ) Add the official key for Docker. $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D Add the source to the sources.list.d and refresh the packages. $ echo \"deb https://apt.dockerproject.org/repo ubuntu-trusty main\" | sudo tee /etc/apt/sources.list.d/docker.list $ sudo apt-get update Now you can install Docker. $ sudo apt-get install docker-engine Change the following in /etc/default/ufw: DEFAULT_APPLICATION_POLICY = \"DROP\" becomes DEFAULT_APPLICATION_POLICY = \"ACCEPT\" Restart the firewall. $ sudo ufw reload Create a Docker group and your current user to it to be able to connect to the Docker daemon. $ sudo groupadd docker $ sudo usermod -aG docker $USER Login again to start using Docker. Now check if Docker is working. $ sudo service docker start $ sudo docker run hello-world Hopefully this last step will download the image and run the container. If you are happy with the result, make it start automatically on system start. $ sudo systemctl enable docker","tags":"posts","url":"install-docker-ubuntu-1404.html"},{"title":"Install lxml for Python on DigitalOcean","text":"Currently I am using a DigitalOcean droplet with 512 MB to run this website. I ran into an issue when I was trying to install lxml. First make sure the correct libraries are installed before lxml is installed. $ sudo apt-get install python-dev libxml2-dev libxslt1-dev zlib1g-dev Next, be aware that the 512 MB is not enough memory to compile the lxml package with Cython when you use pip to install, which means some additional steps are needed. To virtually increase your work memory, you could use a swapfile. Create a swapfile with these commands: $ sudo dd if = /dev/zero of = /swapfile1 bs = 1024 count = 524288 $ sudo mkswap /swapfile1 $ sudo chown root:root /swapfile1 $ sudo chmod 0600 /swapfile1 Now you can use pip to install the lxml Python module $ sudo pip install lxml And of course you need to clean up after installation is done. $ sudo swapoff -v /swapfile1 $ sudo rm /swapfile1","tags":"posts","url":"install-lxml-digital-ocean.html"},{"title":"Getting started with Elasticsearch","text":"Install ElasticSearch $ mkdir ~/es $ cd ~/es $ wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.5/elasticsearch-2.3.5.tar.gz $ tar -xzvf elasticsearch-2.3.5.tar.gz $ cd elasticsearch-2.3.5/ $ ./bin/elasticsearch -d $ curl http://127.0.0.1:9200 At this point you should see something like { \"name\" : \"Gailyn Bailey\" , \"cluster_name\" : \"elasticsearch\" , \"version\" : { \"number\" : \"2.3.5\" , \"build_hash\" : \"90f439ff60a3c0f497f91663701e64ccd01edbb4\" , \"build_timestamp\" : \"2016-07-27T10:36:52Z\" , \"build_snapshot\" : false , \"lucene_version\" : \"5.5.0\" }, \"tagline\" : \"You Know, for Search\" } Create the ES index for the posts In the mappings part we want to differentiate between finding a hit in the title or in the body. A hit of the search in the title has twice as much value as a hit in the body. #!/usr/bin/env python data = { \"settings\" : { \"number_of_shards\" : 4 , \"number_of_replicas\" : 1 }, \"mappings\" : { \"blog\" : { \"properties\" : { \"title\" : { \"type\" : \"string\" , \"boost\" : 4 }, \"body\" : { \"type\" : \"string\" , \"boost\" : 2 }, } } } } import json , requests response = requests . put ( 'http://127.0.0.1:9200/blog_index/' , data = json . dumps ( data )) print response . text Add the entries #!/usr/bin/env python import json , requests from blog.models import Entry data = '' for p in Entry . objects . all (): data += '{\"index\": {\"_id\": \" %s \"}} \\n ' % p . pk data += json . dumps ({ \"title\" : p . title , \"body\" : p . body }) + ' \\n ' response = requests . put ( 'http://127.0.0.1:9200/blog_index/blog/_bulk' , data = data ) print response . text Search the entries #!/usr/bin/env python import json , requests data = { \"query\" : { \"query_string\" : { \"query\" : \"python\" } } } response = requests . post ( 'http://127.0.0.1:9200/blog_index/blog/_search' , data = json . dumps ( data )) print response . json () This gives the following reply: { \"hits\" : { \"hits\" : [ { \"_score\" : 0.63516665 , \"_type\" : \"blog\" , \"_id\" : \"4\" , \"_source\" : { \"body\" : \"```python\\r\\n\\\"\\\"\\\" samples\\/crawl_01.py \\\"\\\"\\\"\\r\\n################################################################################\\r\\n# Application: WebParser example 01\\r\\n# File: samples\\/crawl_01.py\\r\\n# Goal:\\r\\n# Input:\\r\\n# Output:\\r\\n# Example:\\r\\n#\\r\\n# History: 2016-06-27 - JJ Creation of the file\\r\\n# ... main\\r\\n################################################################################\\r\\nif __name__ == \\\"__main__\\\":\\r\\n main()\\r\\n```\" , \"title\" : \"Simple webcrawling in Python \" }, \"_index\" : \"blog_index\" }, { \"_score\" : 0.4232868 , \"_type\" : \"blog\" , \"_id\" : \"7\" , \"_source\" : { \"body\" : \"This is a simple script to crawl information from a website when the content is dynamically loaded.\\r\\n```\\r\\n\\\"\\\"\\\" samples\\/crawl_02.py \\\"\\\"\\\"\\r\\n################################################################################\\r\\n# Application: WebParser example 02\\r\\n# File: samples\\/crawl_01.py\\r\\n# Goal: Retrieve content when JavaScript is used in page\\r\\n# Input:\\r\\n# Output:\\r\\n# Example:\\r\\n#\\r\\n# History: 2016-06-27 - JJ Creation of the file\\r\\n ... main\\r\\n################################################################################\\r\\nif __name__ == \\\"__main__\\\":\\r\\n main()\\r\\n```\" , \"title\" : \"Webcrawling in Python using Selenium\" }, \"_index\" : \"blog_index\" }, { \"_score\" : 0.35721725 , \"_type\" : \"blog\" , \"_id\" : \"13\" , \"_source\" : { \"body\" : \"#### Installation\\r\\nUse the [Anaconda](https:\\/\\/www.continuum.io\\/downloads \\\"Anaconda\\\") package. It will make starting with Data Science way easier, since almost all necessary packages are included and you can start right away.\\r\\n ... [Source](http:\\/\\/twiecki.github.io\\/blog\\/2014\\/11\\/18\\/python-for-data-science\\/ \\\"Twiecki@Github\\\")\" , \"title\" : \"Get started with data science in Python\" }, \"_index\" : \"blog_index\" } ], \"total\" : 3 , \"max_score\" : 0.63516665 }, \"_shards\" : { \"successful\" : 4 , \"failed\" : 0 , \"total\" : 4 }, \"took\" : 23 , \"timed_out\" : false } Each result will get a score and the results will be ordered accordingly. Of course the better the search query, the more the score will say about the likeliness of the result matching your query.","tags":"posts","url":"getting-started-with-elasticsearch.html"},{"title":"Getting started with data science in Python","text":"Installation Use the Anaconda package. It will make starting with Data Science way easier, since almost all necessary packages are included and you can start right away. $ cd ~/Downloads $ wget http://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh $ bash Anaconda2-4.1.1-Linux-x86_64.sh $ source ~/.bashrc $ conda --version $ conda update conda Examples Make your first Data Frame #!/usr/bin/env python import pandas as pd df = pd . DataFrame ({ 'A' : 1. , 'B' : pd . Timestamp ( '20130102' ), 'C' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'D' : pd . Series ([ 1 , 2 , 1 , 2 ], dtype = 'int32' ), 'E' : pd . Categorical ([ \"test\" , \"train\" , \"test\" , \"train\" ]), 'F' : 'foo' }) df . groupby ( 'E' ) . sum () . D Create your first plots First update Seaborn $ conda install seaborn Next, create a plot of an example dataset #!/usr/bin/env python import seaborn as sns # Load one of the data sets that come with seaborn tips = sns . load_dataset ( \"tips\" ) tips . head () sns . jointplot ( \"total_bill\" , \"tip\" , tips , kind = 'reg' ); sns . lmplot ( \"total_bill\" , \"tip\" , tips , col = \"smoker\" ); Source","tags":"posts","url":"getting-started-with-datascience.html"},{"title":"Add spacers in your OSX dock","text":"Run this command in the terminal to add a spacer jitsejan@MBP $ defaults write com.apple.dock persistent-apps -array-add '{tile-data={}; tile-type=\"spacer-tile\";}' and restart the dock by running jitsejan@MBP $ killall Dock","tags":"posts","url":"add-spacers-dock-osx.html"},{"title":"Setting up an AWS EC instance","text":"Go to the EC page Launch Instance Select Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-87564feb Select t2.micro (Free tier eligible) Select Next: Configure Instance Details Select Next: Add Storage Select Next: Tag Instance Give a Name to the Instance Select Next: Configure Security Group Create a new security group Add a Security group name Add a Description Add rule by clicking Add Rule First rule should be Custom TCP Rule, TCP Protocol, Port 80 for source Anywhere Click on Launch Select Review and launch In the pop-up, select Create a new key pair Fill in a Key pair name Download the Key Pair and save in a secure location Go to the instance page and wait until the machine is ready On your computer, change the permissions of the key pair you just downloaded $ chmod 400 keypairfile.pem Connect to the machine via ssh. Click on the Connect button in the instance overview for connection information $ ssh -i keypairfile.pem ec2-xx-xx-x-xx.eu-central-1.compute.amazonaws.com","tags":"posts","url":"setup-ec-instance.html"},{"title":"Change the last modified time of a file","text":"This script will change the last modified time of a file in the current directory to 4 days back. #!/bin/ksh numDays = 4 diff = 86400 * $numDays export diff newDate = $( perl -e 'use POSIX; print strftime \"%Y%m%d%H%M\", localtime time-$ENV{diff};' ) lastFile = $( ls -lt | egrep -v &#94;d | tail -1 | awk ' { print $9 } ' ) touch -t $newDate $lastFile","tags":"posts","url":"change-modified-time-file.html"},{"title":"Show all debug information in Django","text":"< pre > {% filter force_escape %} {% debug %} {% endfilter %} </ pre >","tags":"posts","url":"debugging-in-django.html"},{"title":"Webcrawling in Python using Selenium","text":"For the script to work, four applications need to be installed first. jitsejan@jjsvps:~$ sudo pip install selenium jitsejan@jjsvps:~$ sudo apt-get install firefox jitsejan@jjsvps:~$ sudo pip install pyvirtualdisplay jitsejan@jjsvps:~$ sudo apt-get install xvfb Now the following script can be used. \"\"\" samples/crawl_02.py \"\"\" ################################################################################ # Application: WebParser example 02 # File: samples/crawl_01.py # Goal: Retrieve content when JavaScript is used in page # Input: # Output: # Example: # # History: 2016-06-27 - JJ Creation of the file # ################################################################################ ################################################################################ # Imports ################################################################################ import lxml.html import urllib2 from pyvirtualdisplay import Display from selenium import webdriver ################################################################################ # Definitions ################################################################################ HEADER = { 'Accept-Language' : 'nl-NL' , 'User-Agent' : \"\"\"Mozilla/5.0 (Windows; U; Windows NT 6.1; nl-NL; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729); nl-NL\"\"\" } ################################################################################ # Classes ################################################################################ class WebParser ( object ): \"\"\" Definition of the WebParser \"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\" Initialize the WebParser \"\"\" super ( WebParser , self ) . __init__ ( * args , ** kwargs ) @staticmethod def parse_page ( url ): \"\"\" Open URL and return the element tree of the page \"\"\" display = Display ( visible = 0 , size = ( 1920 , 1080 )) display . start () browser = webdriver . Firefox () browser . get ( url ) data = browser . page_source tree = lxml . html . fromstring ( data ) browser . quit () display . stop () return tree @staticmethod def find_css_element ( etree , element ): \"\"\" Find an element in the element tree and return it \"\"\" return etree . cssselect ( element ) ################################################################################ # Functions ################################################################################ def main (): \"\"\" Main function \"\"\" parser = WebParser () etree = parser . parse_page ( 'http://isitweekendalready.com' ) divs = parser . find_css_element ( etree , '#result' ) print divs [ 0 ] . text . strip () ################################################################################ # main ################################################################################ if __name__ == \"__main__\" : main () Update 05-Oct-2016 Recently I discovered that Selenium does not work well with newer versions of Firefox. Therefore I had to downgrade Firefox to be able to use Selenium. jitsejan@jjsvps:~$ firefox -v jitsejan@jjsvps:~$ sudo apt-get purge firefox jitsejan@jjsvps:~$ wget sourceforge.net/projects/ubuntuzilla/files/mozilla/apt/pool/main/f/firefox-mozilla-build/firefox-mozilla-build_39.0.3-0ubuntu1_amd64.deb jitsejan@jjsvps:~$ sudo dpkg -i firefox-mozilla-build_39.0.3-0ubuntu1_amd64.deb jitsejan@jjsvps:~$ rm firefox-mozilla-build_39.0.3-0ubuntu1_amd64.deb jitsejan@jjsvps:~$ firefox -v","tags":"posts","url":"webcrawling-with-selenium.html"},{"title":"Send attachment from command line","text":"$ echo 'Mail with attachment' | mutt -a \"/file/to/add/\" -s \"FYI: See attachment\" -- name@email.com","tags":"posts","url":"send-attachment-from-command-line.html"},{"title":"Mount Amazon EC as local folder","text":"jitsejan@MBP $ sshfs ubuntu@ec2-34-56-7-89.eu-central-1.compute.amazonaws.com:/home/ubuntu/ ~/AmazonEC2/ -oauto_cache,reconnect,defer_permissions,noappledouble,negative_vncache","tags":"posts","url":"mount-amazon-ec-local-folder.html"},{"title":"Simple webcrawling in Python","text":"\"\"\" samples/crawl_01.py \"\"\" ################################################################################ # Application: WebParser example 01 # File: samples/crawl_01.py # Goal: # Input: # Output: # Example: # # History: 2016-06-27 - JJ Creation of the file # ################################################################################ ################################################################################ # Imports ################################################################################ import lxml.html import urllib2 ################################################################################ # Definitions ################################################################################ HEADER = { 'Accept-Language' : 'nl-NL' , 'User-Agent' : \"\"\"Mozilla/5.0 (Windows; U; Windows NT 6.1; nl-NL; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729); nl-NL\"\"\" } ################################################################################ # Classes ################################################################################ class WebParser ( object ): \"\"\" Definition of the WebParser \"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\" Initialize the WebParser \"\"\" super ( WebParser , self ) . __init__ ( * args , ** kwargs ) @staticmethod def parse_page ( url ): \"\"\" Open URL and return the element tree of the page \"\"\" req = urllib2 . Request ( url , headers = HEADER ) data = urllib2 . urlopen ( req ) . read () tree = lxml . html . fromstring ( data ) return tree @staticmethod def find_css_element ( etree , element ): \"\"\" Find an element in the element tree and return it \"\"\" return etree . cssselect ( element ) ################################################################################ # Functions ################################################################################ def main (): \"\"\" Main function \"\"\" parser = WebParser () etree = parser . parse_page ( 'http://isitweekendyet.com/' ) divs = parser . find_css_element ( etree , 'div' ) print divs [ 0 ] . text . strip () ################################################################################ # main ################################################################################ if __name__ == \"__main__\" : main ()","tags":"posts","url":"simple-webcrawling-python.html"},{"title":"Create big files with dd","text":"Use dd in Unix to create files with a size of 2.7 GB. #!/bin/ksh dir = /this/is/my/outputdir/ numGig = 2 .7 factor = 1024 memLimit = $( expr $numGig * $factor * $factor * $factor | bc ) cd $dir for i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ; do dd if = /dev/urandom of = dummy_ $i .xml count = 204800 bs = $factor done","tags":"posts","url":"create-big-files-with-dd.html"},{"title":"Find most used history command","text":"$ awk '{print $1}' ~/.bash_history | sort | uniq -c | sort -n","tags":"posts","url":"most-used-history-command.html"}]}